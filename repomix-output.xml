This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
app/
  api/
    chat/
      route.ts
    podcast-metadata/
      route.ts
    summarize/
      route.ts
  auth/
    auth-code-error/
      page.tsx
    callback/
      route.ts
  dashboard/
    page.tsx
  validate-metadata/
    page.tsx
  validate-transcript/
    page.tsx
  globals.css
  layout.tsx
  page.tsx
  page.tsx.backup
components/
  Chat.tsx
  ErrorBoundary.tsx
  Navbar.tsx
  PodcastHeader.tsx
  PodcastMetadata.tsx
  Summary.tsx
lib/
  supabase/
    client.ts
    server.ts
  openai.ts
  supabaseClient.ts
  utils.ts
  youtube-transcript.ts
  youtube.ts
public/
  create-screenshot.html
  README.md
.gitignore
DEPLOYMENT_VERCEL.md
middleware.ts
package.json
postcss.config.js
README.md
supabaseplan.md
tailwind.config.js
test-package.json
test-transcript.js
TRANSCRIPT_ISSUE_PLAN.md
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app/auth/auth-code-error/page.tsx">
import React from "react";

export default function AuthCodeError() {
  return (
    <div
      style={{ padding: "20px", fontFamily: "sans-serif", textAlign: "center" }}
    >
      <h1>Authentication Error</h1>
      <p>
        Sorry, we couldn't sign you in. There was an issue during the
        authentication process.
      </p>
      <p>Please try signing in again.</p>
      <a href="/">Go back to Home</a>
    </div>
  );
}
</file>

<file path="app/auth/callback/route.ts">
import { createServerClient, type CookieOptions } from "@supabase/ssr";
import { cookies } from "next/headers";
import { NextResponse, type NextRequest } from "next/server";

export async function GET(request: NextRequest) {
  const { searchParams, origin } = new URL(request.url);
  const code = searchParams.get("code");
  // if "next" is in param, use it as the redirect URL
  const next = searchParams.get("next") ?? "/dashboard"; // Default redirect to dashboard

  if (code) {
    const cookieStore = cookies();
    const supabase = createServerClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
      {
        cookies: {
          get(name: string) {
            return cookieStore.get(name)?.value;
          },
          set(name: string, value: string, options: CookieOptions) {
            cookieStore.set({ name, value, ...options });
          },
          remove(name: string, options: CookieOptions) {
            cookieStore.delete({ name, ...options });
          },
        },
      }
    );
    const { error } = await supabase.auth.exchangeCodeForSession(code);
    if (!error) {
      return NextResponse.redirect(`${origin}${next}`);
    }
  }

  // return the user to an error page with instructions
  console.error("Error exchanging code for session or code not found");
  return NextResponse.redirect(`${origin}/auth/auth-code-error`);
}
</file>

<file path="app/validate-metadata/page.tsx">
"use client";

import { useState } from "react";

export default function ValidateMetadata() {
  const [url, setUrl] = useState("");
  const [metadata, setMetadata] = useState<any>(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState("");
  const [showFullDescription, setShowFullDescription] = useState(false);

  const validateMetadata = async () => {
    setLoading(true);
    setError("");
    setShowFullDescription(false); // Reset description state on new validation
    try {
      const response = await fetch("/api/podcast-metadata", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          url,
          includeFull: true, // Always request the full description in the API
        }),
      });

      const data = await response.json();
      if (!response.ok) {
        throw new Error(data.error || "Failed to validate metadata");
      }

      setMetadata(data.metadata);
    } catch (err: any) {
      setError(err.message || "An error occurred");
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="p-6 max-w-4xl mx-auto">
      <h1 className="text-2xl font-bold mb-4">YouTube Metadata Validator</h1>
      <div className="mb-4">
        <input
          type="text"
          value={url}
          onChange={(e) => setUrl(e.target.value)}
          placeholder="Enter YouTube URL"
          className="w-full p-2 border rounded"
        />
      </div>
      <button
        onClick={validateMetadata}
        disabled={loading}
        className="px-4 py-2 bg-blue-500 text-white rounded disabled:bg-blue-300"
      >
        {loading ? "Validating..." : "Validate Metadata"}
      </button>

      {error && (
        <div className="mt-4 p-3 bg-red-100 text-red-700 rounded">{error}</div>
      )}

      {metadata && (
        <div className="mt-6">
          <h2 className="text-xl font-semibold mb-2">Metadata Results:</h2>

          <div className="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
            <div className="bg-gray-50 p-4 rounded shadow">
              <h3 className="font-medium text-lg mb-2">Basic Information</h3>
              <div className="space-y-2">
                <p>
                  <span className="font-semibold">Title:</span> {metadata.title}
                </p>
                <p>
                  <span className="font-semibold">Channel:</span>{" "}
                  {metadata.channelName}
                </p>
                <p>
                  <span className="font-semibold">Duration:</span>{" "}
                  {metadata.duration}
                </p>
                <p>
                  <span className="font-semibold">Video ID:</span>{" "}
                  {metadata.videoId}
                </p>
                {metadata.publishedAt && (
                  <p>
                    <span className="font-semibold">Published:</span>{" "}
                    {new Date(metadata.publishedAt).toLocaleDateString()}
                  </p>
                )}
                {metadata.viewCount && (
                  <p>
                    <span className="font-semibold">Views:</span>{" "}
                    {parseInt(metadata.viewCount).toLocaleString()}
                  </p>
                )}
                {metadata.likeCount && (
                  <p>
                    <span className="font-semibold">Likes:</span>{" "}
                    {parseInt(metadata.likeCount).toLocaleString()}
                  </p>
                )}
              </div>
            </div>

            <div className="bg-gray-50 p-4 rounded shadow">
              <h3 className="font-medium text-lg mb-2">Description</h3>
              <div className="max-h-60 overflow-y-auto">
                <p className="whitespace-pre-wrap">
                  {showFullDescription
                    ? metadata.fullDescription
                    : metadata.description}
                </p>
                {metadata.descriptionTruncated && (
                  <button
                    onClick={() => setShowFullDescription(!showFullDescription)}
                    className="mt-2 text-blue-500 hover:text-blue-700 text-sm font-medium"
                  >
                    {showFullDescription ? "Show less" : "Show more"}
                  </button>
                )}
              </div>
            </div>
          </div>

          {metadata.thumbnails && (
            <div className="bg-gray-50 p-4 rounded shadow">
              <h3 className="font-medium text-lg mb-2">Thumbnails</h3>
              <div className="grid grid-cols-2 md:grid-cols-4 gap-4">
                {Object.entries(metadata.thumbnails).map(
                  ([key, thumb]: [string, any]) => (
                    <div key={key} className="text-center">
                      <img
                        src={thumb.url}
                        alt={`${key} thumbnail`}
                        className="mx-auto mb-2 rounded"
                      />
                      <p className="text-sm">
                        {key}: {thumb.width}x{thumb.height}
                      </p>
                    </div>
                  )
                )}
              </div>
            </div>
          )}

          <div className="mt-4 bg-gray-100 p-4 rounded overflow-auto max-h-96">
            <h3 className="font-medium text-lg mb-2">Raw JSON Data</h3>
            <pre className="text-xs">{JSON.stringify(metadata, null, 2)}</pre>
          </div>
        </div>
      )}
    </div>
  );
}
</file>

<file path="app/validate-transcript/page.tsx">
"use client";

import { useState } from "react";
import { useRouter } from "next/navigation";

export default function ValidateTranscript() {
  const [url, setUrl] = useState("");
  const [error, setError] = useState<string | null>(null);
  const [transcript, setTranscript] = useState<string | null>(null);
  const [loading, setLoading] = useState(false);
  const router = useRouter();

  const validateTranscript = async () => {
    setLoading(true);
    setError(null);
    setTranscript(null);

    try {
      // Make a simple request to our chat API with a minimal question
      // This will test the transcript fetching functionality
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          url,
          question:
            "Just testing the transcript. Please say 'Transcript fetched successfully!'",
        }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || "Failed to validate transcript");
      }

      setTranscript(data.answer);
    } catch (error) {
      console.error("Error:", error);
      const errorMessage =
        error instanceof Error
          ? error.message
          : "Failed to validate transcript";
      setError(errorMessage);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="p-6 max-w-4xl mx-auto">
      <h1 className="text-2xl font-bold mb-4">YouTube Transcript Validator</h1>
      <div className="mb-4">
        <input
          type="text"
          value={url}
          onChange={(e) => setUrl(e.target.value)}
          placeholder="Enter YouTube URL"
          className="w-full p-2 border rounded"
        />
      </div>
      <button
        onClick={validateTranscript}
        disabled={loading}
        className="px-4 py-2 bg-blue-500 text-white rounded disabled:bg-blue-300"
      >
        {loading ? "Validating..." : "Validate Transcript"}
      </button>

      {error && (
        <div className="mt-4 p-3 bg-red-100 text-red-700 rounded">{error}</div>
      )}

      {transcript && (
        <div className="mt-6">
          <h2 className="text-xl font-semibold mb-2">Transcript Result:</h2>
          <div className="p-4 bg-gray-50 rounded-lg border">{transcript}</div>
        </div>
      )}

      <div className="mt-4">
        <button
          onClick={() => router.push("/")}
          className="px-4 py-2 bg-gray-200 rounded"
        >
          Back to Home
        </button>
      </div>
    </div>
  );
}
</file>

<file path="app/globals.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --foreground-rgb: 0, 0, 0;
  --background-start-rgb: 214, 219, 220;
  --background-end-rgb: 255, 255, 255;
}

body {
  color: rgb(var(--foreground-rgb));
  background: linear-gradient(
      to bottom,
      transparent,
      rgb(var(--background-end-rgb))
    )
    rgb(var(--background-start-rgb));
}

@layer utilities {
  .prose {
    max-width: 65ch;
    line-height: 1.6;
  }

  .prose p {
    margin-bottom: 1.25em;
  }
}

/* Markdown styling */
pre {
  font-family: ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont,
    "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  line-height: 1.6;
  font-size: 1rem;
  background-color: #f3f4f6;
  padding: 1em;
  border-radius: 0.5em;
  overflow-x: auto;
  margin: 1.5em 0;
}

pre code {
  background-color: transparent !important;
  padding: 0 !important;
  border-radius: 0 !important;
}

/* Enhanced Markdown Content Styling */
.markdown-content {
  font-family: ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont,
    "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  line-height: 1.6;
  font-size: 1rem;
  color: #374151;
}

.markdown-content h1,
.markdown-content h2,
.markdown-content h3,
.markdown-content h4,
.markdown-content h5,
.markdown-content h6 {
  font-weight: 600;
  margin-top: 1.5em;
  margin-bottom: 0.75em;
  line-height: 1.3;
  color: #111827;
}

.markdown-content h1 {
  font-size: 1.875rem;
  margin-top: 0;
}

.markdown-content h2 {
  font-size: 1.5rem;
  border-bottom: 1px solid #e5e7eb;
  padding-bottom: 0.5em;
  margin-top: 2em;
}

.markdown-content h3 {
  font-size: 1.25rem;
}

.markdown-content p {
  margin-bottom: 1.25em;
}

.markdown-content ul,
.markdown-content ol {
  padding-left: 1.75em;
  margin: 1em 0;
}

.markdown-content li {
  margin-bottom: 0.5em;
  position: relative;
}

.markdown-content li p {
  margin-bottom: 0.5em;
}

.markdown-content code {
  font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas,
    "Liberation Mono", "Courier New", monospace;
  background-color: #f3f4f6;
  padding: 0.2em 0.4em;
  border-radius: 0.25em;
  font-size: 0.875em;
}

.markdown-content blockquote {
  border-left: 4px solid #e5e7eb;
  padding: 0.5em 1em;
  margin: 1.5em 0;
  background-color: #f9fafb;
  color: #4b5563;
}

.markdown-content a {
  color: #2563eb;
  text-decoration: underline;
  text-underline-offset: 2px;
}

.markdown-content a:hover {
  color: #1d4ed8;
}

.markdown-content hr {
  border: 0;
  border-top: 1px solid #e5e7eb;
  margin: 2em 0;
}

.markdown-content strong {
  font-weight: 600;
  color: #111827;
}

.markdown-content em {
  font-style: italic;
}

/* Timestamp styling */
.markdown-content p:has(code) {
  margin-top: 1.5em;
  font-weight: 500;
}

/* List item spacing */
.markdown-content ul li,
.markdown-content ol li {
  margin-bottom: 0.75em;
}

/* Nested lists */
.markdown-content ul ul,
.markdown-content ol ol,
.markdown-content ul ol,
.markdown-content ol ul {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}

/* Table styling */
.markdown-content table {
  width: 100%;
  border-collapse: collapse;
  margin: 1.5em 0;
}

.markdown-content th,
.markdown-content td {
  padding: 0.75em;
  border: 1px solid #e5e7eb;
}

.markdown-content th {
  background-color: #f9fafb;
  font-weight: 600;
  text-align: left;
}

.markdown-content tr:nth-child(even) {
  background-color: #f9fafb;
}

/* Image styling */
.markdown-content img {
  max-width: 100%;
  height: auto;
  border-radius: 0.375em;
  margin: 1.5em 0;
}
</file>

<file path="app/page.tsx.backup">
"use client";

import React, { useState, useEffect } from "react";
import { ArrowPathIcon } from "@heroicons/react/24/solid";
import toast, { Toaster } from "react-hot-toast";
import Summary from "../components/Summary";
import Chat from "../components/Chat";
import Navbar from "../components/Navbar";
import PodcastHeader from "../components/PodcastHeader";
import { PodcastMetadataProvider } from "../components/PodcastMetadata";

export default function Home() {
  const [url, setUrl] = useState("");
  const [loading, setLoading] = useState(false);
  const [validating, setValidating] = useState(false);
  const [summary, setSummary] = useState<string | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [activeTab, setActiveTab] = useState<"summary" | "chat">("summary");
  const [isUrlValidated, setIsUrlValidated] = useState(false);
  const [isSummaryLoading, setIsSummaryLoading] = useState(false);
  const [isChatLoading, setIsChatLoading] = useState(false);

  // Function to validate YouTube URL format
  const isValidYoutubeUrl = (url: string): boolean => {
    const youtubeRegex = /^(https?:\/\/)?(www\.)?(youtube\.com|youtu\.be)\/.+/;
    return youtubeRegex.test(url);
  };

  // Extract video ID from URL
  const extractVideoId = (url: string): string | null => {
    const match = url.match(
      /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
    );
    return match ? match[1] : null;
  };

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setError(null);

    if (!isValidYoutubeUrl(url)) {
      toast.error("Please enter a valid YouTube URL");
      return;
    }

    const videoId = extractVideoId(url);
    if (!videoId) {
      toast.error("Could not extract video ID from URL");
      return;
    }

    setLoading(true);
    setIsSummaryLoading(true);
    setSummary(null);

    try {
      console.log("Submitting URL:", url);
      const response = await fetch("/api/summarize", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ url }),
      });

      const data = await response.json();
      console.log("Response:", data);

      if (!response.ok) {
        throw new Error(data.error || "Failed to generate summary");
      }

      setSummary(data.summary);
      setIsUrlValidated(true);
      toast.success("Summary generated successfully!");
    } catch (error) {
      console.error("Error:", error);
      const errorMessage =
        error instanceof Error ? error.message : "Failed to generate summary";
      setError(errorMessage);
      toast.error(errorMessage);
    } finally {
      setLoading(false);
      setIsSummaryLoading(false);
    }
  };

  const validateUrl = async () => {
    if (!isValidYoutubeUrl(url)) {
      toast.error("Please enter a valid YouTube URL");
      return;
    }

    const videoId = extractVideoId(url);
    if (!videoId) {
      toast.error("Could not extract video ID from URL");
      return;
    }

    setValidating(true);
    setIsChatLoading(true);
    setError(null);

    try {
      // Just check if the transcript exists
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          url,
          question:
            "Just checking if the transcript exists. Please respond with 'Yes'.",
        }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || "Failed to validate URL");
      }

      setIsUrlValidated(true);
      setActiveTab("chat");
      toast.success("Podcast loaded successfully!");
    } catch (error) {
      console.error("Error:", error);
      const errorMessage =
        error instanceof Error ? error.message : "Failed to validate URL";
      setError(errorMessage);
      toast.error(errorMessage);
    } finally {
      setValidating(false);
      setIsChatLoading(false);
    }
  };

  const handleTabChange = (tab: "summary" | "chat") => {
    if (loading || validating) return;

    setActiveTab(tab);

    // If switching to chat and URL is not yet validated, validate it
    if (tab === "chat" && !isUrlValidated && url) {
      validateUrl();
    }

    // If switching to summary and no summary exists yet, generate it
    if (tab === "summary" && !summary && isUrlValidated) {
      setIsSummaryLoading(true);
      handleSubmit(new Event("submit") as any);
    }
  };

  // Handle URL input change
  const handleUrlChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    setUrl(e.target.value);
    // Reset validation state when URL changes
    if (isUrlValidated) {
      setIsUrlValidated(false);
      setSummary(null);
    }
  };

  return (
    <div className="min-h-screen bg-[#f7f7ff]">
      <Navbar />
      <Toaster position="bottom-right" />

      <div className="max-w-7xl mx-auto px-4">
        <div className="pt-8 pb-16 md:pt-12 md:pb-24 text-center">
          <h1 className="text-5xl md:text-6xl font-bold text-purple-700 mb-6">
            Transform Podcasts into
            <br />
            Interactive Conversations
          </h1>
          <p className="text-lg text-gray-600 max-w-3xl mx-auto mb-10">
            Paste any podcast URL and let AI create summaries and engage in
            meaningful conversations about the content
          </p>

          <div className="max-w-3xl mx-auto bg-white p-8 rounded-2xl shadow-md">
            <div className="flex items-center mb-4">
              <div className="w-6 h-6 mr-2">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                  className="text-purple-600"
                >
                  <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
                  <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
                </svg>
              </div>
              <h2 className="text-xl font-semibold">Enter Podcast URL</h2>
            </div>

            <form onSubmit={handleSubmit}>
              <div className="flex flex-col gap-4">
                <div className="flex flex-col md:flex-row gap-2">
                  <input
                    id="youtube-url"
                    type="text"
                    value={url}
                    onChange={handleUrlChange}
                    placeholder="https://podcast-url.com/episode"
                    className="flex-1 p-3 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-purple-500"
                    disabled={loading || validating}
                    aria-label="YouTube URL"
                  />
                  <button
                    type="submit"
                    disabled={loading || validating || !url.trim()}
                    className="px-6 py-3 bg-purple-600 text-white rounded-lg hover:bg-purple-700 disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center"
                    aria-label={loading ? "Processing..." : "Analyze"}
                  >
                    {loading ? (
                      <div className="flex items-center">
                        <ArrowPathIcon className="w-5 h-5 animate-spin mr-2" />
                        <span>Processing...</span>
                      </div>
                    ) : (
                      <div className="flex items-center">
                        <svg
                          xmlns="http://www.w3.org/2000/svg"
                          viewBox="0 0 24 24"
                          fill="currentColor"
                          className="w-5 h-5 mr-2"
                        >
                          <path d="M18.375 2.25c-1.035 0-1.875.84-1.875 1.875v15.75c0 1.035.84 1.875 1.875 1.875h.75c1.035 0 1.875-.84 1.875-1.875V4.125c0-1.036-.84-1.875-1.875-1.875h-.75zM9.75 8.625c0-1.036.84-1.875 1.875-1.875h.75c1.036 0 1.875.84 1.875 1.875v11.25c0 1.035-.84 1.875-1.875 1.875h-.75c-1.036 0-1.875-.84-1.875-1.875V8.625zM3 13.125c0-1.036.84-1.875 1.875-1.875h.75c1.036 0 1.875.84 1.875 1.875v6.75c0 1.035-.84 1.875-1.875 1.875h-.75C3.84 21.75 3 20.91 3 19.875v-6.75z" />
                        </svg>
                        Analyze
                      </div>
                    )}
                  </button>
                </div>
                {error && (
                  <div className="text-red-600 text-sm p-2 bg-red-50 rounded">
                    Error: {error}
                  </div>
                )}
              </div>
            </form>
          </div>
        </div>

        {/* Feature Cards Section - only shown when no podcast summary is generated */}
        {!isUrlValidated && (
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-8 max-w-7xl mx-auto px-4 pb-20">
            {/* Smart Summaries Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M12 6.042A8.967 8.967 0 006 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 016 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 016-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0018 18a8.967 8.967 0 00-6 2.292m0-14.25v14.25" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Smart Summaries
              </h3>
              <p className="text-gray-600">
                Get comprehensive summaries that capture the key points and
                insights from any podcast.
              </p>
            </div>

            {/* Interactive Chat Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M4.913 2.658c2.075-.27 4.19-.408 6.337-.408 2.147 0 4.262.139 6.337.408 1.922.25 3.291 1.861 3.405 3.727a4.403 4.403 0 00-1.032-.211 50.89 50.89 0 00-8.42 0c-2.358.196-4.04 2.19-4.04 4.434v4.286a4.47 4.47 0 002.433 3.984L7.28 21.53A.75.75 0 016 21v-4.03a48.527 48.527 0 01-1.087-.128C2.905 16.58 1.5 14.833 1.5 12.862V6.638c0-1.97 1.405-3.718 3.413-3.979z" />
                  <path d="M15.75 7.5c-1.376 0-2.739.057-4.086.169C10.124 7.797 9 9.103 9 10.609v4.285c0 1.507 1.128 2.814 2.67 2.94 1.243.102 2.5.157 3.768.165l2.782 2.781a.75.75 0 001.28-.53v-2.39l.33-.026c1.542-.125 2.67-1.433 2.67-2.94v-4.286c0-1.505-1.125-2.811-2.664-2.94A49.392 49.392 0 0015.75 7.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Interactive Chat
              </h3>
              <p className="text-gray-600">
                Ask questions about the podcast content and get detailed answers
                from our AI assistant.
              </p>
            </div>

            {/* Works Everywhere Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
                  <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Works Everywhere
              </h3>
              <p className="text-gray-600">
                Compatible with podcasts from Spotify, Apple Podcasts, YouTube,
                and many more platforms.
              </p>
            </div>

            {/* AI-Powered Insights Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M21.731 2.269a2.625 2.625 0 00-3.712 0l-1.157 1.157 3.712 3.712 1.157-1.157a2.625 2.625 0 000-3.712zM19.513 8.199l-3.712-3.712-8.4 8.4a5.25 5.25 0 00-1.32 2.214l-.8 2.685a.75.75 0 00.933.933l2.685-.8a5.25 5.25 0 002.214-1.32l8.4-8.4z" />
                  <path d="M5.25 5.25a3 3 0 00-3 3v10.5a3 3 0 003 3h10.5a3 3 0 003-3V13.5a.75.75 0 00-1.5 0v5.25a1.5 1.5 0 01-1.5 1.5H5.25a1.5 1.5 0 01-1.5-1.5V8.25a1.5 1.5 0 011.5-1.5h5.25a.75.75 0 000-1.5H5.25z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                AI-Powered Insights
              </h3>
              <p className="text-gray-600">
                Advanced AI analyzes podcasts to extract the most relevant
                information and context.
              </p>
            </div>
          </div>
        )}

        {isUrlValidated && (
          <div className="max-w-5xl mx-auto">
            <PodcastMetadataProvider videoUrl={url}>
              {(metadata, metadataLoading) => (
                <>
                  {metadata && (
                    <PodcastHeader
                      videoId={metadata.videoId}
                      title={metadata.title}
                      channelName={metadata.channelName}
                      duration={metadata.duration}
                      description={metadata.description}
                      onChatClick={() => handleTabChange("chat")}
                      activeTab={activeTab}
                    />
                  )}

                  <div className="bg-white rounded-lg shadow-md mb-8">
                    <div className="flex border-b">
                      <button
                        onClick={() => handleTabChange("summary")}
                        className={`px-4 py-3 font-medium ${
                          activeTab === "summary"
                            ? "text-purple-600 border-b-2 border-purple-600"
                            : "text-gray-500 hover:text-gray-700"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Summary tab"
                        aria-selected={activeTab === "summary"}
                        role="tab"
                      >
                        Summary{" "}
                        {isSummaryLoading && (
                          <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                        )}
                      </button>
                      <button
                        onClick={() => handleTabChange("chat")}
                        className={`px-4 py-3 font-medium ${
                          activeTab === "chat"
                            ? "text-purple-600 border-b-2 border-purple-600"
                            : "text-gray-500 hover:text-gray-700"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Chat Assistant tab"
                        aria-selected={activeTab === "chat"}
                        role="tab"
                      >
                        Chat Assistant{" "}
                        {isChatLoading && (
                          <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                        )}
                      </button>
                    </div>

                    <div className="flex border-b space-x-8 px-4">
                      <button
                        onClick={() => handleTabChange("summary")}
                        className={`pt-4 pb-3 font-medium text-base ${
                          activeTab === "summary"
                            ? "text-purple-600 border-b-2 border-purple-600"
                            : "text-gray-500 hover:text-gray-700"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Summary tab"
                        aria-selected={activeTab === "summary"}
                        role="tab"
                      >
                        <div className="flex items-center">
                          <span>Summary</span>
                          {isSummaryLoading && (
                            <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                          )}
                        </div>
                      </button>
                      <button
                        onClick={() => handleTabChange("chat")}
                        className={`pt-4 pb-3 font-medium text-base ${
                          activeTab === "chat"
                            ? "text-purple-600 border-b-2 border-purple-600"
                            : "text-gray-500 hover:text-gray-700"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Chat Assistant tab"
                        aria-selected={activeTab === "chat"}
                        role="tab"
                      >
                        <div className="flex items-center">
                          <span>Chat Assistant</span>
                          {isChatLoading && (
                            <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                          )}
                        </div>
                      </button>
                    </div>

                    <div
                      role="tabpanel"
                      aria-label="Tab Content"
                      className="p-8"
                    >
                      {activeTab === "summary" && (
                        <Summary summary={summary || ""} videoUrl={url} />
                      )}
                      {activeTab === "chat" && <Chat videoUrl={url} />}
                    </div>
                  </div>
                </>
              )}
            </PodcastMetadataProvider>
          </div>
        )}
      </div>
    </div>
  );
}
</file>

<file path="components/ErrorBoundary.tsx">
"use client";

import React, { Component, ErrorInfo, ReactNode } from "react";

interface Props {
  children: ReactNode;
  fallback?: ReactNode;
}

interface State {
  hasError: boolean;
  error: Error | null;
}

class ErrorBoundary extends Component<Props, State> {
  constructor(props: Props) {
    super(props);
    this.state = {
      hasError: false,
      error: null,
    };
  }

  static getDerivedStateFromError(error: Error): State {
    return {
      hasError: true,
      error,
    };
  }

  componentDidCatch(error: Error, errorInfo: ErrorInfo): void {
    console.error("Error caught by ErrorBoundary:", error, errorInfo);
  }

  render(): ReactNode {
    if (this.state.hasError) {
      if (this.props.fallback) {
        return this.props.fallback;
      }

      return (
        <div className="min-h-screen flex items-center justify-center bg-gray-50 p-4">
          <div className="bg-white p-8 rounded-lg shadow-md max-w-md w-full">
            <h2 className="text-2xl font-bold text-red-600 mb-4">
              Something went wrong
            </h2>
            <p className="text-gray-700 mb-4">
              We're sorry, but there was an error processing your request.
              Please try again later.
            </p>
            <div className="bg-gray-100 p-3 rounded text-sm text-gray-800 font-mono overflow-auto max-h-32 mb-4">
              {this.state.error?.message || "Unknown error"}
            </div>
            <button
              onClick={() => window.location.reload()}
              className="w-full py-2 px-4 bg-blue-600 text-white rounded hover:bg-blue-700 transition-colors"
            >
              Reload Page
            </button>
          </div>
        </div>
      );
    }

    return this.props.children;
  }
}

export default ErrorBoundary;
</file>

<file path="components/PodcastHeader.tsx">
import React, { useState } from "react";
import {
  ClockIcon,
  ChatBubbleLeftRightIcon,
  ShareIcon,
  EyeIcon,
  HandThumbUpIcon,
  CalendarIcon,
} from "@heroicons/react/24/outline";

interface PodcastHeaderProps {
  videoId: string;
  title: string;
  channelName: string;
  duration: string;
  viewCount?: string | null;
  likeCount?: string | null;
  publishedAt?: string | null;
  onChatClick: () => void;
  activeTab: "summary" | "chat";
}

const PodcastHeader: React.FC<PodcastHeaderProps> = ({
  videoId,
  title,
  channelName,
  duration,
  viewCount,
  likeCount,
  publishedAt,
  onChatClick,
  activeTab,
}) => {
  // Use the highest quality thumbnail available (maxresdefault is best quality)
  // With fallback to hqdefault if maxresdefault isn't available
  const thumbnailUrl = `https://i.ytimg.com/vi/${videoId}/maxresdefault.jpg`;
  const fallbackThumbnailUrl = `https://i.ytimg.com/vi/${videoId}/hqdefault.jpg`;
  const [imgSrc, setImgSrc] = useState(thumbnailUrl);

  // Format the published date
  const formatDate = (dateString: string | null) => {
    if (!dateString) return "Unknown date";

    const date = new Date(dateString);
    return date.toLocaleDateString("en-US", {
      year: "numeric",
      month: "long",
      day: "numeric",
    });
  };

  const handleShare = () => {
    if (navigator.share) {
      navigator
        .share({
          title: title,
          text: `Check out this podcast: ${title}`,
          url: `https://www.youtube.com/watch?v=${videoId}`,
        })
        .catch((err) => console.error("Error sharing:", err));
    } else {
      const url = `https://www.youtube.com/watch?v=${videoId}`;
      navigator.clipboard.writeText(url);
      alert("Link copied to clipboard!");
    }
  };

  return (
    <div className="bg-white rounded-xl shadow-md overflow-hidden mb-6">
      <div className="md:flex">
        {/* Fixed width container that matches 16:9 aspect ratio at 420x240 */}
        <div className="md:w-[420px] md:flex-shrink-0 relative">
          {/* Mobile: Dynamic 16:9 aspect ratio with padding trick */}
          {/* Desktop: Fixed height matching 16:9 ratio of width */}
          <div className="w-full pt-[56.25%] md:pt-0 md:h-[236px]">
            <img
              className="absolute inset-0 w-full h-full object-cover"
              src={imgSrc}
              alt={title}
              onError={() => setImgSrc(fallbackThumbnailUrl)}
            />
          </div>
        </div>
        <div className="p-6 flex flex-col justify-between w-full max-w-full">
          <div>
            <p className="text-sm text-purple-600 font-semibold uppercase tracking-wide">
              {channelName}
            </p>
            <h1 className="text-2xl font-bold text-gray-900 mt-1 mb-4">
              {title}
            </h1>

            {/* Metadata stats */}
            <div className="grid grid-cols-1 md:grid-cols-2 gap-y-2 gap-x-4 mb-4">
              <div className="flex items-center text-gray-600">
                <ClockIcon className="h-5 w-5 mr-1.5 text-gray-500" />
                <span>{duration}</span>
              </div>

              {publishedAt && (
                <div className="flex items-center text-gray-600">
                  <CalendarIcon className="h-5 w-5 mr-1.5 text-gray-500" />
                  <span>{formatDate(publishedAt)}</span>
                </div>
              )}

              {viewCount && (
                <div className="flex items-center text-gray-600">
                  <EyeIcon className="h-5 w-5 mr-1.5 text-gray-500" />
                  <span>{parseInt(viewCount).toLocaleString()} views</span>
                </div>
              )}

              {likeCount && (
                <div className="flex items-center text-gray-600">
                  <HandThumbUpIcon className="h-5 w-5 mr-1.5 text-gray-500" />
                  <span>{parseInt(likeCount).toLocaleString()} likes</span>
                </div>
              )}
            </div>
          </div>

          <div className="mt-4 flex gap-3">
            <button
              onClick={onChatClick}
              className={`flex items-center px-4 py-2 rounded-lg text-sm font-medium transition-colors ${
                activeTab === "chat"
                  ? "bg-purple-600 text-white"
                  : "bg-purple-100 text-purple-700 hover:bg-purple-200"
              }`}
            >
              <ChatBubbleLeftRightIcon className="h-4 w-4 mr-2" />
              Chat About This
            </button>
            <button
              onClick={handleShare}
              className="flex items-center px-4 py-2 bg-gray-100 text-gray-700 rounded-lg text-sm font-medium hover:bg-gray-200 transition-colors"
            >
              <ShareIcon className="h-4 w-4 mr-2" />
              Share
            </button>
          </div>
        </div>
      </div>
    </div>
  );
};

export default PodcastHeader;
</file>

<file path="components/PodcastMetadata.tsx">
import React, { useEffect, useState } from "react";

export interface PodcastMetadata {
  title: string;
  channelName: string;
  duration: string;
  videoId: string;
  description?: string;
  fullDescription?: string;
  descriptionTruncated?: boolean;
  publishedAt?: string | null;
  viewCount?: string | null;
  likeCount?: string | null;
  thumbnails?: {
    [key: string]: {
      url: string;
      width: number;
      height: number;
    };
  } | null;
}

interface PodcastMetadataProviderProps {
  videoUrl: string;
  children: (
    metadata: PodcastMetadata | null,
    loading: boolean
  ) => React.ReactNode;
}

const extractVideoId = (url: string): string | null => {
  const match = url.match(
    /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
  );
  return match ? match[1] : null;
};

const formatDuration = (seconds: number): string => {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);

  if (hours > 0) {
    return `${hours}:${minutes.toString().padStart(2, "0")}:${secs
      .toString()
      .padStart(2, "0")}`;
  }

  return `${minutes}:${secs.toString().padStart(2, "0")}`;
};

export const PodcastMetadataProvider: React.FC<
  PodcastMetadataProviderProps
> = ({ videoUrl, children }) => {
  const [metadata, setMetadata] = useState<PodcastMetadata | null>(null);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    const fetchMetadata = async () => {
      try {
        setLoading(true);
        const videoId = extractVideoId(videoUrl);

        if (!videoId) {
          throw new Error("Invalid YouTube URL");
        }

        // Call our metadata API endpoint
        const response = await fetch("/api/podcast-metadata", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify({
            url: videoUrl,
            includeFull: true, // Always request full description
          }),
        });

        if (!response.ok) {
          const errorData = await response.json();
          throw new Error(errorData.error || "Failed to fetch metadata");
        }

        const data = await response.json();
        setMetadata(data.metadata);
      } catch (error) {
        console.error("Error fetching metadata:", error);

        // Fallback metadata using the video ID
        const videoId = extractVideoId(videoUrl);
        if (videoId) {
          setMetadata({
            title: "YouTube Podcast",
            channelName: "Unknown Channel",
            duration: "00:00",
            videoId: videoId,
            description: "No description available for this podcast.",
            fullDescription: "No description available for this podcast.",
            descriptionTruncated: false,
          });
        } else {
          setMetadata(null);
        }
      } finally {
        setLoading(false);
      }
    };

    if (videoUrl) {
      fetchMetadata();
    }
  }, [videoUrl]);

  return <>{children(metadata, loading)}</>;
};
</file>

<file path="lib/supabase/client.ts">
import { createBrowserClient } from "@supabase/ssr";

export function createClient() {
  // Create a supabase client on the browser with project's credentials
  return createBrowserClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
  );
}
</file>

<file path="lib/supabase/server.ts">
import { createServerClient, type CookieOptions } from "@supabase/ssr";
import { cookies } from "next/headers";

export function createClient() {
  const cookieStore = cookies();

  return createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        get(name: string) {
          return cookieStore.get(name)?.value;
        },
        set(name: string, value: string, options: CookieOptions) {
          try {
            cookieStore.set({ name, value, ...options });
          } catch (error) {
            // The `set` method was called from a Server Component.
            // This can be ignored if you have middleware refreshing
            // user sessions.
          }
        },
        remove(name: string, options: CookieOptions) {
          try {
            cookieStore.set({ name, value: "", ...options });
          } catch (error) {
            // The `delete` method was called from a Server Component.
            // This can be ignored if you have middleware refreshing
            // user sessions.
          }
        },
      },
    }
  );
}
</file>

<file path="lib/openai.ts">
import OpenAI from "openai";

// Create a singleton instance of the OpenAI client
// This ensures we only create one instance of the client throughout the application
let openaiInstance: OpenAI | null = null;

// Initialize OpenAI client with proper error handling
export function getOpenAIInstance(): OpenAI {
  if (openaiInstance) {
    return openaiInstance;
  }

  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    throw new Error(
      "OPENAI_API_KEY is missing from environment variables. Please set it in .env.local or your environment."
    );
  }

  try {
    openaiInstance = new OpenAI({
      apiKey,
    });
    return openaiInstance;
  } catch (error: any) {
    console.error("Failed to initialize OpenAI client:", error);
    throw new Error(`Failed to initialize OpenAI client: ${error.message}`);
  }
}

// Export a ready-to-use instance
export const openai = getOpenAIInstance();
</file>

<file path="lib/supabaseClient.ts">
import { createClient } from "@supabase/supabase-js";

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
const supabaseAnonKey = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY;

if (!supabaseUrl || !supabaseAnonKey) {
  throw new Error("Missing Supabase URL or Anon Key in environment variables");
}

export const supabase = createClient(supabaseUrl, supabaseAnonKey);
</file>

<file path="lib/utils.ts">
/**
 * Extracts a YouTube video ID from a URL.
 * Supports various YouTube URL formats including:
 * - Standard watch URLs: https://www.youtube.com/watch?v=VIDEO_ID
 * - Short URLs: https://youtu.be/VIDEO_ID
 * - Embed URLs: https://www.youtube.com/embed/VIDEO_ID
 * - Shortened URLs with additional parameters
 *
 * @param url - The YouTube URL to extract the video ID from
 * @returns The extracted video ID
 * @throws Error if no valid video ID could be extracted
 */
export function extractVideoId(url: string): string {
  if (!url) {
    throw new Error("No URL provided");
  }

  // List of regex patterns to try for different URL formats
  const patterns = [
    // Standard watch URL: https://www.youtube.com/watch?v=VIDEO_ID
    /(?:youtube\.com\/watch\?v=|youtube\.com\/watch\?.+&v=)([^&]+)/i,

    // Short URL: https://youtu.be/VIDEO_ID
    /(?:youtu\.be\/)([^?&/]+)/i,

    // Embed URL: https://www.youtube.com/embed/VIDEO_ID
    /(?:youtube\.com\/embed\/)([^?&/]+)/i,

    // Mobile URL: https://m.youtube.com/watch?v=VIDEO_ID
    /(?:m\.youtube\.com\/watch\?v=|m\.youtube\.com\/watch\?.+&v=)([^&]+)/i,

    // YouTube Shorts: https://youtube.com/shorts/VIDEO_ID
    /(?:youtube\.com\/shorts\/)([^?&/]+)/i,
  ];

  // Try each pattern until we find a match
  for (const pattern of patterns) {
    const match = url.match(pattern);
    if (match && match[1]) {
      return match[1];
    }
  }

  // If we get here, no pattern matched
  throw new Error(`Could not extract video ID from URL: ${url}`);
}

/**
 * Formats a number for display (e.g., 1234 -> "1,234")
 */
export function formatNumber(num: number | string | null | undefined): string {
  if (num === null || num === undefined || num === "") {
    return "0";
  }

  const numValue = typeof num === "string" ? parseInt(num, 10) : num;

  if (isNaN(numValue)) {
    return "0";
  }

  return new Intl.NumberFormat().format(numValue);
}

/**
 * Formats a date for display
 */
export function formatDate(dateString: string | null | undefined): string {
  if (!dateString) {
    return "Unknown date";
  }

  try {
    const date = new Date(dateString);
    return date.toLocaleDateString("en-US", {
      year: "numeric",
      month: "long",
      day: "numeric",
    });
  } catch (e) {
    return "Invalid date";
  }
}
</file>

<file path="public/create-screenshot.html">
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>App Screenshot Generator</title>
    <style>
      body {
        margin: 0;
        padding: 0;
        font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
          Roboto, "Helvetica Neue", Arial, sans-serif;
      }
      .app-container {
        width: 1200px;
        height: 800px;
        overflow: hidden;
        background-color: #f7f7ff;
        position: relative;
      }
      .app-header {
        background-color: white;
        padding: 20px;
        border-bottom: 1px solid #e5e7eb;
        display: flex;
        align-items: center;
      }
      .app-logo {
        width: 40px;
        height: 40px;
        background-color: #8b5cf6;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        margin-right: 12px;
      }
      .app-title {
        font-weight: 700;
        font-size: 22px;
        color: #8b5cf6;
      }
      .content {
        max-width: 1000px;
        margin: 0 auto;
        padding: 30px 20px;
      }
      .podcast-card {
        background-color: white;
        border-radius: 12px;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        overflow: hidden;
      }
      .podcast-header {
        display: flex;
        padding: 24px;
        border-bottom: 1px solid #e5e7eb;
      }
      .podcast-thumbnail {
        width: 180px;
        height: 100px;
        background-color: #e0e7ff;
        border-radius: 8px;
        margin-right: 20px;
        flex-shrink: 0;
        display: flex;
        align-items: center;
        justify-content: center;
      }
      .podcast-info {
        flex: 1;
      }
      .podcast-title {
        font-size: 20px;
        font-weight: 600;
        margin-bottom: 8px;
        color: #1f2937;
      }
      .podcast-author {
        font-size: 16px;
        color: #6b7280;
        margin-bottom: 12px;
      }
      .podcast-metadata {
        display: flex;
        gap: 16px;
        font-size: 14px;
        color: #9ca3af;
      }
      .tabs {
        display: flex;
        border-bottom: 1px solid #e5e7eb;
      }
      .tab {
        flex: 1;
        padding: 16px;
        text-align: center;
        font-weight: 500;
        cursor: pointer;
      }
      .tab.active {
        color: #8b5cf6;
        border-bottom: 2px solid #8b5cf6;
        background-color: #f5f3ff;
      }
      .tab-content {
        padding: 24px;
      }
      .chat-container {
        display: flex;
        flex-direction: column;
        height: 500px;
      }
      .chat-messages {
        flex: 1;
        overflow-y: auto;
        padding: 16px;
        display: flex;
        flex-direction: column;
        gap: 16px;
      }
      .message-pair {
        display: flex;
        flex-direction: column;
        gap: 16px;
        margin-bottom: 24px;
      }
      .message {
        max-width: 85%;
        border-radius: 12px;
        overflow: hidden;
      }
      .message-avatar {
        width: 36px;
        height: 36px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        margin-right: 12px;
        flex-shrink: 0;
      }
      .user-avatar {
        background-color: #e0e7ff;
        color: #4338ca;
      }
      .ai-avatar {
        background-color: #ede9fe;
        color: #8b5cf6;
      }
      .message-content {
        flex: 1;
      }
      .message-wrapper {
        display: flex;
        align-items: flex-start;
      }
      .user-message .message-content {
        background-color: #f3f4f6;
        padding: 16px;
        border-radius: 12px;
        color: #1f2937;
      }
      .ai-message .message-content {
        background-color: #ede9fe;
        padding: 16px;
        border-radius: 12px;
        color: #1f2937;
      }
      .chat-input {
        display: flex;
        padding: 16px;
        border-top: 1px solid #e5e7eb;
      }
      .chat-input input {
        flex: 1;
        padding: 12px 16px;
        border: 1px solid #d1d5db;
        border-radius: 8px;
        margin-right: 12px;
        font-size: 14px;
      }
      .chat-input button {
        padding: 0 16px;
        background-color: #8b5cf6;
        color: white;
        border: none;
        border-radius: 8px;
        font-weight: 500;
      }
      .message-list {
        margin: 0;
        padding-left: 20px;
      }
      .message-list li {
        margin-bottom: 10px;
      }
    </style>
  </head>
  <body>
    <div class="app-container">
      <div class="app-header">
        <div class="app-logo">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="white"
          >
            <path
              d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z"
            />
            <path
              d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z"
            />
          </svg>
        </div>
        <div class="app-title">PodAI</div>
      </div>

      <div class="content">
        <div class="podcast-card">
          <div class="podcast-header">
            <div class="podcast-thumbnail">
              <svg
                xmlns="http://www.w3.org/2000/svg"
                width="32"
                height="32"
                viewBox="0 0 24 24"
                fill="#8b5cf6"
              >
                <path
                  d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z"
                />
                <path
                  d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z"
                />
              </svg>
            </div>
            <div class="podcast-info">
              <div class="podcast-title">
                The AI Economy: Future Trends with Dr. Sophia Chen
              </div>
              <div class="podcast-author">Economics Insights Podcast</div>
              <div class="podcast-metadata">
                <span>58:45</span>
                <span>1.2M views</span>
                <span>87K likes</span>
                <span>June 10, 2023</span>
              </div>
            </div>
          </div>

          <div class="tabs">
            <div class="tab">Summary</div>
            <div class="tab active">Chat Assistant</div>
          </div>

          <div class="tab-content">
            <div class="chat-container">
              <div class="chat-messages">
                <div class="message-pair">
                  <div class="message-wrapper user-message">
                    <div class="message-avatar user-avatar">
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="20"
                        height="20"
                        viewBox="0 0 24 24"
                        fill="currentColor"
                      >
                        <path
                          d="M18.685 19.097A9.723 9.723 0 0021.75 12c0-5.385-4.365-9.75-9.75-9.75S2.25 6.615 2.25 12a9.723 9.723 0 003.065 7.097A9.716 9.716 0 0012 21.75a9.716 9.716 0 006.685-2.653zm-12.54-1.285A7.486 7.486 0 0112 15a7.486 7.486 0 015.855 2.812A8.224 8.224 0 0112 20.25a8.224 8.224 0 01-5.855-2.438zM15.75 9a3.75 3.75 0 11-7.5 0 3.75 3.75 0 017.5 0z"
                        />
                      </svg>
                    </div>
                    <div class="message-content">
                      What were the main economic impacts discussed in the
                      podcast?
                    </div>
                  </div>

                  <div class="message-wrapper ai-message">
                    <div class="message-avatar ai-avatar">
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="20"
                        height="20"
                        viewBox="0 0 24 24"
                        fill="currentColor"
                      >
                        <path
                          d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z"
                        />
                        <path
                          d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z"
                        />
                      </svg>
                    </div>
                    <div class="message-content">
                      <p>
                        The podcast highlighted three main economic impacts:
                      </p>
                      <ol class="message-list">
                        <li>
                          AI automation could increase global GDP by up to 14%
                          by 2030
                        </li>
                        <li>
                          Job market disruption with an estimated 15% of jobs
                          being redefined
                        </li>
                        <li>
                          New industries emerging that could create 97 million
                          new roles
                        </li>
                      </ol>
                      <p>
                        Dr. Chen specifically emphasized the need for reskilling
                        programs as this transition occurs.
                      </p>
                    </div>
                  </div>
                </div>

                <div class="message-wrapper user-message">
                  <div class="message-avatar user-avatar">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      width="20"
                      height="20"
                      viewBox="0 0 24 24"
                      fill="currentColor"
                    >
                      <path
                        d="M18.685 19.097A9.723 9.723 0 0021.75 12c0-5.385-4.365-9.75-9.75-9.75S2.25 6.615 2.25 12a9.723 9.723 0 003.065 7.097A9.716 9.716 0 0012 21.75a9.716 9.716 0 006.685-2.653zm-12.54-1.285A7.486 7.486 0 0112 15a7.486 7.486 0 015.855 2.812A8.224 8.224 0 0112 20.25a8.224 8.224 0 01-5.855-2.438zM15.75 9a3.75 3.75 0 11-7.5 0 3.75 3.75 0 017.5 0z"
                      />
                    </svg>
                  </div>
                  <div class="message-content">
                    What sectors did she mention would be most affected?
                  </div>
                </div>
              </div>

              <div class="chat-input">
                <input
                  type="text"
                  placeholder="Ask a question about the podcast content..."
                />
                <button>
                  <svg
                    xmlns="http://www.w3.org/2000/svg"
                    width="20"
                    height="20"
                    viewBox="0 0 24 24"
                    fill="none"
                    stroke="currentColor"
                    stroke-width="2"
                    stroke-linecap="round"
                    stroke-linejoin="round"
                  >
                    <line x1="22" y1="2" x2="11" y2="13"></line>
                    <polygon points="22 2 15 22 11 13 2 9 22 2"></polygon>
                  </svg>
                </button>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Instructions -->
    <div style="margin: 20px; font-family: sans-serif">
      <h1>App Screenshot Generator</h1>
      <p>
        Take a screenshot of the above app interface and save it as
        "app-screenshot.png" in the public directory.
      </p>
      <p>
        On macOS: Press Command + Shift + 4, then select the app interface area.
      </p>
      <p>
        On Windows: Use the Snipping Tool or press Windows + Shift + S to
        capture the interface.
      </p>
    </div>
  </body>
</html>
</file>

<file path="public/README.md">
# App Screenshot Instructions

This directory contains a file `create-screenshot.html` that can be used to generate a screenshot for the landing page.

## How to generate the screenshot:

1. Open the `create-screenshot.html` file in a web browser
2. Take a screenshot of the application interface (not including the instructions)
3. Save the screenshot as `app-screenshot.png` in this directory (public/)

## Taking a screenshot on different operating systems:

- **macOS**: Press `Command + Shift + 4`, then select the area you want to capture
- **Windows**: Use the Snipping Tool or press `Windows + Shift + S` to capture a specific area
- **Linux**: Use a tool like GNOME Screenshot or press `PrtScn` key

The screenshot will be used on the landing page to show visitors what the application looks like.
</file>

<file path=".gitignore">
# dependencies
/node_modules
/.pnp
.pnp.js

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# local env files
.env*.local

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

# IDE
.idea/
.vscode/
*.swp
*.swo
</file>

<file path="middleware.ts">
import { createServerClient, type CookieOptions } from "@supabase/ssr";
import { NextResponse, type NextRequest } from "next/server";

export async function middleware(request: NextRequest) {
  let response = NextResponse.next({
    request: {
      headers: request.headers,
    },
  });

  const supabase = createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        get(name: string) {
          return request.cookies.get(name)?.value;
        },
        set(name: string, value: string, options: CookieOptions) {
          request.cookies.set({
            name,
            value,
            ...options,
          });
          response = NextResponse.next({
            request: {
              headers: request.headers,
            },
          });
          response.cookies.set({
            name,
            value,
            ...options,
          });
        },
        remove(name: string, options: CookieOptions) {
          request.cookies.set({
            name,
            value: "",
            ...options,
          });
          response = NextResponse.next({
            request: {
              headers: request.headers,
            },
          });
          response.cookies.set({
            name,
            value: "",
            ...options,
          });
        },
      },
    }
  );

  const {
    data: { user },
    error,
  } = await supabase.auth.getUser();

  if (error && error.message !== "Auth session missing!") {
    console.error("[Middleware] Error getting user:", error.message);
  }

  // Protect the /dashboard route
  if (!user && request.nextUrl.pathname.startsWith("/dashboard")) {
    return NextResponse.redirect(new URL("/", request.url));
  }

  // Redirect authenticated users from the landing page to the dashboard
  if (user && request.nextUrl.pathname === "/") {
    return NextResponse.redirect(new URL("/dashboard", request.url));
  }

  return response;
}

export const config = {
  matcher: [
    /*
     * Match all request paths except for the ones starting with:
     * - _next/static (static files)
     * - _next/image (image optimization files)
     * - favicon.ico (favicon file)
     * Feel free to modify this pattern to include more paths.
     */
    "/((?!_next/static|_next/image|favicon.ico|.*\\.(?:svg|png|jpg|jpeg|gif|webp)$).*)",
  ],
};
</file>

<file path="postcss.config.js">
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};
</file>

<file path="supabaseplan.md">
# Supabase Google Authentication Implementation Plan

## 1. Set up Supabase project

- [x] Create a new Supabase project (Used existing: PodAI, ID: wjkqshfnsqkmhmqsiaol)
- [ ] Configure database schema for user authentication (Using default `auth.users` for now)
- [ ] Set up Row Level Security (RLS) policies (Will implement when user-specific tables are added)

## 2. Configure Google OAuth credentials

- [x] Create a Google Cloud project
- [x] Set up OAuth consent screen
- [x] Generate OAuth client ID and secret
- [x] Add authorized redirect URIs for your application (`https://wjkqshfnsqkmhmqsiaol.supabase.co/auth/v1/callback`)

## 3. Configure Supabase Auth with Google provider

- [x] Add Google OAuth credentials to Supabase Auth settings
- [x] Configure Supabase redirect URLs (Verified match)

## 4. Implement frontend authentication flow

- [x] Install Supabase client library (`@supabase/ssr` and `@supabase/supabase-js`)
- [x] Create authentication components (Updated `Navbar.tsx` & `app/page.tsx` with state, sign-in/out)
- [x] Implement sign-in, sign-out, and session management (Done via `Navbar.tsx`, `app/page.tsx` and `middleware.ts`)
- [x] Set up protected routes for authenticated users (Done via `middleware.ts`)

## 5. Update application to use authentication context

- [x] Modify existing components to respect authentication state (`app/page.tsx`, `middleware.ts`)
- [x] Ensure dashboard is only accessible to authenticated users (Done via `middleware.ts`)
- [x] Redirect unauthenticated users to landing page (Done via `middleware.ts`)

## 6. Test authentication flow

- [x] Verify sign-in process works correctly
- [x] Test session persistence
- [x] Ensure protected routes are properly secured
</file>

<file path="test-package.json">
{
  "name": "transcript-test",
  "version": "1.0.0",
  "description": "Test script for YouTube transcript fetching",
  "main": "test-transcript.js",
  "type": "module",
  "scripts": {
    "test": "node test-transcript.js"
  }
}
</file>

<file path="test-transcript.js">
// Test script for transcript fetching
import { fetchTranscript } from "./lib/youtube-transcript.js";

async function testTranscriptFetch() {
  const videoId = "d3dPRkyNbj8"; // The video ID that was failing

  console.log(`Testing transcript fetch for video ID: ${videoId}`);

  try {
    const transcript = await fetchTranscript(videoId);
    console.log(`Success! Fetched ${transcript.length} transcript segments`);
    // Print the first few segments
    console.log("First 3 segments:");
    transcript.slice(0, 3).forEach((segment, i) => {
      console.log(`[${i}] ${segment.offset}s: ${segment.text}`);
    });
    return true;
  } catch (error) {
    console.error(`Failed to fetch transcript: ${error.message}`);
    return false;
  }
}

// Run the test
testTranscriptFetch().then((success) => {
  console.log(`Test ${success ? "PASSED" : "FAILED"}`);
});
</file>

<file path="app/api/podcast-metadata/route.ts">
import { NextRequest, NextResponse } from "next/server";
import { fetchMetadataFromYouTubeAPI } from "@/lib/youtube"; // Import the shared function
import { PodcastMetadata } from "@/components/PodcastMetadata"; // Use correct import path

// Define a simple type for oEmbed response
interface OEmbedResponse {
  title?: string;
  author_name?: string;
  thumbnail_url?: string;
  // Add other fields if needed (e.g., width, height for thumbnail)
}

// Simple oEmbed fetch function - returns only basic fields matching PodcastMetadata structure
async function fetchOEmbedMetadata(
  videoId: string
): Promise<Partial<PodcastMetadata> | null> {
  const url = `https://www.youtube.com/oembed?url=https://www.youtube.com/watch?v=${videoId}&format=json`;
  try {
    console.log(`Attempting oEmbed fallback for video ID: ${videoId}`);
    const response = await fetch(url, {
      next: { revalidate: 3600 },
      signal: AbortSignal.timeout(8000),
    }); // Cache & 8s timeout
    if (!response.ok) {
      console.error(
        `oEmbed request failed with status ${response.status} ${response.statusText}`
      );
      return null;
    }
    const data: OEmbedResponse = await response.json();

    // Map oEmbed response to PodcastMetadata structure
    const metadata: Partial<PodcastMetadata> = {
      videoId: videoId,
      title: data.title || "YouTube Video (oEmbed)",
      channelName: data.author_name || "Unknown Channel (oEmbed)",
      // Construct a basic thumbnail object if URL exists
      thumbnails: data.thumbnail_url
        ? { default: { url: data.thumbnail_url, width: 0, height: 0 } }
        : null,
      duration: "0:00", // oEmbed doesn't provide duration
      description: "Description unavailable via oEmbed.",
      fullDescription: "Description unavailable via oEmbed.",
      descriptionTruncated: false,
      // Other fields like viewCount, likeCount, publishedAt are not available via oEmbed
    };
    console.log(
      `Successfully fetched partial metadata via oEmbed fallback for video ID: ${videoId}`
    );
    return metadata;
  } catch (error: any) {
    if (error.name === "AbortError") {
      console.error(`oEmbed request timed out for video ID ${videoId}.`);
    } else {
      console.error(
        `Error fetching oEmbed metadata for video ID ${videoId}:`,
        error.message || error
      );
    }
    return null;
  }
}

export async function POST(request: NextRequest) {
  let videoId = ""; // For logging scope
  try {
    // includeFull is no longer needed as API function gets full description anyway
    const { url } = await request.json();

    // Extract video ID from URL
    const videoIdMatch = url.match(
      /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
    );

    if (!videoIdMatch) {
      return NextResponse.json(
        { error: "Could not extract video ID from URL" },
        { status: 400 }
      );
    }

    videoId = videoIdMatch[1];
    console.log(`[${videoId}] Received request for metadata.`);

    // Attempt to fetch metadata using the shared YouTube API function
    let metadata = await fetchMetadataFromYouTubeAPI(videoId);

    // If API fetch fails or returns null, attempt oEmbed fallback
    if (!metadata) {
      console.warn(
        `[${videoId}] YouTube API metadata fetch failed or returned null, attempting oEmbed fallback.`
      );
      metadata = await fetchOEmbedMetadata(videoId);
    }

    // If both methods fail, return an error
    if (!metadata) {
      console.error(`[${videoId}] All methods failed to fetch metadata.`);
      // Return a more specific error message
      return NextResponse.json(
        {
          error: `Failed to fetch podcast metadata for video ${videoId} using all available methods.`,
        },
        { status: 500 }
      );
    }

    // Return the successful metadata (either from API or oEmbed)
    console.log(`[${videoId}] Successfully returning metadata.`);
    // Ensure the returned object matches the expected structure (even if partial)
    return NextResponse.json({ metadata: metadata as PodcastMetadata }); // Cast to full type if confident, otherwise handle partial data downstream
  } catch (error: any) {
    // Catch potential errors during request parsing or ID extraction
    const idSuffix = videoId ? ` for video ${videoId}` : "";
    console.error(
      `Error processing podcast metadata request${idSuffix}:`,
      error.message || error
    );
    return NextResponse.json(
      { error: `Internal server error processing metadata request${idSuffix}` },
      { status: 500 }
    );
  }
}
</file>

<file path="app/layout.tsx">
import type { Metadata } from "next";
import { Poppins } from "next/font/google";
import "./globals.css";
import ErrorBoundary from "../components/ErrorBoundary";

const poppins = Poppins({
  weight: ["400", "500", "600", "700"],
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "YouTube AI Podcast Assistant",
  description: "Summarize and chat with YouTube podcasts using AI",
  keywords: "YouTube, podcast, AI, summarizer, chat, assistant, transcript",
  authors: [{ name: "Your Name" }],
  viewport: "width=device-width, initial-scale=1",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" suppressHydrationWarning>
      <body className={poppins.className}>
        <ErrorBoundary>{children}</ErrorBoundary>
      </body>
    </html>
  );
}
</file>

<file path="app/page.tsx">
"use client";

import React, { useEffect, useState } from "react";
import { useRouter } from "next/navigation";
import Link from "next/link";
import Image from "next/image";
import { createClient } from "@/lib/supabase/client";

// NoSSR wrapper component to prevent hydration mismatches
function NoSSR({ children }: { children: React.ReactNode }) {
  const [mounted, setMounted] = useState(false);

  useEffect(() => {
    setMounted(true);
  }, []);

  return mounted ? <>{children}</> : null;
}

export default function LandingPage() {
  const router = useRouter();
  const [mounted, setMounted] = useState(false);
  const [loading, setLoading] = useState(false);
  const supabase = createClient();

  useEffect(() => {
    setMounted(true);
  }, []);

  const handleNavigation = (path: string) => {
    if (mounted) {
      router.push(path);
    }
  };

  const handleSignIn = async () => {
    setLoading(true);
    const { error } = await supabase.auth.signInWithOAuth({
      provider: "google",
      options: {
        redirectTo: `${window.location.origin}/auth/callback`,
      },
    });
    if (error) {
      console.error("Error signing in:", error.message);
    }
  };

  return (
    <div className="min-h-screen bg-[#f7f7ff]">
      {/* Navigation */}
      <nav className="bg-white shadow-sm">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
          <div className="flex justify-between h-16">
            <div className="flex items-center">
              <div className="text-purple-700 font-bold text-xl">
                YouTube AI Podcast Assistant
              </div>
            </div>
            <div className="flex items-center space-x-4">
              <button
                onClick={handleSignIn}
                disabled={loading}
                className="px-4 py-2 bg-purple-600 text-white rounded-lg hover:bg-purple-700 disabled:opacity-50"
              >
                {loading ? "Signing In..." : "Sign In"}
              </button>
            </div>
          </div>
        </div>
      </nav>

      {/* Hero Section - Two Column Layout */}
      <div className="max-w-7xl mx-auto px-4 pt-16 pb-12 sm:pt-24 sm:pb-20">
        <div className="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
          {/* Left Column - Text Content */}
          <div className="text-left">
            <h1 className="text-4xl md:text-5xl lg:text-6xl font-bold text-purple-700 mb-6">
              Transform Podcasts into Interactive Conversations
            </h1>
            <p className="text-lg text-gray-600 mb-10">
              Paste any podcast URL and let AI create summaries and engage in
              meaningful conversations about the content
            </p>
            <div className="flex flex-col sm:flex-row gap-4 sm:gap-6">
              <button
                onClick={handleSignIn}
                disabled={loading}
                className="px-8 py-3 bg-purple-600 text-white rounded-lg hover:bg-purple-700 text-lg font-medium disabled:opacity-50"
              >
                {loading ? "Redirecting..." : "Get Started"}
              </button>
              <a
                href="#how-it-works"
                className="px-8 py-3 bg-purple-100 text-purple-700 rounded-lg hover:bg-purple-200 text-lg font-medium text-center"
              >
                Learn More
              </a>
            </div>
          </div>

          {/* Right Column - Screenshot */}
          <div className="relative flex justify-center md:justify-end">
            <div
              className="relative w-full rounded-xl shadow-2xl overflow-hidden border-8 border-white"
              style={{ maxHeight: "80vh" }}
            >
              <div
                style={{
                  position: "relative",
                  width: "100%",
                  paddingTop: "64.3%",
                }}
              >
                <NoSSR>
                  <Image
                    src="/app-screenshot-new.png"
                    alt="AI chat interface analyzing economic impacts from a podcast"
                    fill
                    sizes="(max-width: 768px) 100vw, 50vw"
                    priority
                    className="object-cover rounded-lg"
                    style={{ objectFit: "cover" }}
                  />
                </NoSSR>
              </div>
            </div>
          </div>
        </div>
      </div>

      {/* Feature Cards Section */}
      <div className="bg-white py-16">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
          <div className="text-center mb-12">
            <h2 className="text-3xl font-bold text-gray-900">Key Features</h2>
            <p className="mt-4 text-lg text-gray-600">
              Everything you need to get more from your podcast listening
              experience
            </p>
          </div>
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-8">
            {/* Smart Summaries Card */}
            <div className="bg-[#f7f7ff] p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M12 6.042A8.967 8.967 0 006 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 016 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 016-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0018 18a8.967 8.967 0 00-6 2.292m0-14.25v14.25" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Smart Summaries
              </h3>
              <p className="text-gray-600">
                Get comprehensive summaries that capture the key points and
                insights from any podcast.
              </p>
            </div>

            {/* Interactive Chat Card */}
            <div className="bg-[#f7f7ff] p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M4.913 2.658c2.075-.27 4.19-.408 6.337-.408 2.147 0 4.262.139 6.337.408 1.922.25 3.291 1.861 3.405 3.727a4.403 4.403 0 00-1.032-.211 50.89 50.89 0 00-8.42 0c-2.358.196-4.04 2.19-4.04 4.434v4.286a4.47 4.47 0 002.433 3.984L7.28 21.53A.75.75 0 016 21v-4.03a48.527 48.527 0 01-1.087-.128C2.905 16.58 1.5 14.833 1.5 12.862V6.638c0-1.97 1.405-3.718 3.413-3.979z" />
                  <path d="M15.75 7.5c-1.376 0-2.739.057-4.086.169C10.124 7.797 9 9.103 9 10.609v4.285c0 1.507 1.128 2.814 2.67 2.94 1.243.102 2.5.157 3.768.165l2.782 2.781a.75.75 0 001.28-.53v-2.39l.33-.026c1.542-.125 2.67-1.433 2.67-2.94v-4.286c0-1.505-1.125-2.811-2.664-2.94A49.392 49.392 0 0015.75 7.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Interactive Chat
              </h3>
              <p className="text-gray-600">
                Ask questions about the podcast content and get detailed answers
                from our AI assistant.
              </p>
            </div>

            {/* Works Everywhere Card */}
            <div className="bg-[#f7f7ff] p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
                  <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Works Everywhere
              </h3>
              <p className="text-gray-600">
                Compatible with podcasts from YouTube and many more platforms.
              </p>
            </div>

            {/* AI-Powered Insights Card */}
            <div className="bg-[#f7f7ff] p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M21.731 2.269a2.625 2.625 0 00-3.712 0l-1.157 1.157 3.712 3.712 1.157-1.157a2.625 2.625 0 000-3.712zM19.513 8.199l-3.712-3.712-8.4 8.4a5.25 5.25 0 00-1.32 2.214l-.8 2.685a.75.75 0 00.933.933l2.685-.8a5.25 5.25 0 002.214-1.32l8.4-8.4z" />
                  <path d="M5.25 5.25a3 3 0 00-3 3v10.5a3 3 0 003 3h10.5a3 3 0 003-3V13.5a.75.75 0 00-1.5 0v5.25a1.5 1.5 0 01-1.5 1.5H5.25a1.5 1.5 0 01-1.5-1.5V8.25a1.5 1.5 0 011.5-1.5h5.25a.75.75 0 000-1.5H5.25z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                AI-Powered Insights
              </h3>
              <p className="text-gray-600">
                Advanced AI analyzes podcasts to extract the most relevant
                information and context.
              </p>
            </div>
          </div>
        </div>
      </div>

      {/* How It Works Section */}
      <div id="how-it-works" className="py-16">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
          <div className="text-center mb-12">
            <h2 className="text-3xl font-bold text-gray-900">How It Works</h2>
            <p className="mt-4 text-lg text-gray-600">
              Simple process, powerful results
            </p>
          </div>
          <div className="grid grid-cols-1 md:grid-cols-3 gap-8">
            {/* Step 1 */}
            <div className="text-center">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5 mx-auto">
                <span className="text-2xl font-bold text-purple-600">1</span>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Paste YouTube URL
              </h3>
              <p className="text-gray-600">
                Simply paste the URL of any YouTube podcast you want to analyze
              </p>
            </div>

            {/* Step 2 */}
            <div className="text-center">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5 mx-auto">
                <span className="text-2xl font-bold text-purple-600">2</span>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                AI Processes the Content
              </h3>
              <p className="text-gray-600">
                Our advanced AI analyzes the audio transcript to extract key
                information
              </p>
            </div>

            {/* Step 3 */}
            <div className="text-center">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5 mx-auto">
                <span className="text-2xl font-bold text-purple-600">3</span>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Interact with Results
              </h3>
              <p className="text-gray-600">
                Get a comprehensive summary or chat with the AI about specific
                content
              </p>
            </div>
          </div>
        </div>
      </div>

      {/* CTA Section */}
      <div className="bg-purple-700 py-16">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
          <h2 className="text-3xl font-bold text-white mb-4">
            Ready to Unlock Podcast Insights?
          </h2>
          <p className="text-lg text-purple-200 mb-8">
            Start summarizing and chatting with your favorite podcasts today.
          </p>
          <button
            onClick={handleSignIn}
            disabled={loading}
            className="px-8 py-3 bg-white text-purple-700 rounded-lg hover:bg-gray-100 text-lg font-medium disabled:opacity-50"
          >
            {loading ? "Redirecting..." : "Get Started Now"}
          </button>
        </div>
      </div>

      {/* Footer */}
      <footer className="bg-[#f7f7ff] py-8">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center text-gray-500">
          &copy; {new Date().getFullYear()} YouTube AI Podcast Assistant. All
          rights reserved.
        </div>
      </footer>
    </div>
  );
}
</file>

<file path="components/Chat.tsx">
import React, { useState, useRef, useEffect } from "react";
import ReactMarkdown from "react-markdown";
import remarkGfm from "remark-gfm";
import { toast } from "react-hot-toast";
import { PaperAirplaneIcon } from "@heroicons/react/24/outline";

interface Message {
  role: "user" | "assistant";
  content: string;
  timestamp?: Date;
}

interface ChatProps {
  videoUrl: string;
}

const Chat: React.FC<ChatProps> = ({ videoUrl }) => {
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState<Message[]>([]);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const inputRef = useRef<HTMLInputElement>(null);

  // Scroll to bottom of chat when messages change
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [messages]);

  // Focus input field when component mounts
  useEffect(() => {
    if (!loading && inputRef.current) {
      inputRef.current.focus();
    }
  }, [loading]);

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!input.trim() || loading) return;

    const userMessage: Message = {
      role: "user",
      content: input,
      timestamp: new Date(),
    };
    setMessages((prev) => [...prev, userMessage]);
    setInput("");
    setError(null);
    setLoading(true);

    try {
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          url: videoUrl,
          question: input,
          chatHistory: messages.map(({ role, content }) => ({ role, content })),
        }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || "Failed to get response");
      }

      const assistantMessage: Message = {
        role: "assistant",
        content: data.answer,
        timestamp: new Date(),
      };
      setMessages((prev) => [...prev, assistantMessage]);
    } catch (err) {
      console.error("Error:", err);
      const errorMessage =
        err instanceof Error ? err.message : "Failed to get response";
      setError(errorMessage);
      toast.error(`Error: ${errorMessage}`);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="bg-white rounded-lg flex flex-col h-full">
      <div className="flex-1 flex flex-col">
        <div
          className="flex-1 overflow-y-auto p-4 space-y-4 bg-gray-50"
          style={{ maxHeight: "calc(70vh - 130px)" }}
          aria-live="polite"
        >
          {messages.length === 0 ? (
            <div className="text-center text-gray-500 my-8 bg-white p-6 rounded-lg">
              <p className="font-medium text-gray-700 mb-3">
                Ask any question about this podcast
              </p>
              <p className="text-sm mb-3">For example:</p>
              <div className="grid grid-cols-1 md:grid-cols-2 gap-2">
                {[
                  "What were the main topics discussed?",
                  "Summarize the key points about [specific topic]",
                  "What did the speaker say about [specific concept]?",
                  "What were the most interesting insights shared?",
                ].map((suggestion, index) => (
                  <button
                    key={index}
                    onClick={() => {
                      setInput(suggestion);
                      if (inputRef.current) inputRef.current.focus();
                    }}
                    className="text-sm text-left p-2 bg-purple-50 text-purple-700 rounded-md hover:bg-purple-100 transition-colors"
                  >
                    {suggestion}
                  </button>
                ))}
              </div>
            </div>
          ) : (
            messages.map((message, index) => (
              <div
                key={index}
                className={`flex ${
                  message.role === "user" ? "justify-end" : "justify-start"
                }`}
              >
                <div
                  className={`max-w-[80%] rounded-lg p-4 ${
                    message.role === "user"
                      ? "bg-purple-100 text-purple-900"
                      : "bg-white text-gray-800 border border-gray-100"
                  }`}
                >
                  <div className="prose prose-sm max-w-none">
                    <ReactMarkdown remarkPlugins={[remarkGfm]}>
                      {message.content}
                    </ReactMarkdown>
                  </div>
                  {message.timestamp && (
                    <div className="text-xs text-gray-500 mt-2 text-right">
                      {message.timestamp.toLocaleTimeString([], {
                        hour: "2-digit",
                        minute: "2-digit",
                      })}
                    </div>
                  )}
                </div>
              </div>
            ))
          )}
          {loading && (
            <div className="flex justify-start">
              <div className="max-w-[80%] rounded-lg p-4 bg-white border border-gray-100">
                <div className="flex items-center space-x-2">
                  <div
                    className="w-2 h-2 rounded-full bg-purple-400 animate-bounce"
                    style={{ animationDelay: "0ms" }}
                  ></div>
                  <div
                    className="w-2 h-2 rounded-full bg-purple-500 animate-bounce"
                    style={{ animationDelay: "150ms" }}
                  ></div>
                  <div
                    className="w-2 h-2 rounded-full bg-purple-600 animate-bounce"
                    style={{ animationDelay: "300ms" }}
                  ></div>
                </div>
              </div>
            </div>
          )}
          {error && (
            <div className="text-red-600 text-sm p-3 bg-red-50 rounded border border-red-200">
              Error: {error}
            </div>
          )}
          <div ref={messagesEndRef} />
        </div>
        <form onSubmit={handleSubmit} className="p-4 bg-white">
          <div className="flex items-center">
            <input
              type="text"
              ref={inputRef}
              value={input}
              onChange={(e) => setInput(e.target.value)}
              placeholder="Ask a question about this podcast..."
              className="flex-1 p-3 border border-gray-300 rounded-l-lg focus:outline-none focus:ring-2 focus:ring-purple-500"
              disabled={loading}
              aria-label="Your question"
            />
            <button
              type="submit"
              disabled={loading || !input.trim()}
              className="px-4 py-3 bg-purple-600 text-white rounded-r-lg hover:bg-purple-700 disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center"
              aria-label={loading ? "Sending..." : "Send message"}
            >
              {loading ? (
                <div className="flex items-center space-x-1">
                  <div className="w-1.5 h-1.5 bg-white rounded-full animate-pulse"></div>
                  <div
                    className="w-1.5 h-1.5 bg-white rounded-full animate-pulse"
                    style={{ animationDelay: "150ms" }}
                  ></div>
                  <div
                    className="w-1.5 h-1.5 bg-white rounded-full animate-pulse"
                    style={{ animationDelay: "300ms" }}
                  ></div>
                </div>
              ) : (
                <PaperAirplaneIcon className="h-5 w-5" />
              )}
            </button>
          </div>
        </form>
      </div>
    </div>
  );
};

export default Chat;
</file>

<file path="components/Navbar.tsx">
"use client";

import React, { useState, useEffect } from "react";
import Link from "next/link";
import { usePathname, useRouter } from "next/navigation";
import { createClient } from "@/lib/supabase/client";
import { Session } from "@supabase/supabase-js";

const Navbar = () => {
  const pathname = usePathname();
  const router = useRouter();
  const [session, setSession] = useState<Session | null>(null);
  const [loading, setLoading] = useState(true);
  const supabase = createClient();

  useEffect(() => {
    supabase.auth.getSession().then(({ data: { session } }) => {
      setSession(session);
      setLoading(false);
    });

    const {
      data: { subscription },
    } = supabase.auth.onAuthStateChange((_event, session) => {
      setSession(session);
      setLoading(false);
    });

    return () => subscription?.unsubscribe();
  }, [supabase.auth]);

  const handleSignIn = async () => {
    setLoading(true);
    const { error } = await supabase.auth.signInWithOAuth({
      provider: "google",
      options: {
        redirectTo: `${window.location.origin}/auth/callback`,
      },
    });
    if (error) {
      console.error("Error signing in:", error.message);
      setLoading(false);
    }
  };

  const handleSignOut = async () => {
    setLoading(true);
    const { error } = await supabase.auth.signOut();
    if (error) {
      console.error("Error signing out:", error.message);
    } else {
      setSession(null);
      router.push("/");
    }
    setLoading(false);
  };

  return (
    <nav className="flex items-center justify-between w-full max-w-7xl mx-auto px-4 py-6">
      <Link href="/" className="flex items-center">
        <div className="w-10 h-10 bg-purple-600 rounded-full flex items-center justify-center mr-2">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            viewBox="0 0 24 24"
            fill="white"
            className="w-6 h-6"
          >
            <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
            <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
          </svg>
        </div>
        <span className="text-2xl font-bold text-purple-600">PodAI</span>
      </Link>
      <div className="flex items-center space-x-4">
        {loading ? (
          <div className="text-gray-500">Loading...</div>
        ) : session ? (
          <>
            {pathname === "/dashboard" ? (
              <Link
                href="/"
                className="text-gray-600 hover:text-purple-600 transition-colors"
              >
                Home
              </Link>
            ) : (
              <Link
                href="/dashboard"
                className="text-gray-600 hover:text-purple-600 transition-colors"
              >
                Dashboard
              </Link>
            )}
            <Link
              href="/validate-transcript"
              className="text-gray-600 hover:text-purple-600 transition-colors"
            >
              Validate Transcript
            </Link>
            <span className="text-sm text-gray-600 hidden sm:inline">
              {session.user.email}
            </span>
            <button
              onClick={handleSignOut}
              className="px-4 py-2 border border-purple-600 text-purple-600 rounded-lg hover:bg-purple-50 transition-colors"
            >
              Sign Out
            </button>
          </>
        ) : (
          <>
            <Link
              href="/validate-transcript"
              className="text-gray-600 hover:text-purple-600 transition-colors mr-4"
            >
              Validate Transcript
            </Link>
            <button
              onClick={handleSignIn}
              className="px-4 py-2 bg-purple-600 text-white rounded-lg hover:bg-purple-700 transition-colors flex items-center space-x-2"
            >
              <span>Sign In with Google</span>
            </button>
          </>
        )}
      </div>
    </nav>
  );
};

export default Navbar;
</file>

<file path="components/Summary.tsx">
import React, { useEffect, useState } from "react";
import ReactMarkdown from "react-markdown";
import remarkGfm from "remark-gfm";
import { toast } from "react-hot-toast";
import { DocumentDuplicateIcon } from "@heroicons/react/24/outline";
import { PodcastMetadata } from "./PodcastMetadata";

interface SummaryProps {
  summary: string;
  videoUrl: string;
}

const Summary: React.FC<SummaryProps> = ({ summary, videoUrl }) => {
  const [metadata, setMetadata] = useState<PodcastMetadata | null>(null);
  const [loading, setLoading] = useState(true);

  // Fetch metadata if it's not already in the summary
  useEffect(() => {
    const fetchMetadata = async () => {
      if (!videoUrl) return;

      try {
        setLoading(true);
        const response = await fetch("/api/podcast-metadata", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify({ url: videoUrl }),
        });

        if (response.ok) {
          const data = await response.json();
          setMetadata(data.metadata);
        }
      } catch (error) {
        console.error("Error fetching metadata:", error);
      } finally {
        setLoading(false);
      }
    };

    fetchMetadata();
  }, [videoUrl]);

  const handleCopy = async () => {
    try {
      // Include metadata in the copied content if available
      let fullContent = "";

      if (metadata) {
        fullContent += `# ${metadata.title}\n`;
        fullContent += `Channel: ${metadata.channelName}\n`;
        fullContent += `Duration: ${metadata.duration}\n`;

        // Add published date if available
        if (metadata.publishedAt) {
          const date = new Date(metadata.publishedAt);
          fullContent += `Published: ${date.toLocaleDateString()}\n`;
        }

        // Add view count if available
        if (metadata.viewCount) {
          fullContent += `Views: ${parseInt(
            metadata.viewCount
          ).toLocaleString()}\n`;
        }

        // Add like count if available
        if (metadata.likeCount) {
          fullContent += `Likes: ${parseInt(
            metadata.likeCount
          ).toLocaleString()}\n`;
        }

        fullContent += "\n";
      }

      fullContent += summary;

      await navigator.clipboard.writeText(fullContent);
      toast.success("Summary copied to clipboard!");
    } catch (error) {
      console.error("Failed to copy:", error);
      toast.error("Failed to copy to clipboard. Please try again.");
    }
  };

  return (
    <div className="bg-white rounded-lg">
      <div className="flex justify-end p-2">
        <button
          onClick={handleCopy}
          className="flex items-center px-3 py-1.5 text-sm bg-gray-100 text-gray-700 rounded-lg hover:bg-gray-200 transition-colors"
          aria-label="Copy summary to clipboard"
        >
          <DocumentDuplicateIcon className="h-4 w-4 mr-1.5" />
          Copy
        </button>
      </div>

      <div className="px-4 pb-4 overflow-y-auto max-h-[70vh]">
        {/* Metadata section has been removed */}

        <div className="w-full max-w-full overflow-hidden markdown-content bg-gray-50 border border-gray-200 rounded-lg p-6 py-0">
          <div className="prose prose-lg !max-w-full !w-full prose-headings:text-purple-700 prose-h1:text-2xl prose-h2:text-xl prose-h2:mt-8 prose-h2:mb-4 prose-h2:pb-2 prose-h2:border-b prose-h2:border-gray-200 prose-h3:text-lg prose-h3:text-gray-700 prose-h3:mt-6 prose-p:text-gray-600 prose-p:my-4 prose-p:leading-relaxed prose-ul:my-4 prose-ol:my-4 prose-li:my-1.5 prose-li:text-gray-600 prose-strong:text-purple-800 prose-strong:font-medium prose-a:text-purple-600 prose-a:no-underline hover:prose-a:underline">
            <ReactMarkdown remarkPlugins={[remarkGfm]}>{summary}</ReactMarkdown>
          </div>
        </div>
      </div>
    </div>
  );
};

export default Summary;
</file>

<file path="lib/youtube.ts">
import { PodcastMetadata } from "@/components/PodcastMetadata"; // Corrected import path

const YOUTUBE_API_KEY = process.env.YOUTUBE_API_KEY;
const YOUTUBE_API_URL = "https://www.googleapis.com/youtube/v3/videos";

/**
 * Fetches video metadata directly from the YouTube Data API v3.
 *
 * @param videoId - The ID of the YouTube video.
 * @returns A promise that resolves to the podcast metadata or null if fetching fails.
 */
export async function fetchMetadataFromYouTubeAPI(
  videoId: string
): Promise<Partial<PodcastMetadata> | null> {
  if (!YOUTUBE_API_KEY) {
    console.warn("YouTube API key is missing. Cannot fetch metadata via API.");
    return null;
  }

  const url = `${YOUTUBE_API_URL}?part=snippet,contentDetails,statistics&id=${videoId}&key=${YOUTUBE_API_KEY}`;

  try {
    console.log(`Fetching metadata from YouTube API for video ID: ${videoId}`);
    const response = await fetch(url, {
      headers: {
        Accept: "application/json",
        // Add a specific referer header that matches what's allowed in Google Cloud Console
        // or use the app's own domain when deployed
        Referer: "https://youtube-ai-podcast.vercel.app/",
        // Add an origin header to match the referer
        Origin: "https://youtube-ai-podcast.vercel.app",
      },
      next: { revalidate: 3600 }, // Cache for 1 hour
      signal: AbortSignal.timeout(10000), // 10 second timeout
    });

    if (!response.ok) {
      const errorData = await response.text();
      console.error(
        `YouTube API request failed with status ${response.status}: ${errorData}`
      );
      // Distinguish between common errors
      if (response.status === 401 || response.status === 403) {
        console.error(
          "Potential YouTube API Key issue (invalid, restricted, or quota exceeded)."
        );
        // Log more details for debugging
        console.error(`Full error: ${errorData}`);
        console.error(
          `Request URL: ${url.replace(
            YOUTUBE_API_KEY || "",
            "[API_KEY_REDACTED]"
          )}`
        );
      } else if (response.status === 404) {
        console.error(`Video with ID ${videoId} not found via YouTube API.`);
      }
      return null; // Indicate failure to the caller
    }

    const data = await response.json();

    if (!data.items || data.items.length === 0) {
      console.warn(
        `No items found in YouTube API response for video ID: ${videoId}`
      );
      return null;
    }

    const item = data.items[0];
    const snippet = item.snippet;
    const contentDetails = item.contentDetails;
    const statistics = item.statistics;

    // Basic validation
    if (!snippet || !contentDetails || !statistics) {
      console.warn(
        `Incomplete data received from YouTube API for video ID: ${videoId}`
      );
      return null;
    }

    // Construct the thumbnails object expected by the interface
    const bestThumbnail =
      snippet.thumbnails?.maxres ||
      snippet.thumbnails?.high ||
      snippet.thumbnails?.medium ||
      snippet.thumbnails?.default;
    const thumbnailsData = bestThumbnail
      ? {
          high: {
            url: bestThumbnail.url,
            width: bestThumbnail.width,
            height: bestThumbnail.height,
          },
        }
      : null;

    // Format duration if needed (implement formatISODuration)
    const formattedDuration = contentDetails.duration
      ? formatISODuration(contentDetails.duration)
      : "0:00";

    const metadata: Partial<PodcastMetadata> = {
      videoId: videoId,
      title: snippet.title || "Untitled Podcast",
      channelName: snippet.channelTitle || "Unknown Channel",
      thumbnails: thumbnailsData,
      publishedAt: snippet.publishedAt || null,
      description: snippet.description || null,
      fullDescription: snippet.description || null,
      duration: formattedDuration,
      viewCount: statistics.viewCount || null,
      likeCount: statistics.likeCount || null,
      descriptionTruncated: snippet.description
        ? snippet.description.length > 300
        : false,
    };

    console.log(
      `Successfully fetched metadata from YouTube API for video ID: ${videoId}`
    );
    return metadata;
  } catch (error: any) {
    console.error(
      `Error fetching metadata from YouTube API for video ID ${videoId}:`,
      error.message || error
    );
    // Add more specific error logging if needed
    if (error.name === "AbortError") {
      console.error("YouTube API request timed out.");
    } else if (error instanceof TypeError) {
      console.error("Network error or issue constructing YouTube API request.");
    }
    return null; // Indicate failure
  }
}

// Helper function to format ISO 8601 duration to readable time (HH:MM:SS or MM:SS)
// This should match the format expected by PodcastMetadata interface
const formatISODuration = (isoDuration: string): string => {
  const regex = /PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?/;
  const matches = isoDuration.match(regex);

  if (!matches) {
    return "0:00"; // Default or error format
  }

  const hours = matches[1] ? parseInt(matches[1]) : 0;
  const minutes = matches[2] ? parseInt(matches[2]) : 0;
  const seconds = matches[3] ? parseInt(matches[3]) : 0;

  if (hours > 0) {
    return `${hours}:${minutes.toString().padStart(2, "0")}:${seconds
      .toString()
      .padStart(2, "0")}`;
  }

  return `${minutes}:${seconds.toString().padStart(2, "0")}`;
};

// Define PodcastMetadata type if not imported globally or adjust import path
// interface PodcastMetadata {
//   videoId: string;
//   title: string;
//   channelName: string;
//   thumbnailUrl: string;
// //   duration?: number; // In seconds - optional
// }
</file>

<file path="DEPLOYMENT_VERCEL.md">
# Deploying YouTube AI Podcast Assistant to Vercel

This document outlines the steps to deploy the application to Vercel.

## 1. Prerequisites

- Ensure your code is pushed to a Git repository (GitHub, GitLab, or Bitbucket).
- Have a Vercel account ([https://vercel.com/signup](https://vercel.com/signup)).

## 2. Vercel Project Setup

1.  Log in to your Vercel dashboard.
2.  Click **Add New...** -> **Project**.
3.  **Import Git Repository**: Select the Git provider where your repository is hosted and import the `youtube-ai-podcast-assistant` repository.
4.  Vercel should automatically detect it as a Next.js project.

## 3. Configure Project Settings

1.  **Framework Preset**: Verify Vercel sets it to "Next.js".
2.  **Build & Development Settings**: The defaults usually work for Next.js (`npm run build`, output directory `.next`). No changes are typically needed.
3.  **Environment Variables**: This is crucial.
    - Navigate to your project's **Settings** -> **Environment Variables**.
    - Add the following variables:
      - `OPENAI_API_KEY`: Enter your OpenAI API key. Mark it as **Secret**.
      - `NEXT_PUBLIC_SUPABASE_URL`: Enter your Supabase project URL.
      - `NEXT_PUBLIC_SUPABASE_ANON_KEY`: Enter your Supabase Anon Key.
      - `YOUTUBE_API_KEY`: Enter your YouTube Data API key. Mark it as **Secret**.
    - _Note: The `NEXT_PUBLIC_` prefixes are required for the Supabase variables to be accessible in the browser.\_

## 4. Deploy

1.  Review the settings.
2.  Click the **Deploy** button.
3.  Vercel will clone the repository, install dependencies (`npm install`), build the project (`npm run build`), and deploy it. Wait for the process to complete.

## 5. Update Supabase & Google Cloud Redirect URIs

_This is a critical post-deployment step._

1.  **Get Deployment URL**: Once Vercel deployment is successful, note your production URL (e.g., `your-project-name.vercel.app`).
2.  **Update Supabase**:
    - Go to your Supabase project dashboard -> **Authentication** -> **URL Configuration**.
    - In the **Redirect URLs** section, add your Vercel production URL followed by `/auth/v1/callback`.
    - Example: `https://your-project-name.vercel.app/auth/v1/callback`
    - Save the changes.
3.  **Update Google Cloud Console**:
    - Go to your Google Cloud project -> **APIs & Services** -> **Credentials**.
    - Find the OAuth 2.0 Client ID you configured for Supabase authentication.
    - Edit the client ID.
    - Under **Authorized redirect URIs**, add the _exact same_ Vercel callback URL as added in Supabase.
    - Example: `https://your-project-name.vercel.app/auth/v1/callback`
    - Save the changes.

_Failure to update these redirect URIs will prevent the Google Sign-In from working on your deployed Vercel application._

## 6. Testing

1.  Access your Vercel deployment URL in a browser.
2.  Verify the landing page loads correctly.
3.  Test the **Sign In** button and complete the Google OAuth flow. You should be redirected to the `/dashboard`.
4.  On the dashboard, enter a valid YouTube podcast URL.
5.  Click **Process Podcast**.
6.  Verify that the metadata, summary, and chat features work as expected.
7.  Test the **Sign Out** functionality.
</file>

<file path="README.md">
# YouTube AI Podcast Assistant

A modern web application that helps users extract insights from YouTube podcasts through AI-powered summarization and interactive chat.

## Features

- **User Authentication**: Secure sign-in via Google OAuth powered by Supabase
- **Podcast Transcription**: Automatically extracts transcripts from YouTube videos
- **YouTube Metadata Extraction**: Fetches video title, channel name, and duration for better context
- **AI-Powered Summarization**: Generates structured summaries of podcast content with:
  - Executive Summary
  - Key Insights with timestamps
  - Detailed Timeline
  - Notable Quotes
  - Related Resources
  - Thought-provoking Questions
- **Interactive Chat**: Ask specific questions about the podcast content and get AI-generated answers
- **Seamless Experience**: Enter a YouTube URL once and switch between summary and chat features
- **Markdown Support**: All AI-generated content is formatted in Markdown for better readability
- **Responsive Design**: Works well on both desktop and mobile devices
- **Error Handling**: Robust error handling for transcript extraction and API responses
- **Landing Page**: Modern landing page for unauthenticated users with sign-in options
- **Protected Dashboard**: Authenticated user workspace with podcast processing tools accessible only after login

## Tech Stack

- **Frontend**: Next.js 14, React, TypeScript, Tailwind CSS
- **Authentication**: Supabase (Auth, Google Provider)
- **UI Components**: React Markdown, Headless UI, Hero Icons, `@supabase/ssr`, `@supabase/supabase-js`
- **Notifications**: React Hot Toast
- **AI Integration**: OpenAI API (using GPT-4o-mini)
- **YouTube Integration**: YouTube Transcript API, YouTube oEmbed API
- **State Management**: React useState/useEffect hooks
- **Build Tools**: TypeScript, PostCSS, Autoprefixer
- **Routing**: App Router with middleware for authenticated/unauthenticated routes

## Getting Started

### Prerequisites

- Node.js 18+ and npm
- OpenAI API key
- Supabase Project: Set up a project on [Supabase](https://supabase.com/)
- Supabase Project URL and Anon Key
- Google Cloud Project: Configured with OAuth 2.0 credentials (Client ID & Secret)
- Authorized Redirect URI in Google Cloud matching your Supabase callback URL (e.g., `YOUR_SUPABASE_URL/auth/v1/callback`)
- Supabase Authentication configured with your Google Client ID and Secret

### Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/youtube-ai-podcast-assistant.git
   cd youtube-ai-podcast-assistant
   ```

2. Install dependencies:

   ```bash
   npm install
   npm install @supabase/ssr @supabase/supabase-js
   ```

3. Create a `.env.local` file in the root directory with your keys:

   ```dotenv
   OPENAI_API_KEY=your_openai_api_key_here
   NEXT_PUBLIC_SUPABASE_URL=your_supabase_project_url
   NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
   ```

4. Start the development server:

   ```bash
   npm run dev
   ```

5. Open [http://localhost:3000](http://localhost:3000) in your browser to see the application.

## User Experience

### Non-authenticated Users

Non-authenticated users will see the landing page at the root URL (`/`) with:

- Navigation bar with "Sign In" button
- Engaging hero section with a visual representation of the chat interface and a "Get Started" button
- Feature highlights showcasing key capabilities
- Step-by-step guide on how the platform works
- Call-to-action buttons ("Get Started", "Sign In") triggering the Google OAuth flow via Supabase

### Authenticated Users

After signing in (via Google OAuth redirect handled by Supabase), users are directed to the protected dashboard (`/dashboard`) where they can:

1. See their email in the Navbar and a "Sign Out" button
2. Enter a YouTube podcast URL in the input field
3. Click "Process Podcast" to extract the content
4. View the AI-generated summary in the Summary tab, including video metadata
5. Switch to the Chat tab to ask specific questions about the podcast content

## How It Works

1. **Authentication (Optional for Landing Page, Required for Dashboard)**:
   - User clicks "Sign In" or "Get Started" on the landing page.
   - App initiates Supabase Google OAuth flow.
   - Supabase handles the redirect to Google and the callback (`/auth/callback`).
   - Session is established via cookies managed by `@supabase/ssr`.
   - Middleware (`middleware.ts`) protects `/dashboard` and redirects users based on auth state.
2. **User Input**: Authenticated user enters a YouTube podcast URL in the dashboard input field
3. **Metadata & Transcript Extraction**:
   - App extracts metadata (title, channel, duration) using YouTube oEmbed API
   - App extracts the transcript from the video using the YouTube Transcript API
4. **AI Processing**:
   - For summaries: Transcript and metadata are sent to OpenAI API with specific prompts
   - For chat: User questions are sent with the transcript context to get relevant answers
5. **Display**: Results are displayed in a clean, user-friendly interface with proper Markdown formatting

## Technical Highlights

- Supabase integration for secure Google OAuth authentication
- Server-side session management using Next.js Middleware and `@supabase/ssr` helpers
- Client-side authentication state handling in components (`Navbar`, `app/page.tsx`)
- Handles large transcripts by truncating them to fit within OpenAI token limits
- Custom prompts to generate well-structured summaries with specific sections
- Metadata enrichment for improved context in AI processing
- Chat history management for contextual conversation
- Accessibility features including proper ARIA labels
- Responsive UI with loading indicators for better user experience
- Error handling for various failure scenarios (invalid URLs, missing transcripts, API failures)

## Project Structure

```
youtube-ai-podcast-assistant/
├── app/                  # Next.js app directory
│   ├── api/              # API routes
│   │   ├── chat/         # Chat API endpoint
│   │   ├── podcast-metadata/ # Metadata API endpoint
│   │   └── summarize/    # Summarization API endpoint
│   ├── auth/             # Authentication related routes
│   │   ├── callback/     # Supabase OAuth callback handler
│   │   │   └── route.ts
│   │   └── auth-code-error/ # Error page for auth failures
│   │       └── page.tsx
│   ├── dashboard/        # Protected dashboard page (authenticated users)
│   │   └── page.tsx      # Dashboard component
│   ├── globals.css       # Global styles
│   ├── layout.tsx        # Root layout
│   └── page.tsx          # Landing page component (unauthenticated users)
├── components/           # React components
│   ├── Chat.tsx          # Chat interface component
│   ├── Summary.tsx       # Summary display component
│   ├── PodcastHeader.tsx # Podcast header component
│   ├── PodcastMetadata.tsx # Metadata provider component
│   ├── Navbar.tsx        # Navigation bar (used in Dashboard, shows auth state)
│   └── ErrorBoundary.tsx # Error handling component
├── lib/
│   └── supabase/         # Supabase client utilities
│       ├── client.ts     # Browser client
│       └── server.ts     # Server/Middleware client
├── public/               # Static assets
│   ├── app-screenshot-new.png    # Updated screenshot name
│   └── create-screenshot.html # Tool for generating app screenshots
├── .env.local            # Environment variables (API keys, Supabase URL/Key)
├── middleware.ts         # Next.js middleware for auth redirects & session refresh
├── next.config.js        # Next.js configuration
├── package.json          # Project dependencies
├── postcss.config.js     # PostCSS configuration
├── supabaseplan.md       # Supabase implementation plan (optional)
├── tailwind.config.js    # Tailwind CSS configuration
└── tsconfig.json         # TypeScript configuration
```

## Landing Page Features

The landing page includes:

- Navigation bar with "Sign In" button
- Hero section with two-column layout (text and app screenshot)
- Feature cards highlighting key capabilities
- "How It Works" section explaining the user flow
- Call-to-action section for conversion
- Responsive design that works well on all devices

## Dashboard Features

The dashboard includes:

- YouTube URL input form
- Tabbed interface for Summary and Chat views
- Podcast metadata display with video thumbnail
- Interactive chat interface
- AI-generated summary with structured sections
- Responsive layout for different screen sizes

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- OpenAI for providing the AI capabilities
- Next.js team for the amazing framework
- All open-source libraries used in this project
</file>

<file path="tailwind.config.js">
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    "./pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      backgroundImage: {
        "gradient-radial": "radial-gradient(var(--tw-gradient-stops))",
        "gradient-conic":
          "conic-gradient(from 180deg at 50% 50%, var(--tw-gradient-stops))",
      },
    },
  },
  plugins: [require("@tailwindcss/aspect-ratio")],
};
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": false,
    "noEmit": true,
    "incremental": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "plugins": [
      {
        "name": "next"
      }
    ],
    "baseUrl": ".",
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": ["next-env.d.ts", ".next/types/**/*.ts", "**/*.ts", "**/*.tsx"],
  "exclude": ["node_modules"]
}
</file>

<file path="app/dashboard/page.tsx">
"use client";

import React, { useState, useEffect } from "react";
import { ArrowPathIcon } from "@heroicons/react/24/solid";
import toast, { Toaster } from "react-hot-toast";
import Summary from "../../components/Summary";
import Chat from "../../components/Chat";
import Navbar from "../../components/Navbar";
import PodcastHeader from "../../components/PodcastHeader";
import { PodcastMetadataProvider } from "../../components/PodcastMetadata";

export default function Dashboard() {
  const [url, setUrl] = useState("");
  const [loading, setLoading] = useState(false);
  const [validating, setValidating] = useState(false);
  const [summary, setSummary] = useState<string | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [activeTab, setActiveTab] = useState<"summary" | "chat">("summary");
  const [isUrlValidated, setIsUrlValidated] = useState(false);
  const [isSummaryLoading, setIsSummaryLoading] = useState(false);
  const [isChatLoading, setIsChatLoading] = useState(false);

  // Function to validate YouTube URL format
  const isValidYoutubeUrl = (url: string): boolean => {
    const youtubeRegex = /^(https?:\/\/)?(www\.)?(youtube\.com|youtu\.be)\/.+/;
    return youtubeRegex.test(url);
  };

  // Extract video ID from URL
  const extractVideoId = (url: string): string | null => {
    const match = url.match(
      /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
    );
    return match ? match[1] : null;
  };

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setError(null);

    if (!isValidYoutubeUrl(url)) {
      toast.error("Please enter a valid YouTube URL");
      return;
    }

    const videoId = extractVideoId(url);
    if (!videoId) {
      toast.error("Could not extract video ID from URL");
      return;
    }

    setLoading(true);
    setIsSummaryLoading(true);
    setSummary(null);

    try {
      console.log("Submitting URL:", url, "Video ID:", videoId);
      const response = await fetch("/api/summarize", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ videoId }),
      });

      const data = await response.json();
      console.log("Response:", data);

      if (!response.ok) {
        throw new Error(data.error || "Failed to generate summary");
      }

      setSummary(data.summary);
      setIsUrlValidated(true);
      toast.success("Summary generated successfully!");
    } catch (error) {
      console.error("Error:", error);
      const errorMessage =
        error instanceof Error ? error.message : "Failed to generate summary";
      setError(errorMessage);
      toast.error(errorMessage);
    } finally {
      setLoading(false);
      setIsSummaryLoading(false);
    }
  };

  const validateUrl = async () => {
    if (!isValidYoutubeUrl(url)) {
      toast.error("Please enter a valid YouTube URL");
      return;
    }

    const videoId = extractVideoId(url);
    if (!videoId) {
      toast.error("Could not extract video ID from URL");
      return;
    }

    setValidating(true);
    setIsChatLoading(true);
    setError(null);

    try {
      // Just check if the transcript exists
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          videoId,
          question:
            "Just checking if the transcript exists. Please respond with 'Yes'.",
        }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || "Failed to validate URL");
      }

      setIsUrlValidated(true);
      setActiveTab("chat");
      toast.success("Podcast loaded successfully!");
    } catch (error) {
      console.error("Error:", error);
      const errorMessage =
        error instanceof Error ? error.message : "Failed to validate URL";
      setError(errorMessage);
      toast.error(errorMessage);
    } finally {
      setValidating(false);
      setIsChatLoading(false);
    }
  };

  const handleTabChange = (tab: "summary" | "chat") => {
    if (loading || validating) return;

    setActiveTab(tab);

    // If switching to chat and URL is not yet validated, validate it
    if (tab === "chat" && !isUrlValidated && url) {
      validateUrl();
    }

    // If switching to summary and no summary exists yet, generate it
    if (tab === "summary" && !summary && isUrlValidated) {
      setIsSummaryLoading(true);
      // Create a proper event to avoid type errors
      const fakeEvent = { preventDefault: () => {} } as React.FormEvent;
      handleSubmit(fakeEvent);
    }
  };

  // Handle URL input change
  const handleUrlChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    setUrl(e.target.value);
    // Reset validation state when URL changes
    if (isUrlValidated) {
      setIsUrlValidated(false);
      setSummary(null);
    }
  };

  return (
    <div className="min-h-screen bg-[#f7f7ff]">
      <Navbar />
      <Toaster position="bottom-right" />

      <div className="max-w-7xl mx-auto px-4">
        <div className="pt-8 pb-16 md:pt-12 md:pb-24 text-center">
          <h1 className="text-5xl md:text-6xl font-bold text-purple-700 mb-6">
            Transform Podcasts into
            <br />
            Interactive Conversations
          </h1>
          <p className="text-lg text-gray-600 max-w-3xl mx-auto mb-10">
            Paste any podcast URL and let AI create summaries and engage in
            meaningful conversations about the content
          </p>

          <div className="max-w-3xl mx-auto bg-white p-8 rounded-2xl shadow-md">
            <div className="flex items-center mb-4">
              <div className="w-6 h-6 mr-2">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                  className="text-purple-600"
                >
                  <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
                  <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
                </svg>
              </div>
              <h2 className="text-xl font-semibold">Enter Podcast URL</h2>
            </div>

            <form onSubmit={handleSubmit}>
              <div className="flex flex-col gap-4">
                <div className="flex flex-col md:flex-row gap-2">
                  <input
                    id="youtube-url"
                    type="text"
                    value={url}
                    onChange={handleUrlChange}
                    placeholder="https://podcast-url.com/episode"
                    className="flex-1 p-3 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-purple-500"
                    disabled={loading || validating}
                    aria-label="YouTube URL"
                  />
                  <button
                    type="submit"
                    disabled={loading || validating || !url.trim()}
                    className="px-6 py-3 bg-purple-600 text-white rounded-lg hover:bg-purple-700 disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center"
                    aria-label={loading ? "Processing..." : "Analyze"}
                  >
                    {loading ? (
                      <div className="flex items-center">
                        <ArrowPathIcon className="w-5 h-5 animate-spin mr-2" />
                        <span>Processing...</span>
                      </div>
                    ) : (
                      <div className="flex items-center">
                        <svg
                          xmlns="http://www.w3.org/2000/svg"
                          viewBox="0 0 24 24"
                          fill="currentColor"
                          className="w-5 h-5 mr-2"
                        >
                          <path d="M18.375 2.25c-1.035 0-1.875.84-1.875 1.875v15.75c0 1.035.84 1.875 1.875 1.875h.75c1.035 0 1.875-.84 1.875-1.875V4.125c0-1.036-.84-1.875-1.875-1.875h-.75zM9.75 8.625c0-1.036.84-1.875 1.875-1.875h.75c1.036 0 1.875.84 1.875 1.875v11.25c0 1.035-.84 1.875-1.875 1.875h-.75c-1.036 0-1.875-.84-1.875-1.875V8.625zM3 13.125c0-1.036.84-1.875 1.875-1.875h.75c1.036 0 1.875.84 1.875 1.875v6.75c0 1.035-.84 1.875-1.875 1.875h-.75C3.84 21.75 3 20.91 3 19.875v-6.75z" />
                        </svg>
                        Analyze
                      </div>
                    )}
                  </button>
                </div>
                {error && (
                  <div className="text-red-600 text-sm p-2 bg-red-50 rounded">
                    Error: {error}
                  </div>
                )}
              </div>
            </form>
          </div>
        </div>

        {/* Feature Cards Section - only shown when no podcast summary is generated */}
        {!isUrlValidated && (
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-8 max-w-7xl mx-auto px-4 pb-20">
            {/* Smart Summaries Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M12 6.042A8.967 8.967 0 006 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 016 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 016-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0018 18a8.967 8.967 0 00-6 2.292m0-14.25v14.25" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Smart Summaries
              </h3>
              <p className="text-gray-600">
                Get comprehensive summaries that capture the key points and
                insights from any podcast.
              </p>
            </div>

            {/* Interactive Chat Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M4.913 2.658c2.075-.27 4.19-.408 6.337-.408 2.147 0 4.262.139 6.337.408 1.922.25 3.291 1.861 3.405 3.727a4.403 4.403 0 00-1.032-.211 50.89 50.89 0 00-8.42 0c-2.358.196-4.04 2.19-4.04 4.434v4.286a4.47 4.47 0 002.433 3.984L7.28 21.53A.75.75 0 016 21v-4.03a48.527 48.527 0 01-1.087-.128C2.905 16.58 1.5 14.833 1.5 12.862V6.638c0-1.97 1.405-3.718 3.413-3.979z" />
                  <path d="M15.75 7.5c-1.376 0-2.739.057-4.086.169C10.124 7.797 9 9.103 9 10.609v4.285c0 1.507 1.128 2.814 2.67 2.94 1.243.102 2.5.157 3.768.165l2.782 2.781a.75.75 0 001.28-.53v-2.39l.33-.026c1.542-.125 2.67-1.433 2.67-2.94v-4.286c0-1.505-1.125-2.811-2.664-2.94A49.392 49.392 0 0015.75 7.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Interactive Chat
              </h3>
              <p className="text-gray-600">
                Ask questions about the podcast content and get detailed answers
                from our AI assistant.
              </p>
            </div>

            {/* Works Everywhere Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
                  <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Works Everywhere
              </h3>
              <p className="text-gray-600">
                Compatible with podcasts from Spotify, Apple Podcasts, YouTube,
                and many more platforms.
              </p>
            </div>

            {/* AI-Powered Insights Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M21.731 2.269a2.625 2.625 0 00-3.712 0l-1.157 1.157 3.712 3.712 1.157-1.157a2.625 2.625 0 000-3.712zM19.513 8.199l-3.712-3.712-8.4 8.4a5.25 5.25 0 00-1.32 2.214l-.8 2.685a.75.75 0 00.933.933l2.685-.8a5.25 5.25 0 002.214-1.32l8.4-8.4z" />
                  <path d="M5.25 5.25a3 3 0 00-3 3v10.5a3 3 0 003 3h10.5a3 3 0 003-3V13.5a.75.75 0 00-1.5 0v5.25a1.5 1.5 0 01-1.5 1.5H5.25a1.5 1.5 0 01-1.5-1.5V8.25a1.5 1.5 0 011.5-1.5h5.25a.75.75 0 000-1.5H5.25z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                AI-Powered Insights
              </h3>
              <p className="text-gray-600">
                Advanced AI analyzes podcasts to extract the most relevant
                information and context.
              </p>
            </div>
          </div>
        )}

        {isUrlValidated && (
          <div className="max-w-5xl mx-auto">
            <PodcastMetadataProvider videoUrl={url}>
              {(metadata, metadataLoading) => (
                <>
                  {metadata && (
                    <PodcastHeader
                      videoId={metadata.videoId}
                      title={metadata.title}
                      channelName={metadata.channelName}
                      duration={metadata.duration}
                      viewCount={metadata.viewCount}
                      likeCount={metadata.likeCount}
                      publishedAt={metadata.publishedAt}
                      onChatClick={() => handleTabChange("chat")}
                      activeTab={activeTab}
                    />
                  )}

                  <div className="bg-white rounded-lg mb-8">
                    <div className="flex border-b">
                      <button
                        onClick={() => handleTabChange("summary")}
                        className={`flex-1 py-4 font-medium text-center transition-colors ${
                          activeTab === "summary"
                            ? "text-purple-600 border-b-2 border-purple-600 bg-purple-50"
                            : "text-gray-500 hover:text-gray-700 hover:bg-gray-50"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Summary tab"
                        aria-selected={activeTab === "summary"}
                        role="tab"
                      >
                        <div className="flex items-center justify-center">
                          <svg
                            xmlns="http://www.w3.org/2000/svg"
                            className="h-5 w-5 mr-2"
                            fill="none"
                            viewBox="0 0 24 24"
                            stroke="currentColor"
                          >
                            <path
                              strokeLinecap="round"
                              strokeLinejoin="round"
                              strokeWidth={2}
                              d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-14.25C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.247 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"
                            />
                          </svg>
                          <span>Summary</span>
                          {isSummaryLoading && (
                            <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                          )}
                        </div>
                      </button>
                      <button
                        onClick={() => handleTabChange("chat")}
                        className={`flex-1 py-4 font-medium text-center transition-colors ${
                          activeTab === "chat"
                            ? "text-purple-600 border-b-2 border-purple-600 bg-purple-50"
                            : "text-gray-500 hover:text-gray-700 hover:bg-gray-50"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Chat Assistant tab"
                        aria-selected={activeTab === "chat"}
                        role="tab"
                      >
                        <div className="flex items-center justify-center">
                          <svg
                            xmlns="http://www.w3.org/2000/svg"
                            className="h-5 w-5 mr-2"
                            fill="none"
                            viewBox="0 0 24 24"
                            stroke="currentColor"
                          >
                            <path
                              strokeLinecap="round"
                              strokeLinejoin="round"
                              strokeWidth={2}
                              d="M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z"
                            />
                          </svg>
                          <span>Chat Assistant</span>
                          {isChatLoading && (
                            <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                          )}
                        </div>
                      </button>
                    </div>

                    <div
                      role="tabpanel"
                      aria-label="Tab Content"
                      className="w-full"
                    >
                      <div className="max-w-full">
                        {activeTab === "summary" && (
                          <Summary summary={summary || ""} videoUrl={url} />
                        )}
                        {activeTab === "chat" && <Chat videoUrl={url} />}
                      </div>
                    </div>
                  </div>
                </>
              )}
            </PodcastMetadataProvider>
          </div>
        )}
      </div>
    </div>
  );
}
</file>

<file path="package.json">
{
  "name": "youtube-ai-transcriber",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@headlessui/react": "^1.7.18",
    "@heroicons/react": "^2.1.1",
    "@supabase/ssr": "^0.6.1",
    "@supabase/supabase-js": "^2.49.4",
    "@tailwindcss/aspect-ratio": "^0.4.2",
    "axios": "^1.6.7",
    "next": "14.2.26",
    "next-auth": "^4.24.5",
    "node-fetch": "^2.7.0",
    "openai": "^4.28.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-hot-toast": "^2.4.1",
    "react-markdown": "^10.0.0",
    "remark-gfm": "^4.0.1",
    "youtube-transcript": "^1.0.6"
  },
  "devDependencies": {
    "@types/node": "^20.11.19",
    "@types/node-fetch": "^2.6.12",
    "@types/react": "^18.2.57",
    "@types/react-dom": "^18.2.19",
    "autoprefixer": "^10.4.17",
    "eslint": "^8.56.0",
    "eslint-config-next": "14.1.0",
    "postcss": "^8.4.35",
    "tailwindcss": "^3.4.1",
    "typescript": "^5.3.3"
  }
}
</file>

<file path="app/api/chat/route.ts">
import { NextResponse } from "next/server";
// Replace the youtube-transcript npm package with our custom implementation
// import { YoutubeTranscript } from "youtube-transcript";
import { fetchTranscript, TranscriptLine } from "@/lib/youtube-transcript";
import OpenAI from "openai";

// Check if OpenAI API key is set
if (!process.env.OPENAI_API_KEY) {
  console.error("OPENAI_API_KEY is not set in environment variables");
  throw new Error("OPENAI_API_KEY is not set in environment variables");
}

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Custom direct fetch function is now handled in the youtube-transcript library
// Removing the redundant fetchYouTubeTranscriptDirectly function

export async function POST(request: Request) {
  try {
    // Parse request body
    const body = await request.json();
    const { url, videoId: providedVideoId, question, chatHistory = [] } = body;

    // Support both direct videoId and url parameter
    let videoId = providedVideoId;

    // If videoId is not provided but url is, try to extract videoId from url
    if (!videoId && url) {
      console.log("Processing chat for URL:", url);
      videoId = url.match(
        /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
      )?.[1];

      if (!videoId) {
        console.error("Invalid YouTube URL:", url);
        return NextResponse.json(
          { error: "Invalid YouTube URL" },
          { status: 400 }
        );
      }
      console.log("Extracted video ID from URL:", videoId);
    } else if (videoId) {
      console.log("Processing chat with provided video ID:", videoId);
    }

    if (!videoId) {
      console.error("No videoId or URL provided");
      return NextResponse.json(
        { error: "No videoId or URL provided" },
        { status: 400 }
      );
    }

    console.log("Question:", question);

    if (!question) {
      console.error("No question provided");
      return NextResponse.json(
        { error: "No question provided" },
        { status: 400 }
      );
    }

    // Get transcript
    try {
      console.log(`[${videoId}] Attempting to fetch transcript for chat...`);

      let transcript;

      // Use our custom library that handles all methods internally
      try {
        transcript = await fetchTranscript(videoId);
        console.log(
          `[${videoId}] Successfully fetched transcript for chat: ${transcript?.length} segments`
        );
      } catch (error: any) {
        console.error(`[${videoId}] Transcript fetch failed for chat:`, error);

        // Check for specific error about captions being unavailable
        if (
          error.message.includes("doesn't have captions") ||
          error.message.includes("has disabled captions")
        ) {
          return NextResponse.json(
            {
              error: "CAPTIONS_UNAVAILABLE",
              message:
                "This video doesn't have captions or has disabled captions. Please try a video with captions enabled.",
            },
            { status: 404 }
          );
        }

        return NextResponse.json(
          { error: "No transcript found for this video" },
          { status: 404 }
        );
      }

      console.log(
        `[${videoId}] Raw transcript response received for chat. Length: ${transcript?.length}`
      );

      if (!transcript || transcript.length === 0) {
        console.error(
          `[${videoId}] No transcript data found in the response for chat.`
        );
        return NextResponse.json(
          { error: "No transcript found for this video" },
          { status: 404 }
        );
      }

      const transcriptText = transcript.map((item) => item.text).join(" ");
      console.log(
        `[${videoId}] Transcript processed for chat. Text length: ${transcriptText.length}`
      );

      if (!transcriptText || transcriptText.trim() === "") {
        console.error(
          `[${videoId}] Processed transcript text is empty for chat.`
        );
        return NextResponse.json(
          { error: "Empty transcript found for this video" },
          { status: 404 }
        );
      }

      // Truncate transcript if it's too long (OpenAI has token limits)
      const maxChars = 42000; // Approximately 12000 tokens
      const truncatedText =
        transcriptText.length > maxChars
          ? transcriptText.slice(0, maxChars) + "..."
          : transcriptText;

      console.log(
        `[${videoId}] Truncated transcript text length for chat: ${truncatedText.length}`
      );

      // Prepare chat history for the API
      const messages = [
        {
          role: "system",
          content: `You're a friendly podcast assistant who helps users understand podcast content better. You have access to the transcript of a YouTube podcast. Answer questions about the podcast in a conversational, helpful way. 

When answering:
- Be specific and reference the content directly
- If you can identify timestamps for relevant parts, include them
- If the question asks about something not covered in the podcast, politely explain that it wasn't discussed
- Keep your tone casual and friendly, like you're chatting with a friend
- If appropriate, mention related topics that were discussed in the podcast that might interest the user

Here's the podcast transcript: ${truncatedText}`,
        },
        ...chatHistory,
        {
          role: "user",
          content: question,
        },
      ];

      // Generate answer using OpenAI
      try {
        console.log("Calling OpenAI API for chat...");
        const completion = await openai.chat.completions.create({
          messages,
          model: "gpt-4o-mini",
          max_tokens: 1024,
          temperature: 0.7,
        });

        console.log("OpenAI API response received");
        const answer = completion.choices[0].message.content;

        if (!answer) {
          console.error("No answer generated by OpenAI");
          throw new Error("No answer generated by OpenAI");
        }

        console.log("Answer length:", answer.length, "characters");
        return NextResponse.json({
          answer,
          chatHistory: [
            ...chatHistory,
            { role: "user", content: question },
            { role: "assistant", content: answer },
          ],
        });
      } catch (openaiError: any) {
        console.error("OpenAI API Error:", {
          message: openaiError.message,
          type: openaiError.type,
          stack: openaiError.stack,
          response: openaiError.response?.data,
        });
        return NextResponse.json(
          {
            error: `Failed to generate answer using AI: ${openaiError.message}`,
          },
          { status: 500 }
        );
      }
    } catch (transcriptError: any) {
      console.error(`[${videoId}] Transcript Fetching Error (Chat):`, {
        message: transcriptError.message,
        name: transcriptError.name,
        // Consider logging more properties if available, e.g., error code
        stack: transcriptError.stack,
      });
      return NextResponse.json(
        {
          error: `Failed to fetch video transcript: ${transcriptError.message}`,
        },
        { status: 404 }
      );
    }
  } catch (error: any) {
    console.error("General Error:", {
      message: error.message,
      stack: error.stack,
    });
    return NextResponse.json(
      { error: `Failed to process request: ${error.message}` },
      { status: 500 }
    );
  }
}
</file>

<file path="TRANSCRIPT_ISSUE_PLAN.md">
# YouTube Transcript Issue Troubleshooting Plan

This document outlines our systematic approach to resolving the YouTube transcript fetching issue that occurs on the Vercel deployment but not in local development.

## Current Issue

- ✅ Application works correctly in local development environment
- ✅ Application now works in Vercel deployment after fixing multiple issues:
  - ✅ Metadata fetching fixed (API Key Referrer Restriction)
  - ✅ Transcript fetching fixed with enhanced approaches (Direct & Innertube methods)
  - ✅ Parameter consistency fixed between client and server

## Hypotheses

1. **CORS Restrictions**: Vercel's environment may have stricter CORS policies than local development
2. **YouTube IP Blocking / Page Structure Differences**: YouTube may be blocking requests or serving different HTML/data structures to Vercel's server IP ranges compared to local development (CONFIRMED - primary cause of transcript failures)
3. **Library Compatibility**: The transcript fetching library might not be compatible with Vercel's serverless environment (Mitigated by custom implementation)
4. **Authentication/Headers**: The requests from Vercel might be missing necessary headers or user-agent information (CONFIRMED - addressed by enhanced browser-like headers)
5. **Server vs. Client Execution**: The code might be executing in a different context (client vs server) on Vercel
6. ~~YouTube Page Structure~~: (Covered by Hypothesis 2)
7. ~~API Key Issues~~: (Resolved for metadata, but potentially still relevant if API method for transcripts is used/enabled)
8. ~~Page Format Changes~~: (Covered by Hypothesis 2)
9. **Parameter Inconsistency**: The dashboard was sending `url` parameter instead of `videoId` to the API endpoints (CONFIRMED - fixed by making client and server consistent)

## Current Diagnosis

Initial diagnosis indicated issues with API keys and URL construction. Further investigation and fixes revealed:

1. ~~URL construction for internal API calls was missing the proper protocol prefix~~ (Fixed by refactoring).
2. **YouTube returns different page structures/data when accessed from cloud providers (Vercel) vs. residential IPs**. This was the primary cause for transcript failures for Direct/Innertube methods. (CONFIRMED and FIXED)
3. The specific regex patterns and JSON parsing logic for extracting captions/transcript URLs were failing on the data received in the Vercel environment. (FIXED with more robust patterns)
4. ✅ **The YouTube Data API metadata fetch failed due to API Key HTTP Referrer restrictions**. This was resolved by removing the restriction in Google Cloud Console, allowing server-side calls.
5. **Inconsistent parameter usage**: The dashboard was sending `url` but the API expected `videoId`. (FIXED)

## Implemented Solution

We've addressed these issues with a multi-faceted approach:

1. Added more detailed logging to track down the exact point of failure (including HTML/JSON snippets on failure).
2. Fixed the URL construction for internal API calls by refactoring to direct function calls.
3. Implemented custom fallback methods (`Direct`, `Innertube`) for transcript fetching with browser-like headers (`User-Agent`, `Accept-Language`).
4. Added retry logic to try multiple methods of fetching transcripts (Library -> Direct -> Innertube).
5. **Refactored transcript fetching logic into a dedicated library (`lib/youtube-transcript.ts`)**.
6. Implemented detailed parsing logic within `lib/youtube-transcript.ts` including:
   - Multiple regex patterns to extract captions data from HTML.
   - Logic to find and select the transcript `baseUrl`.
   - Fetching the transcript XML/TTML content.
   - Parsing both XML (`<text>`) and TTML (`<p>`) formats.
7. Added timeout handling for HTTP requests.
8. Improved error handling and user-facing error messages.
9. Added direct oEmbed fallback for metadata.
10. Enhanced the Innertube method with key/context extraction and a fallback to direct HTML parsing if its API calls fail.
11. Provided more robust error handling to present useful information to the user even when transcripts can't be fetched.
12. Added session management with proper cookie handling to maintain state between requests.
13. Enhanced browser emulation with convincing headers to bypass YouTube's IP-based restrictions.
14. Fixed parameter inconsistency between client and server components.

### Phase 1: Initial Diagnosis & Fixes

✅ Identified missing API keys and URL construction issues

### Phase 2: First Iteration of Solutions

✅ Improved the server-side transcript fetching code with a robust fallback solution
✅ Added proper error handling with specific error types
✅ Added browser-like headers to all requests

### Phase 3: Enhanced Approach

✅ **3.1 Multiple Approach Strategy**

- Implemented multiple methods: standard library, direct fetch, Innertube API (API method currently disabled/commented out).
- Added cascading fallbacks to try all methods before failing.
- Improved regex patterns for HTML data extraction.
- Added parsing logic for both XML and TTML transcript formats.
- Enhanced Innertube with fallback to direct HTML parsing.

✅ **3.2 API Key & Authentication**

- **Fixed YouTube Data API key HTTP referrer restriction**, enabling metadata fetch from Vercel server-side.
- Implemented direct oEmbed approach for metadata fallback.

✅ **3.3 Enhanced Error Handling**

- Improved error classification and user-friendly messages.
- Return metadata even when transcript fetching fails.
- Added specific handling for different error types (timeouts, parsing, etc.).
- Added detailed logging within `catch` blocks and parsing functions for better Vercel diagnostics.

✅ **3.4 YouTube Innertube API**

- Implemented YouTube's internal API approach for transcript fetching.
- Added dynamic extraction for API key and client context from page HTML.
- Included fallbacks at every level of the process (including parsing initial HTML if API fails).

✅ **3.5 API Refactoring (Commit dd95513)**

- **Centralized Metadata Fetching**: Created `lib/youtube.ts`.
- **Removed Internal API Call**: Refactored routes to call `fetchMetadataFromYouTubeAPI` directly.
- **Standardized Fallbacks**: Consistent oEmbed fallback.
- **Improved Sequential Transcript Logic**: Refined sequence in `/api/summarize/route.ts`.

✅ **3.6 Transcript Library Refactoring (Commits d4f2b4a, dcd558b, 1c7f63d)**

- **Moved Transcript Logic**: Created `lib/youtube-transcript.ts` to house fetching and parsing functions (`fetchTranscriptDirect`, `fetchTranscriptInnertube`, `extractAndParseTranscriptFromHtml`, `parseTimestamp`).
- **Added Type Definitions**: Defined `TranscriptLine` interface.
- **Implemented Parsing**: Moved and refined HTML data extraction and XML/TTML parsing logic into the library file.
- **Enhanced Logging**: Added more detailed logs within the library functions.
- **Fixed Build Issues**: Resolved type errors related to the refactoring.

### Phase 4: Cloud Provider Compatibility (Commit 91711f2)

✅ **4.1 Enhanced Browser Emulation**

- **Updated Browser Headers**: Implemented more convincing browser-like headers with current Chrome versions.
- **Added Proxy Detection Bypass**: Added headers like X-Forwarded-For to simulate requests from Google crawler IPs instead of Vercel.
- **Enhanced Accept Headers**: Updated content type headers to match modern browsers.

✅ **4.2 Intelligent Cookie Management**

- **Cookie Parsing & Merging**: Added proper cookie parsing and management to maintain YouTube session.
- **Session Preservation**: Implemented cookie jar to preserve session state between requests.
- **Consent Page Handling**: Improved the handling of YouTube's cookie consent flows.

✅ **4.3 Rate Limiting Protection**

- **Intelligent Delays**: Added rate limiting with random delays between requests.
- **Request Timing**: Implemented a waitBetweenRequests function to avoid detection as a bot.

✅ **4.4 Enhanced Transcript Extraction**

- **Expanded Regex Patterns**: Added multiple new regex patterns to extract captions from different YouTube HTML structures.
- **Deep Object Search**: Implemented recursive object search for finding caption data in complex nested structures.
- **Multiple Format Support**: Added support for all known TTML variants used by YouTube.
- **Parameter Enhancement**: Auto-added necessary URL parameters for transcript requests.

✅ **4.5 Client-Server Parameter Consistency**

- **Dashboard Fix**: Updated dashboard to correctly send videoId parameter to API endpoints.
- **API Flexibility**: Enhanced API to accept both url and videoId parameters for backward compatibility.
- **Parameter Extraction**: Improved videoId extraction from URLs on the server side.

## Monitoring and Validation

For the implemented solution:

1. ✅ Deploy latest commit (91711f2) to Vercel.
2. ✅ Test with specific video IDs that previously failed: `d3dPRkyNbj8`.
3. **Monitor Vercel logs closely** for output from `lib/youtube-transcript.ts`.
4. ✅ Verify transcripts are successfully retrieved and displayed.
5. Test with other videos (with captions, auto-generated, non-English) to ensure no regressions.

## Success Criteria

- ✅ Application successfully retrieves transcripts for videos that have them available **on Vercel**.
- ✅ Application properly handles and communicates when transcripts are unavailable.
- ✅ Solution works consistently across multiple videos and over time.
- ✅ User receives helpful error messages when transcripts cannot be fetched.
- ✅ Basic functionality continues to work even when some components fail (graceful degradation).

## Future Enhancements (If Needed)

- [ ] **Proxy Solution**: If Vercel IPs begin getting blocked in the future, consider implementing a proxy service.
- [ ] **YouTube Data API for Captions**: Explore OAuth for official caption fetching for more stability.
- [ ] **Caching Strategy**: Implement intelligent caching to reduce the number of requests to YouTube.
- [ ] **Performance Optimization**: Further optimize the transcript fetching and parsing for speed.

## Implementation Notes

Our solution successfully bypasses YouTube's IP-based restrictions and variations in page structure between residential and cloud provider IPs. The combination of enhanced browser emulation, cookie management, and robust pattern matching has resolved the transcript fetching issues in the Vercel environment.

The solution is resilient against YouTube's frequent page structure changes due to the multiple fallback mechanisms and diverse regex patterns implemented. Session management with proper cookie handling ensures we maintain state between requests, which is crucial for working with YouTube's authentication flows.

## Rollback Plan

If any implementation significantly degrades the application:

- Revert to the previous working deployment.
- Document what caused the issue for future reference.
</file>

<file path="lib/youtube-transcript.ts">
interface TranscriptLine {
  text: string;
  duration: number;
  offset: number;
}

// Export the interface to be used in other modules
export type { TranscriptLine };

// Helper to parse HH:MM:SS.ms format to seconds
function parseTimestamp(timestamp: string): number {
  const parts = timestamp.split(":");
  let seconds = 0;
  if (parts.length === 3) {
    seconds += parseFloat(parts[0]) * 3600;
    seconds += parseFloat(parts[1]) * 60;
    seconds += parseFloat(parts[2]);
  } else if (parts.length === 2) {
    seconds += parseFloat(parts[0]) * 60;
    seconds += parseFloat(parts[1]);
  } else if (parts.length === 1) {
    seconds += parseFloat(parts[0]);
  }
  return isNaN(seconds) ? 0 : seconds; // Return 0 if parsing fails
}

// Enhanced headers to better mimic a real browser
const getBrowserLikeHeaders = () => {
  return {
    "User-Agent":
      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
    Accept:
      "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Sec-Fetch-Site": "same-origin",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-User": "?1",
    "Sec-Fetch-Dest": "document",
    "sec-ch-ua":
      '"Chromium";v="123", "Google Chrome";v="123", "Not:A-Brand";v="99"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "Upgrade-Insecure-Requests": "1",
    Referer: "https://www.youtube.com/",
    "Cache-Control": "max-age=0",
    // Additional headers to help bypass proxy detection
    "X-Forwarded-For": "66.249.66.1", // Simulating a Google crawler IP
    "X-Forwarded-Host": "www.youtube.com",
    DNT: "1", // Do Not Track
    Connection: "keep-alive",
  };
};

// Storage for cookies and session data between requests
let cookieJar = "";
let consentToken = "";
// Track last request time to avoid rate limiting
let lastRequestTime = 0;
const MIN_REQUEST_INTERVAL = 500; // Minimum 500ms between requests

// Simple cookie parser
const parseCookies = (cookieHeader: string | null): Record<string, string> => {
  const cookies: Record<string, string> = {};
  if (!cookieHeader) return cookies;

  cookieHeader.split(";").forEach((cookie) => {
    const parts = cookie.split("=");
    if (parts.length >= 2) {
      const name = parts[0].trim();
      const value = parts.slice(1).join("=").trim();
      cookies[name] = value;
    }
  });
  return cookies;
};

// Merge cookies to maintain session
const mergeCookies = (
  existingCookies: string,
  newCookieHeader: string | null
): string => {
  if (!newCookieHeader) return existingCookies;

  const existing = parseCookies(existingCookies);
  const newCookies = parseCookies(newCookieHeader);

  // Merge cookies, new ones override existing
  const merged = { ...existing, ...newCookies };

  // Convert back to cookie string
  return Object.entries(merged)
    .map(([name, value]) => `${name}=${value}`)
    .join("; ");
};

// Simple cache for HTML content by videoId
const htmlCache: Record<string, { html: string; timestamp: number }> = {};
const CACHE_TTL = 60 * 1000; // 1 minute cache

// Add rate limiting protection
const waitBetweenRequests = async (): Promise<void> => {
  const now = Date.now();
  const elapsed = now - lastRequestTime;

  if (elapsed < MIN_REQUEST_INTERVAL) {
    // Add some randomness to make the delay less predictable (450-550ms)
    const delay = MIN_REQUEST_INTERVAL - elapsed + (Math.random() * 100 - 50);
    await new Promise((resolve) => setTimeout(resolve, delay));
  }

  lastRequestTime = Date.now();
};

// Helper function to establish YouTube session and get initial cookies
async function establishYouTubeSession(): Promise<boolean> {
  try {
    // Reset cookie jar for a fresh session
    cookieJar = "";

    // Wait to avoid rate limiting
    await waitBetweenRequests();

    // Initial touch to get YouTube cookies and possibly consent page
    const initialResponse = await fetch("https://www.youtube.com/", {
      headers: getBrowserLikeHeaders(),
      signal: AbortSignal.timeout(10000),
      redirect: "follow",
    });

    // Save cookies from initial request
    const setCookieHeader = initialResponse.headers.get("set-cookie");
    if (setCookieHeader) {
      cookieJar = setCookieHeader;

      // Look for and process consent tokens if available
      const html = await initialResponse.text();
      const consentMatch = html.match(/consent.youtube.com\/[^"]+/);
      if (consentMatch) {
        const consentUrl = `https://${consentMatch[0]}`;
        await processConsentPage(consentUrl);
      }

      return true;
    }
    return false;
  } catch (error) {
    console.warn(`Failed to establish YouTube session: ${error}`);
    return false;
  }
}

// Helper function to handle YouTube consent pages
async function processConsentPage(consentUrl: string): Promise<void> {
  try {
    // Wait to avoid rate limiting
    await waitBetweenRequests();

    // First fetch the consent page
    const consentPageResponse = await fetch(consentUrl, {
      headers: {
        ...getBrowserLikeHeaders(),
        ...(cookieJar ? { Cookie: cookieJar } : {}),
      },
      redirect: "follow",
    });

    // Update cookies
    const consentCookies = consentPageResponse.headers.get("set-cookie");
    if (consentCookies) {
      cookieJar = mergeCookies(cookieJar, consentCookies);
    }

    // Parse the consent page
    const consentHtml = await consentPageResponse.text();

    // Find the form data needed for consent
    const formMatch = consentHtml.match(/<form[^>]*>[\s\S]*?<\/form>/i);
    if (formMatch) {
      // Extract form action URL
      const actionMatch = formMatch[0].match(/action="([^"]+)"/);
      const formAction = actionMatch ? actionMatch[1] : "";

      // Extract hidden fields - fix for TypeScript compatibility
      const hiddenFieldRegex =
        /<input[^>]*type="hidden"[^>]*name="([^"]+)"[^>]*value="([^"]*)"[^>]*>/g;
      const formData = new URLSearchParams();

      // Use a regular RegExp.exec approach instead of matchAll
      let hiddenMatch;
      while ((hiddenMatch = hiddenFieldRegex.exec(formMatch[0])) !== null) {
        formData.append(hiddenMatch[1], hiddenMatch[2]);
      }

      // Add consent selection (agree to all)
      formData.append("consent_submitted", "true");
      formData.append("continue", "https://www.youtube.com/");
      formData.append("bl", "boq_identityfrontenduiserver_20231128.03_p0");
      formData.append("hl", "en");
      formData.append("consent_ack", "yes");
      formData.append("consent_hl", "en");
      formData.append("consent_gac", "1");

      // Wait to avoid rate limiting
      await waitBetweenRequests();

      // Submit the consent form
      const submitResponse = await fetch(formAction || consentUrl, {
        method: "POST",
        headers: {
          ...getBrowserLikeHeaders(),
          "Content-Type": "application/x-www-form-urlencoded",
          ...(cookieJar ? { Cookie: cookieJar } : {}),
        },
        body: formData.toString(),
        redirect: "follow",
      });

      // Update cookies once more
      const submitCookies = submitResponse.headers.get("set-cookie");
      if (submitCookies) {
        cookieJar = mergeCookies(cookieJar, submitCookies);
        console.log("Processed YouTube consent page successfully");
      }
    }
  } catch (error) {
    console.warn(`Failed to process consent page: ${error}`);
  }
}

// Extracts caption data from HTML, finds URL, fetches, and parses transcript
async function extractAndParseTranscriptFromHtml(
  html: string,
  videoId: string
): Promise<TranscriptLine[] | null> {
  console.log(`[${videoId}] Extracting transcript data from HTML...`);
  let rawCaptionsData = null;

  // More comprehensive patterns to extract captions data
  const patterns = [
    // Standard pattern for captions object
    /"captions":\s*(\{.*?"captionTracks":.*?\}),\s*"videoDetails"/,

    // Alternative pattern for direct captionTracks array
    /"captionTracks":\s*(\[.*?\])/,

    // Direct baseUrl extraction pattern
    /"baseUrl":"(https:\/\/www\.youtube\.com\/api\/timedtext[^"]+)"/,

    // Newer YouTube format with embedded JSON
    /\{"captionTracks":(\[.*?\]),"audioTracks"/,

    // Patterns for various JSON structures
    /"playerCaptionsTracklistRenderer"\s*:\s*(\{.*?\})/,

    // Special pattern for timedtext in video_id format
    new RegExp(`\\/api\\/timedtext\\?.*?v=${videoId}[^"]+`, "g"),

    // Try to find playerResponse JSON object which contains captions
    /ytInitialPlayerResponse\s*=\s*(\{.*?\});/,

    // New pattern for finding playerResponse in newer YouTube structure
    /"playerResponse":"(\{.*?\})"/,

    // Additional pattern for player_response in legacy structure
    /"player_response":"(.*?)"/,

    // Pattern for initial data in newer YouTube
    /ytInitialData\s*=\s*(\{.*?\});/,

    // Pattern for finding caption tracks in ytcfg data
    /"CAPTION_TRACKS_RESPONSE":"(.*?)"/,
  ];

  // First try to extract from ytInitialPlayerResponse which is more reliable
  const playerResponseMatch = html.match(
    /ytInitialPlayerResponse\s*=\s*(\{.*?\});/
  );

  if (playerResponseMatch && playerResponseMatch[1]) {
    try {
      const playerResponse = JSON.parse(playerResponseMatch[1]);
      // Check if captions exist in the player response
      if (
        playerResponse?.captions?.playerCaptionsTracklistRenderer?.captionTracks
      ) {
        console.log(`[${videoId}] Found captions in ytInitialPlayerResponse`);
        rawCaptionsData = JSON.stringify(playerResponse.captions);
      }
    } catch (e) {
      console.warn(
        `[${videoId}] Failed to parse ytInitialPlayerResponse: ${e}`
      );
    }
  }

  // If we couldn't get data from ytInitialPlayerResponse, try other patterns
  if (!rawCaptionsData) {
    for (const pattern of patterns) {
      const match = html.match(pattern);
      if (match && match[1]) {
        let matchedData = match[1].replace(/\\"/g, '"').replace(/\\\\/g, "\\");

        // Handle URL encoded JSON (common in player_response)
        if (
          pattern.source.includes("player_response") ||
          pattern.source.includes("CAPTION_TRACKS_RESPONSE")
        ) {
          try {
            matchedData = decodeURIComponent(matchedData);
          } catch (e) {
            console.warn(`[${videoId}] Failed to decode URI component: ${e}`);
          }
        }

        console.log(
          `[${videoId}] Found potential captions data using pattern: ${pattern.source.substring(
            0,
            30
          )}...`
        );

        if (
          pattern.source.includes("baseUrl") ||
          pattern.source.includes("timedtext")
        ) {
          // Handle direct URL pattern
          try {
            // If it's already a URL, use it directly; otherwise, decode it
            let decodedUrl = matchedData;
            if (matchedData.includes('\\"')) {
              decodedUrl = decodeURIComponent(JSON.parse(`"${matchedData}"`)); // Safer decoding
            }

            // If the URL is relative, make it absolute
            if (decodedUrl.startsWith("/api/timedtext")) {
              decodedUrl = `https://www.youtube.com${decodedUrl}`;
            }

            rawCaptionsData = JSON.stringify({
              playerCaptionsTracklistRenderer: {
                captionTracks: [{ baseUrl: decodedUrl }],
              },
            });
            console.log(
              `[${videoId}] Extracted direct transcript URL: ${decodedUrl.substring(
                0,
                60
              )}...`
            );
          } catch (e) {
            console.warn(`[${videoId}] Failed to decode URL: ${e}`);
          }
        } else {
          // Attempt to parse as JSON for other patterns
          try {
            // Enhanced JSON parsing with better error handling
            let parsed;

            // Handle escaped JSON strings
            if (matchedData.startsWith('"') && matchedData.endsWith('"')) {
              try {
                matchedData = JSON.parse(matchedData);
              } catch (e) {
                console.warn(`[${videoId}] Failed to parse JSON string: ${e}`);
              }
            }

            let dataToParse = matchedData;
            try {
              if (pattern.source.includes("ytInitialData")) {
                // Pre-process specifically for ytInitialData:
                // Replace unescaped newlines within potential string values.
                dataToParse = dataToParse
                  .replace(/(?<!\\)\n/g, "\\\\n")
                  .replace(/(?<!\\)\r/g, "\\\\r");
              }
              parsed = JSON.parse(dataToParse);
            } catch (e: any) {
              if (pattern.source.includes("ytInitialData")) {
                console.error(
                  `[${videoId}] Failed to parse ytInitialData. Error: ${e.message}`
                );
                const errorPositionMatch = e.message.match(/(\d+)/);
                if (errorPositionMatch && errorPositionMatch[1]) {
                  const errorPos = parseInt(errorPositionMatch[1], 10);
                  const contextChars = 100;
                  const startIndex = Math.max(0, errorPos - contextChars);
                  const endIndex = Math.min(
                    dataToParse.length,
                    errorPos + contextChars
                  );
                  const errorContext = dataToParse.substring(
                    startIndex,
                    endIndex
                  );
                  console.error(
                    `[${videoId}] Raw data around error (pos ${errorPos}): ...${errorContext}...`
                  );
                  console.error(
                    `[${videoId}] Total length of matchedData for ytInitialData: ${dataToParse.length}`
                  );
                } else {
                  console.error(
                    `[${videoId}] Could not extract error position from message: ${e.message}`
                  );
                  console.error(
                    `[${videoId}] Full matchedData for ytInitialData (first 500 chars): ${dataToParse.substring(
                      0,
                      500
                    )}`
                  );
                }
                console.error(
                  `[${videoId}] Stack trace for ytInitialData parsing error: ${e.stack}`
                );
              }
              // Try cleaning the string more aggressively if initial parse fails
              console.warn(
                `[${videoId}] First parse attempt failed for pattern ${pattern.source.substring(
                  0,
                  50
                )}, trying with additional cleaning`
              );
              let retryData = matchedData;
              retryData = retryData
                .replace(/\n/g, "")
                .replace(/\r/g, "")
                .replace(/\t/g, "")
                .replace(/\\x(\d\d)/g, (_, p1) =>
                  String.fromCharCode(parseInt(p1, 16))
                );

              // Try replacing escaped backslashes and quotes more thoroughly
              retryData = retryData
                .replace(/\\\\"/g, '"')
                .replace(/\\"/g, '"')
                .replace(/\\\\/g, "\\");

              try {
                retryData = retryData
                  .replace(/[\\]+"/g, '"')
                  .replace(/[\\]+/g, "\\");
                parsed = JSON.parse(retryData);
              } catch (e3) {
                console.warn(`[${videoId}] Third parse attempt failed: ${e3}`);
                throw e; // Throw the original error
              }
            }

            // Process the parsed data based on its structure
            if (parsed) {
              if (parsed.captionTracks) {
                rawCaptionsData = JSON.stringify({
                  playerCaptionsTracklistRenderer: {
                    captionTracks: parsed.captionTracks,
                  },
                });
              } else if (
                parsed.playerCaptionsTracklistRenderer &&
                parsed.playerCaptionsTracklistRenderer.captionTracks
              ) {
                rawCaptionsData = JSON.stringify(parsed);
              } else if (
                parsed.captions &&
                parsed.captions.playerCaptionsTracklistRenderer
              ) {
                rawCaptionsData = JSON.stringify(parsed.captions);
              } else {
                // Advanced search through the object
                const findCaptionTracks = (obj: any) => {
                  if (!obj || typeof obj !== "object") return null;

                  // Direct match for captionTracks array
                  if (
                    Array.isArray(obj.captionTracks) &&
                    obj.captionTracks.length > 0
                  ) {
                    return {
                      playerCaptionsTracklistRenderer: {
                        captionTracks: obj.captionTracks,
                      },
                    };
                  }

                  // Match for common playerCaptionsTracklistRenderer pattern
                  if (obj.playerCaptionsTracklistRenderer?.captionTracks) {
                    return {
                      playerCaptionsTracklistRenderer:
                        obj.playerCaptionsTracklistRenderer,
                    };
                  }

                  // Look for captions object
                  if (
                    obj.captions?.playerCaptionsTracklistRenderer?.captionTracks
                  ) {
                    return obj.captions;
                  }

                  // Recursively search keys that are objects or arrays
                  for (const key in obj) {
                    if (obj[key] && typeof obj[key] === "object") {
                      const result = findCaptionTracks(obj[key]);
                      if (result) return result;
                    }
                  }

                  return null;
                };

                const captionsObject = findCaptionTracks(parsed);
                if (captionsObject) {
                  rawCaptionsData = JSON.stringify(captionsObject);
                  console.log(
                    `[${videoId}] Found caption tracks through deep object search`
                  );
                }
              }

              if (rawCaptionsData) {
                console.log(`[${videoId}] Successfully parsed extracted JSON.`);
              } else {
                console.warn(
                  `[${videoId}] Parsed JSON has unexpected structure.`
                );
                // Dump first 200 chars of structure for debugging
                console.warn(
                  `[${videoId}] Structure: ${JSON.stringify(parsed).substring(
                    0,
                    200
                  )}`
                );
              }
            }
          } catch (parseError) {
            console.warn(
              `[${videoId}] Failed to parse extracted data for pattern ${pattern.source.substring(
                0,
                30
              )}...: ${parseError}`
            );
          }
        }
        if (rawCaptionsData) break; // Stop if valid data found
      }
    }
  }

  // Last resort: try to find any timedtext URL in the HTML
  if (!rawCaptionsData) {
    const timedTextMatch = html.match(
      /https:\/\/www\.youtube\.com\/api\/timedtext[^"]+/
    );
    if (timedTextMatch) {
      try {
        const timedTextUrl = timedTextMatch[0].replace(/\\u0026/g, "&");
        console.log(
          `[${videoId}] Found fallback timedtext URL: ${timedTextUrl.substring(
            0,
            60
          )}...`
        );
        rawCaptionsData = JSON.stringify({
          playerCaptionsTracklistRenderer: {
            captionTracks: [{ baseUrl: timedTextUrl }],
          },
        });
      } catch (e) {
        console.warn(
          `[${videoId}] Failed to process fallback timedtext URL: ${e}`
        );
      }
    }
  }

  if (!rawCaptionsData) {
    console.error(
      `[${videoId}] No captions data found in video page using any pattern.`
    );
    // Log a snippet of the HTML for debugging
    console.error(`[${videoId}] HTML snippet: ${html.substring(0, 500)}...`);
    return null;
  }

  let captionsData;
  try {
    captionsData = JSON.parse(rawCaptionsData); // Already cleaned during extraction
  } catch (e: any) {
    console.error(
      `[${videoId}] Failed to parse final captions JSON: ${
        e.message
      }. Raw Data: ${rawCaptionsData.substring(0, 200)}...`
    );
    throw new Error(`Failed to parse final captions data: ${e.message}`);
  }

  // More robust extraction of caption tracks
  let captionTracks = null;

  // Check multiple possible structures
  if (captionsData?.playerCaptionsTracklistRenderer?.captionTracks) {
    captionTracks = captionsData.playerCaptionsTracklistRenderer.captionTracks;
  } else if (captionsData?.captionTracks) {
    captionTracks = captionsData.captionTracks;
  }

  if (!captionTracks || captionTracks.length === 0) {
    console.warn(
      `[${videoId}] Parsed captions data lacks track information or is empty.`
    );
    throw new Error("Transcript tracks unavailable or disabled in parsed data");
  }

  // Find the first usable transcript URL (prefer 'en' if available, otherwise take first)
  let transcriptUrl = "";
  const englishTrack = captionTracks.find(
    (track: any) => track.languageCode === "en"
  );
  transcriptUrl = englishTrack?.baseUrl || captionTracks[0]?.baseUrl;

  if (!transcriptUrl) {
    console.error(
      `[${videoId}] Could not find a valid baseUrl in caption tracks.`
    );
    throw new Error("No valid transcript URL found in caption tracks");
  }

  // Add necessary URL parameters if they're missing
  if (!transcriptUrl.includes("lang=")) {
    transcriptUrl += transcriptUrl.includes("?") ? "&lang=en" : "?lang=en";
  }
  if (!transcriptUrl.includes("fmt=")) {
    transcriptUrl += "&fmt=srv3"; // Request a modern format
  }

  console.log(
    `[${videoId}] Using transcript URL: ${transcriptUrl.substring(0, 60)}...`
  );

  // Wait to avoid rate limiting
  await waitBetweenRequests();

  // Fetch the transcript XML/TTML with enhanced error handling
  let transcriptResponse;
  try {
    console.log(`[${videoId}] Fetching transcript content...`);
    transcriptResponse = await fetch(transcriptUrl, {
      headers: getBrowserLikeHeaders(),
      signal: AbortSignal.timeout(10000), // 10 second timeout
    });
  } catch (fetchError: any) {
    console.error(
      `[${videoId}] Failed to fetch transcript content: ${fetchError.message}`
    );
    if (fetchError.name === "AbortError") {
      throw new Error("Transcript fetch timed out - try again later");
    }
    throw new Error(`Failed to fetch transcript: ${fetchError.message}`);
  }

  if (!transcriptResponse.ok) {
    const errorText = await transcriptResponse.text();
    console.error(
      `[${videoId}] Failed to fetch transcript content: ${
        transcriptResponse.status
      } ${transcriptResponse.statusText}. Response: ${errorText.substring(
        0,
        200
      )}`
    );
    throw new Error(
      `Failed to fetch transcript data: ${transcriptResponse.status} ${transcriptResponse.statusText}`
    );
  }

  const transcriptContent = await transcriptResponse.text();

  // Enhanced validation of transcript content
  if (!transcriptContent || transcriptContent.length < 50) {
    console.error(`[${videoId}] Invalid or empty transcript content received.`);
    throw new Error("Invalid or empty transcript data received");
  }

  // Enhanced content type detection
  const contentType = transcriptResponse.headers.get("content-type");
  console.log(`[${videoId}] Transcript content type: ${contentType}`);

  // More robust check: look for common transcript tags
  const hasTextTags = transcriptContent.includes("<text");
  const hasPTags = transcriptContent.includes("<p");

  if (!hasTextTags && !hasPTags) {
    console.warn(
      `[${videoId}] Transcript content might be invalid (missing common tags): ${transcriptContent.substring(
        0,
        200
      )}...`
    );
    // Don't throw immediately, still try to parse
  }

  // Parse the XML/TTML to extract transcript segments
  const transcript = [];

  // Handle standard XML format <text start="..." dur="...">...</text>
  const xmlRegex = /<text start="([^"]*)" dur="([^"]*)">([^<]*)<\/text>/g;

  // Handle alternative format (often in TTML) <p begin="..." end="..." ...>...</p>
  const ttmlRegex = /<p begin="([^"]*)" end="([^"]*)"[^>]*>([^<]*)<\/p>/g;

  // Handle another TTML variant
  const ttmlRegex2 = /<p t="([^"]*)" d="([^"]*)"[^>]*>([^<]*)<\/p>/g;

  // Handle yet another YouTube variant
  const ttmlRegex3 =
    /<p id="[^"]*" begin="([^"]*)" end="([^"]*)"[^>]*>([^<]*)<\/p>/g;

  let match;
  let foundSegments = false;

  // Try XML first
  while ((match = xmlRegex.exec(transcriptContent)) !== null) {
    foundSegments = true;
    const offset = parseFloat(match[1]);
    const duration = parseFloat(match[2]);
    // Basic sanity check for parsed numbers
    if (!isNaN(offset) && !isNaN(duration)) {
      transcript.push({
        text: decodeAndCleanText(match[3]),
        duration: duration,
        offset: offset,
      });
    } else {
      console.warn(
        `[${videoId}] Skipping segment due to invalid number format (XML): start='${match[1]}', dur='${match[2]}'`
      );
    }
  }

  // If XML parsing yielded nothing, try TTML format
  if (transcript.length === 0) {
    console.log(
      `[${videoId}] XML pattern found no segments, trying TTML pattern...`
    );
    while ((match = ttmlRegex.exec(transcriptContent)) !== null) {
      foundSegments = true;
      const offset = parseTimestamp(match[1]); // Need helper to parse HH:MM:SS.ms
      const end = parseTimestamp(match[2]);
      const duration = end - offset;
      // Basic sanity check for parsed numbers
      if (!isNaN(offset) && !isNaN(duration) && duration >= 0) {
        transcript.push({
          text: decodeAndCleanText(match[3]),
          duration: duration,
          offset: offset,
        });
      } else {
        console.warn(
          `[${videoId}] Skipping segment due to invalid number format (TTML): begin='${match[1]}', end='${match[2]}'`
        );
      }
    }
  }

  // Try the second TTML variant if still no results
  if (transcript.length === 0) {
    console.log(
      `[${videoId}] TTML pattern found no segments, trying alternative TTML pattern...`
    );
    while ((match = ttmlRegex2.exec(transcriptContent)) !== null) {
      foundSegments = true;
      const offset = parseFloat(match[1]) / 1000; // Convert ms to seconds
      const duration = parseFloat(match[2]) / 1000; // Convert ms to seconds

      if (!isNaN(offset) && !isNaN(duration) && duration >= 0) {
        transcript.push({
          text: decodeAndCleanText(match[3]),
          duration: duration,
          offset: offset,
        });
      } else {
        console.warn(
          `[${videoId}] Skipping segment due to invalid number format (TTML2): t='${match[1]}', d='${match[2]}'`
        );
      }
    }
  }

  // Try the third TTML variant if still no results
  if (transcript.length === 0) {
    console.log(
      `[${videoId}] Alternative TTML pattern found no segments, trying third TTML pattern...`
    );
    while ((match = ttmlRegex3.exec(transcriptContent)) !== null) {
      foundSegments = true;
      const offset = parseTimestamp(match[1]);
      const end = parseTimestamp(match[2]);
      const duration = end - offset;

      if (!isNaN(offset) && !isNaN(duration) && duration >= 0) {
        transcript.push({
          text: decodeAndCleanText(match[3]),
          duration: duration,
          offset: offset,
        });
      } else {
        console.warn(
          `[${videoId}] Skipping segment due to invalid number format (TTML3): begin='${match[1]}', end='${match[2]}'`
        );
      }
    }
  }

  if (transcript.length === 0) {
    if (foundSegments) {
      console.error(
        `[${videoId}] Found transcript segments but failed to parse them properly. Content sample: ${transcriptContent.substring(
          0,
          500
        )}...`
      );
    } else {
      console.error(
        `[${videoId}] Failed to parse transcript content - no segments found. Content sample: ${transcriptContent.substring(
          0,
          500
        )}...`
      );
    }
    throw new Error("Failed to parse transcript XML/TTML content");
  }

  console.log(
    `[${videoId}] Successfully fetched and parsed transcript: ${transcript.length} segments`
  );
  return transcript;
}

// Helper function to decode and clean text from XML/HTML
function decodeAndCleanText(text: string): string {
  return text
    .replace(/&amp;/g, "&")
    .replace(/&lt;/g, "<")
    .replace(/&gt;/g, ">")
    .replace(/&#39;/g, "'")
    .replace(/&quot;/g, '"')
    .replace(/\\n/g, " ")
    .replace(/\s+/g, " ") // Normalize whitespace
    .trim();
}

// Extracts caption data from HTML, finds URL, fetches, and parses transcript
async function fetchTranscriptDirect(
  videoId: string
): Promise<TranscriptLine[] | null> {
  console.log(`[${videoId}] Attempting Direct Fetch Strategy...`);

  try {
    // Check if we already have a session, if not establish one
    if (!cookieJar) {
      console.log(
        `[${videoId}] No active session, establishing YouTube session first...`
      );
      await establishYouTubeSession();
    }

    // Check cache first
    if (
      htmlCache[videoId] &&
      Date.now() - htmlCache[videoId].timestamp < CACHE_TTL
    ) {
      console.log(`[${videoId}] Using cached HTML content`);
      const transcript = await extractAndParseTranscriptFromHtml(
        htmlCache[videoId].html,
        videoId
      );
      if (transcript) {
        return transcript;
      }
      // If parsing failed with cached content, clear cache and try fresh fetch
      delete htmlCache[videoId];
    }

    // Random delay to mimic human behavior (100-300ms)
    await waitBetweenRequests();
    await new Promise((resolve) =>
      setTimeout(resolve, 100 + Math.random() * 200)
    );

    // Log request headers for debugging
    const requestHeaders = {
      ...getBrowserLikeHeaders(),
      ...(cookieJar ? { Cookie: cookieJar } : {}),
    };
    console.log(
      `[${videoId}] Request headers (subset): User-Agent=${requestHeaders[
        "User-Agent"
      ]?.substring(0, 30)}..., Accept-Language=${
        requestHeaders["Accept-Language"]
      }, Cookie=${cookieJar ? "Set" : "Not set"}`
    );

    // Main video page request with cookies
    const response = await fetch(`https://www.youtube.com/watch?v=${videoId}`, {
      headers: requestHeaders,
      signal: AbortSignal.timeout(15000),
    });

    // Log response details for debugging
    console.log(
      `[${videoId}] YouTube response status: ${response.status}, URL: ${
        response.url
      }, Content-Type: ${response.headers.get("content-type")}`
    );

    // Check if we were redirected to consent page
    const finalUrl = response.url;
    if (finalUrl.includes("consent.youtube.com")) {
      console.log(`[${videoId}] Redirected to consent page, processing...`);
      await processConsentPage(finalUrl);

      // Retry the main request after consent
      console.log(
        `[${videoId}] Retrying main request after processing consent...`
      );
      await waitBetweenRequests();
      const retryResponse = await fetch(
        `https://www.youtube.com/watch?v=${videoId}`,
        {
          headers: {
            ...getBrowserLikeHeaders(),
            ...(cookieJar ? { Cookie: cookieJar } : {}),
          },
          signal: AbortSignal.timeout(15000),
        }
      );

      // Log retry response details
      console.log(
        `[${videoId}] Retry response status: ${retryResponse.status}, URL: ${retryResponse.url}`
      );

      // Update cookies from the retry response
      const retryCookies = retryResponse.headers.get("set-cookie");
      if (retryCookies) {
        cookieJar = mergeCookies(cookieJar, retryCookies);
      }

      // Continue with this response
      if (!retryResponse.ok) {
        throw new Error(
          `Failed to fetch video page after consent: ${retryResponse.status}`
        );
      }

      const html = await retryResponse.text();
      // Log HTML size and check for key markers
      console.log(
        `[${videoId}] Received HTML size: ${
          html.length
        }, Contains captions data: ${html.includes(
          "captionTracks"
        )}, Contains player data: ${html.includes("ytInitialPlayerResponse")}`
      );

      // Cache the HTML content
      htmlCache[videoId] = { html, timestamp: Date.now() };

      // Continue with parsing
      const transcript = await extractAndParseTranscriptFromHtml(html, videoId);
      if (!transcript) {
        throw new Error("Transcript extraction/parsing failed after consent");
      }
      return transcript;
    }

    // Update cookies from the video page response
    const videoPageCookies = response.headers.get("set-cookie");
    if (videoPageCookies) {
      cookieJar = mergeCookies(cookieJar, videoPageCookies);
    }

    if (!response.ok) {
      throw new Error(
        `Failed to fetch video page: ${response.status} ${response.statusText}`
      );
    }

    const contentType = response.headers.get("content-type");
    if (!contentType || !contentType.includes("text/html")) {
      console.warn(
        `[${videoId}] Direct fetch received unexpected content-type: ${contentType}. Content might be blocked.`
      );
      // Attempt to read text anyway, might contain error info
      const maybeErrorText = await response.text();
      console.warn(
        `[${videoId}] Received content snippet: ${maybeErrorText.substring(
          0,
          500
        )}`
      );
      throw new Error(`Invalid content type returned: ${contentType}`);
    }

    const html = await response.text();
    if (!html || html.length < 500) {
      throw new Error(
        `Empty or too short response from YouTube (Length: ${html?.length})`
      );
    }

    // Log HTML content diagnostics
    console.log(
      `[${videoId}] HTML content length: ${
        html.length
      }, Contains captions data: ${html.includes(
        "captionTracks"
      )}, Contains player data: ${html.includes("ytInitialPlayerResponse")}`
    );

    // Update cache with the fresh HTML
    htmlCache[videoId] = { html, timestamp: Date.now() };

    // Call the combined extraction and parsing function
    const transcript = await extractAndParseTranscriptFromHtml(html, videoId);

    if (!transcript) {
      // Error logging happens inside extractAndParseTranscriptFromHtml
      throw new Error("Transcript extraction/parsing failed");
    }

    return transcript;
  } catch (error: any) {
    console.error(`[${videoId}] Direct Fetch Strategy failed:`, error.message);
    // Log specific failure details if available
    if (error.message.includes("No captions data found")) {
      console.error(
        `[${videoId}] Specific failure: Could not find captions JSON/patterns in HTML.`
      );
    }
    // Re-throw or return null/empty based on desired handling for the sequence
    // Throwing allows the sequence runner to catch and log appropriately
    throw error; // Let the calling sequence handle the failure logging for this method
  }
}

// fetchTranscriptInnertube remains largely the same, as it relies on a different initial fetch mechanism
// Potentially add more specific parsing/error handling within it if needed.
async function fetchTranscriptInnertube(
  videoId: string
): Promise<TranscriptLine[] | null> {
  console.log(`[${videoId}] Attempting Innertube API Strategy...`);

  try {
    // Wait to avoid rate limiting
    await waitBetweenRequests();

    // 1. Fetch Initial Page Content with enhanced headers
    const initialResponse = await fetch(
      `https://www.youtube.com/watch?v=${videoId}`,
      {
        headers: {
          ...getBrowserLikeHeaders(),
          ...(cookieJar ? { Cookie: cookieJar } : {}),
        },
        signal: AbortSignal.timeout(15000),
      }
    );

    // Update cookies from this response
    const setCookieHeader = initialResponse.headers.get("set-cookie");
    if (setCookieHeader) {
      cookieJar = mergeCookies(cookieJar, setCookieHeader);
    }

    if (!initialResponse.ok) {
      throw new Error(
        `Innertube initial page fetch failed: ${initialResponse.status} ${initialResponse.statusText}`
      );
    }
    const html = await initialResponse.text();

    // 2. Extract Innertube API Key and Context from the HTML
    // Simplified regex - robust extraction is complex and brittle
    const apiKeyMatch = html.match(/"INNERTUBE_API_KEY":"(.*?)"/);
    const clientVersionMatch = html.match(
      /"INNERTUBE_CONTEXT_CLIENT_VERSION":"(.*?)"/
    );

    if (!apiKeyMatch || !clientVersionMatch) {
      console.error(
        `[${videoId}] Failed to extract Innertube API key or client version from page.`
      );
      // Maybe fallback to trying transcript extraction from this HTML directly?
      console.log(
        `[${videoId}] Attempting direct extraction from Innertube strategy's initial fetch...`
      );
      return await extractAndParseTranscriptFromHtml(html, videoId); // Fallback within fallback
      // throw new Error("Failed to extract Innertube API key/version");
    }
    const INNERTUBE_API_KEY = apiKeyMatch[1];
    const INNERTUBE_CONTEXT = {
      client: {
        clientName: "WEB",
        clientVersion: clientVersionMatch[1],
        // Other context fields might be necessary depending on YT changes
      },
    };

    console.log(
      `[${videoId}] Extracted Innertube Key: ${INNERTUBE_API_KEY.substring(
        0,
        10
      )}..., Version: ${INNERTUBE_CONTEXT.client.clientVersion}`
    );

    // Wait to avoid rate limiting
    await waitBetweenRequests();

    // 3. Make the Innertube API request with enhanced headers
    const playerApiUrl = `https://www.youtube.com/youtubei/v1/player?key=${INNERTUBE_API_KEY}`;
    const playerApiResponse = await fetch(playerApiUrl, {
      method: "POST",
      headers: {
        ...getBrowserLikeHeaders(),
        "Content-Type": "application/json",
        "X-YouTube-Client-Name": "1",
        "X-YouTube-Client-Version": INNERTUBE_CONTEXT.client.clientVersion,
        ...(cookieJar ? { Cookie: cookieJar } : {}),
      },
      body: JSON.stringify({
        context: INNERTUBE_CONTEXT,
        videoId: videoId,
        contentCheckOk: true,
        racyCheckOk: true,
      }),
      signal: AbortSignal.timeout(10000),
    });

    // Update cookies from this API response
    const apiCookies = playerApiResponse.headers.get("set-cookie");
    if (apiCookies) {
      cookieJar = mergeCookies(cookieJar, apiCookies);
    }

    if (!playerApiResponse.ok) {
      const errorText = await playerApiResponse.text();
      console.error(
        `[${videoId}] Innertube Player API request failed: ${
          playerApiResponse.status
        }. Response: ${errorText.substring(0, 300)}`
      );
      throw new Error(
        `Innertube Player API request failed: ${playerApiResponse.status}`
      );
    }

    const playerResponse = await playerApiResponse.json();

    // 4. Find Caption Tracks within Player Response
    if (
      !playerResponse?.captions?.playerCaptionsTracklistRenderer
        ?.captionTracks ||
      playerResponse.captions.playerCaptionsTracklistRenderer.captionTracks
        .length === 0
    ) {
      console.error(
        `[${videoId}] No caption tracks found in Innertube player response`
      );
      console.error(
        "[Innertube Debug] No caption tracks found. Player Response Snippet:",
        JSON.stringify(playerResponse?.captions || {}).substring(0, 1000)
      );
      console.error(
        "[Innertube Info] No caption tracks listed in player response."
      );
      // Don't throw yet, maybe try extracting from initial page fetch as last resort
      // throw new Error('No caption tracks found in Innertube player response');
      console.log(
        `[${videoId}] Innertube API had no tracks, attempting direct extraction from initial page fetch...`
      );
      return await extractAndParseTranscriptFromHtml(html, videoId); // Final fallback attempt
    }

    const captionTracks =
      playerResponse.captions.playerCaptionsTracklistRenderer.captionTracks;

    // 5. Select and Fetch Transcript URL (similar to direct method)
    let transcriptUrl = "";
    const englishTrack = captionTracks.find(
      (track: any) => track.languageCode === "en"
    );
    transcriptUrl = englishTrack?.baseUrl || captionTracks[0]?.baseUrl;

    if (!transcriptUrl) {
      console.error(
        `[${videoId}] Could not find a valid baseUrl in Innertube caption tracks.`
      );
      return null; // Or throw
    }
    console.log(
      `[${videoId}] Using Innertube transcript URL: ${transcriptUrl.substring(
        0,
        60
      )}...`
    );

    // Wait to avoid rate limiting
    await waitBetweenRequests();

    // 6. Fetch the transcript with enhanced headers
    const transcriptResponse = await fetch(transcriptUrl, {
      headers: {
        ...getBrowserLikeHeaders(),
        ...(cookieJar ? { Cookie: cookieJar } : {}),
      },
      signal: AbortSignal.timeout(10000),
    });

    // Update cookies again
    const transcriptCookies = transcriptResponse.headers.get("set-cookie");
    if (transcriptCookies) {
      cookieJar = mergeCookies(cookieJar, transcriptCookies);
    }

    if (!transcriptResponse.ok) {
      const errorText = await transcriptResponse.text();
      console.error(
        `[${videoId}] Failed to fetch Innertube transcript content: ${
          transcriptResponse.status
        } ${transcriptResponse.statusText}. Response: ${errorText.substring(
          0,
          200
        )}`
      );
      throw new Error(
        `Failed to fetch Innertube transcript data: ${transcriptResponse.status} ${transcriptResponse.statusText}`
      );
    }

    const transcriptContent = await transcriptResponse.text();
    if (!transcriptContent || transcriptContent.length < 50) {
      console.error(
        `[${videoId}] Invalid or empty Innertube transcript content received.`
      );
      return null;
    }
    if (
      !transcriptContent.includes("<text") &&
      !transcriptContent.includes("<p")
    ) {
      console.warn(
        `[${videoId}] Innertube transcript content might be invalid (missing common tags): ${transcriptContent.substring(
          0,
          200
        )}...`
      );
    }

    // Parse the XML/TTML to extract transcript segments
    const transcript: TranscriptLine[] = [];
    const xmlRegex = /<text start="([^"]*)" dur="([^"]*)">([^<]*)<\/text>/g;
    const ttmlRegex = /<p begin="([^"]*)" end="([^"]*)"[^>]*>([^<]*)<\/p>/g;
    let match;

    while ((match = xmlRegex.exec(transcriptContent)) !== null) {
      const offset = parseFloat(match[1]);
      const duration = parseFloat(match[2]);
      if (!isNaN(offset) && !isNaN(duration)) {
        transcript.push({
          text: decodeAndCleanText(match[3]),
          duration: duration,
          offset: offset,
        });
      }
    }

    if (transcript.length === 0) {
      console.log(
        `[${videoId}] Innertube XML pattern found no segments, trying TTML pattern...`
      );
      while ((match = ttmlRegex.exec(transcriptContent)) !== null) {
        const offset = parseTimestamp(match[1]);
        const end = parseTimestamp(match[2]);
        const duration = end - offset;
        if (!isNaN(offset) && !isNaN(duration) && duration >= 0) {
          transcript.push({
            text: decodeAndCleanText(match[3]),
            duration: duration,
            offset: offset,
          });
        }
      }
    }

    if (transcript.length === 0) {
      console.error(
        `[${videoId}] Failed to parse Innertube transcript content using both XML and TTML patterns.`
      );
      return null;
    }

    console.log(
      `[${videoId}] Successfully fetched and parsed transcript via Innertube API: ${transcript.length} segments`
    );
    return transcript;
  } catch (error: any) {
    console.error(`[${videoId}] Innertube API Strategy failed:`, error.message);
    // Check if it failed before even getting tracks, if so, try direct extraction from initial page fetch
    if (
      error.message.includes("Failed to extract Innertube API key") ||
      error.message.includes("Innertube Player API request failed")
    ) {
      console.log(
        `[${videoId}] Innertube core API failed, attempting direct extraction from initial page fetch...`
      );
      // Need to ensure html was fetched if error occurred early
      try {
        // Wait to avoid rate limiting
        await waitBetweenRequests();

        const initialResponse = await fetch(
          `https://www.youtube.com/watch?v=${videoId}`,
          {
            headers: getBrowserLikeHeaders(),
            signal: AbortSignal.timeout(15000),
          }
        );
        if (initialResponse.ok) {
          const html = await initialResponse.text();
          return await extractAndParseTranscriptFromHtml(html, videoId);
        }
      } catch (fallbackError: any) {
        console.error(
          `[${videoId}] Final fallback extraction also failed:`,
          fallbackError.message
        );
      }
    }
    throw error; // Let the calling sequence handle the failure logging for this method
  }
}

// Helper function to check if video likely has no captions available
async function checkIfCaptionsUnavailable(videoId: string): Promise<boolean> {
  try {
    // Fetch the video page to look for indicators that captions are unavailable
    const response = await fetch(`https://www.youtube.com/watch?v=${videoId}`, {
      headers: getBrowserLikeHeaders(),
      signal: AbortSignal.timeout(10000),
    });

    if (!response.ok) {
      return false; // Can't determine, don't assume unavailable
    }

    const html = await response.text();

    // Check for markers that indicate no captions
    const noSubtitlesIndicators = [
      'class="ytp-subtitles-button ytp-button" style="display: none;"', // Subtitle button hidden
      '"captionTracks":[]', // Empty captions tracks array
      '"hasCaptions":false', // Explicit indicator
    ];

    for (const indicator of noSubtitlesIndicators) {
      if (html.includes(indicator)) {
        console.log(
          `[${videoId}] Found indicator that captions are unavailable: ${indicator}`
        );
        return true;
      }
    }

    // Check video metadata section for captions flag - without using 's' flag
    const playerResponseMatch = html.match(
      /"playerResponse":\s*(\{[\s\S]*?\}\});/
    );
    if (playerResponseMatch && playerResponseMatch[1]) {
      try {
        const playerData = JSON.parse(playerResponseMatch[1]);
        if (playerData?.videoDetails?.isCrawlable === false) {
          console.log(
            `[${videoId}] Video is marked as not crawlable, captions likely unavailable`
          );
          return true;
        }

        // Check if captions are explicitly disabled
        if (
          playerData?.captions?.playerCaptionsTracklistRenderer
            ?.captionTracks === undefined ||
          (Array.isArray(
            playerData?.captions?.playerCaptionsTracklistRenderer?.captionTracks
          ) &&
            playerData?.captions?.playerCaptionsTracklistRenderer?.captionTracks
              .length === 0)
        ) {
          console.log(
            `[${videoId}] No caption tracks found in player response`
          );
          return true;
        }
      } catch (e) {
        console.warn(`[${videoId}] Error parsing player response:`, e);
      }
    }

    return false; // No clear indication that captions are unavailable
  } catch (error) {
    console.warn(
      `[${videoId}] Error checking if captions are unavailable:`,
      error
    );
    return false; // Can't determine, don't assume unavailable
  }
}

// Export the main transcript fetching functions
export async function fetchTranscript(
  videoId: string
): Promise<TranscriptLine[] | null> {
  console.log(
    `[${videoId}] Starting transcript fetching sequence with all methods...`
  );
  let lastError: Error | null = null;

  // First check if captions are likely unavailable to avoid unnecessary attempts
  const captionsLikelyUnavailable = await checkIfCaptionsUnavailable(videoId);
  if (captionsLikelyUnavailable) {
    console.log(
      `[${videoId}] Pre-check indicates captions are unavailable for this video`
    );
    throw new Error(
      "This video doesn't have captions or has disabled captions. Consider using a video with captions or a different video."
    );
  }

  // Try each method in sequence
  try {
    console.log(`[${videoId}] Trying transcript method: Library...`);
    const transcript = await fetchTranscriptLibrary(videoId);
    if (transcript && transcript.length > 0) {
      console.log(
        `[${videoId}] Transcript successfully fetched using method: Library (${transcript.length} segments).`
      );
      return transcript;
    }
  } catch (error: any) {
    console.warn(`[${videoId}] Transcript method Library failed.`);
    lastError = error;
  }

  try {
    console.log(`[${videoId}] Trying transcript method: Direct...`);
    const transcript = await fetchTranscriptDirect(videoId);
    if (transcript && transcript.length > 0) {
      console.log(
        `[${videoId}] Transcript successfully fetched using method: Direct (${transcript.length} segments).`
      );
      return transcript;
    }
  } catch (error: any) {
    console.warn(`[${videoId}] Transcript method Direct failed.`);
    lastError = error;
  }

  try {
    console.log(`[${videoId}] Trying transcript method: Innertube...`);
    const transcript = await fetchTranscriptInnertube(videoId);
    if (transcript && transcript.length > 0) {
      console.log(
        `[${videoId}] Transcript successfully fetched using method: Innertube (${transcript.length} segments).`
      );
      return transcript;
    } else if (transcript === null) {
      console.warn(
        `[${videoId}] Transcript method Innertube completed but returned null/undefined.`
      );
    }
  } catch (error: any) {
    console.warn(`[${videoId}] Transcript method Innertube failed.`);
    lastError = error;
  }

  // If we get here, all methods failed
  if (lastError) {
    // Check errors for specific patterns
    if (
      lastError.message.includes("No captions data found") ||
      lastError.message.includes("No valid transcript URL found") ||
      lastError.message.includes("Transcript tracks unavailable")
    ) {
      console.error(
        `[${videoId}] This video doesn't appear to have captions available.`
      );
      throw new Error(
        "This video doesn't have captions or has disabled captions. Consider using a video with captions or a different video."
      );
    } else {
      console.error(
        `[${videoId}] Failed to fetch transcript using all available methods. Last error: ${lastError.message}`
      );
      throw lastError;
    }
  } else {
    console.error(
      `[${videoId}] Failed to fetch transcript using all available methods. Last error: Unknown`
    );
    throw new Error(
      "Failed to fetch transcript using all available methods. The video may not have captions available."
    );
  }
}

// Simple wrapper for the third-party library
async function fetchTranscriptLibrary(
  videoId: string
): Promise<TranscriptLine[]> {
  // This would normally use the third-party library, but since we have our own implementation,
  // we can throw an error that will cause the sequence to try the next method
  throw new Error("Library method not implemented");
}

export {
  fetchTranscriptDirect,
  fetchTranscriptInnertube,
  extractAndParseTranscriptFromHtml,
};
</file>

<file path="app/api/summarize/route.ts">
import { NextRequest, NextResponse } from "next/server";
// Replace the youtube-transcript npm package with our custom implementation
// import { YoutubeTranscript, TranscriptConfig } from "youtube-transcript";
import { fetchTranscript, TranscriptLine } from "@/lib/youtube-transcript";
import { openai } from "@/lib/openai";
import { fetchMetadataFromYouTubeAPI } from "@/lib/youtube";
import { PodcastMetadata } from "@/components/PodcastMetadata";
import { extractVideoId } from "@/lib/utils";

// OpenAI API key check and client instantiation are handled by the imported '@/lib/openai'.

const MAX_TOKENS_TRANSCRIPT = 100000; // Max tokens for transcript (approx 25k words)

// Define a simple type for oEmbed response (can be shared or redefined)
interface OEmbedResponse {
  title?: string;
  author_name?: string;
  thumbnail_url?: string;
}

// Simple oEmbed fetch function (can be shared or redefined) - adjusted for this context
async function fetchOEmbedMetadataForSummarize(
  videoId: string
): Promise<Partial<PodcastMetadata> | null> {
  const url = `https://www.youtube.com/oembed?url=https://www.youtube.com/watch?v=${videoId}&format=json`;
  try {
    console.log(
      `[${videoId}] (Summarize) Attempting oEmbed fallback for metadata...`
    );
    const response = await fetch(url, {
      next: { revalidate: 3600 },
      signal: AbortSignal.timeout(8000),
    }); // Cache & 8s timeout
    if (!response.ok) {
      console.error(
        `[${videoId}] (Summarize) oEmbed request failed with status ${response.status} ${response.statusText}`
      );
      return null;
    }
    const data: OEmbedResponse = await response.json();
    const metadata: Partial<PodcastMetadata> = {
      videoId: videoId,
      title: data.title || "YouTube Video (oEmbed)",
      channelName: data.author_name || "Unknown Channel (oEmbed)",
      thumbnails: data.thumbnail_url
        ? { default: { url: data.thumbnail_url, width: 0, height: 0 } }
        : null,
      duration: "0:00", // oEmbed doesn't provide duration
    };
    console.log(
      `[${videoId}] (Summarize) Successfully fetched partial metadata via oEmbed fallback.`
    );
    return metadata;
  } catch (error: any) {
    if (error.name === "AbortError") {
      console.error(`[${videoId}] (Summarize) oEmbed request timed out.`);
    } else {
      console.error(
        `[${videoId}] (Summarize) Error fetching oEmbed metadata:`,
        error.message || error
      );
    }
    return null;
  }
}

// Custom function to fetch YouTube transcript directly
async function fetchYouTubeTranscriptDirectly(videoId: string) {
  console.log(`[${videoId}] Attempting direct transcript fetch...`);
  try {
    // First, fetch the video page to extract necessary tokens
    const videoPageResponse = await fetch(
      `https://www.youtube.com/watch?v=${videoId}`,
      {
        headers: {
          "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
          "Accept-Language": "en-US,en;q=0.9",
          Accept:
            "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        },
        signal: AbortSignal.timeout(15000), // 15 second timeout
      }
    );

    if (!videoPageResponse.ok) {
      throw new Error(
        `Failed to fetch video page: ${videoPageResponse.status} ${videoPageResponse.statusText}`
      );
    }

    const contentType = videoPageResponse.headers.get("content-type");
    if (!contentType || !contentType.includes("text/html")) {
      // Sometimes YouTube might return JSON even on direct page request, handle this
      if (contentType.includes("application/json")) {
        console.warn(
          `[${videoId}] Direct fetch received JSON instead of HTML. Content might be blocked or page structure changed.`
        );
        // Optionally try to parse JSON for errors if relevant
      } else {
        throw new Error(`Invalid content type returned: ${contentType}`);
      }
    }

    const videoPageContent = await videoPageResponse.text();

    if (!videoPageContent || videoPageContent.length < 500) {
      // Increased threshold slightly
      throw new Error(
        `Empty or too short response from YouTube (Length: ${videoPageContent?.length})`
      );
    }

    // --- Start Regex Extraction ---
    let rawCaptionsData = null;
    const patterns = [
      /"captions":\s*(\{.*?"captionTracks":.*?\}),\s*"videoDetails"/, // Main pattern
      /"captionTracks":\s*(\[.*?\])/, // Simpler track array
      /"baseUrl":"(https:\/\/www\.youtube\.com\/api\/timedtext.*?)"/, // Corrected: Reduced escaping for slashes
    ];

    for (const pattern of patterns) {
      const match = videoPageContent.match(pattern);
      if (match && match[1]) {
        const matchedData = match[1]
          .replace(/\\"/g, '"')
          .replace(/\\\\/g, "\\"); // Basic cleaning
        console.log(
          `[${videoId}] Found potential captions data using pattern: ${pattern.source.substring(
            0,
            30
          )}...`
        );

        if (pattern.source.includes("baseUrl")) {
          // Handle direct URL pattern
          // Decode URL-encoded characters
          const decodedUrl = decodeURIComponent(JSON.parse(`"${matchedData}"`)); // Safer decoding
          rawCaptionsData = JSON.stringify({
            playerCaptionsTracklistRenderer: {
              captionTracks: [{ baseUrl: decodedUrl }],
            },
          });
          console.log(`[${videoId}] Extracted direct transcript URL.`);
        } else {
          // Attempt to parse as JSON for other patterns
          try {
            // More robust cleaning might be needed depending on YouTube's output
            const parsed = JSON.parse(matchedData);
            // Ensure structure is somewhat valid before accepting
            if (
              parsed &&
              (parsed.captionTracks ||
                (parsed.playerCaptionsTracklistRenderer &&
                  parsed.playerCaptionsTracklistRenderer.captionTracks))
            ) {
              // Re-stringify into a consistent format if needed, or use directly if structure matches
              if (
                parsed.captionTracks &&
                !parsed.playerCaptionsTracklistRenderer
              ) {
                rawCaptionsData = JSON.stringify({
                  playerCaptionsTracklistRenderer: {
                    captionTracks: parsed.captionTracks,
                  },
                });
              } else {
                rawCaptionsData = JSON.stringify(parsed); // Assume structure is okay
              }
              console.log(`[${videoId}] Successfully parsed extracted JSON.`);
            } else {
              console.warn(
                `[${videoId}] Parsed JSON has unexpected structure.`
              );
            }
          } catch (parseError) {
            console.warn(
              `[${videoId}] Failed to parse extracted data for pattern ${pattern.source.substring(
                0,
                30
              )}...: ${parseError}`
            );
            // Continue to next pattern
          }
        }
        if (rawCaptionsData) break; // Stop if valid data found
      }
    }
    // --- End Regex Extraction ---

    if (!rawCaptionsData) {
      console.error(
        `[${videoId}] No captions data found in video page using any pattern.`
      );
      // Optional: Log a snippet of the page for debugging (be careful with size/PII)
      // console.log(`[${videoId}] Page Snippet: ${videoPageContent.substring(0, 500)}`);
      throw new Error("No captions data found in video page");
    }

    let captionsData;
    try {
      captionsData = JSON.parse(rawCaptionsData); // Already cleaned during extraction
    } catch (e: any) {
      console.error(
        `[${videoId}] Failed to parse final captions JSON: ${
          e.message
        }. Raw Data: ${rawCaptionsData.substring(0, 200)}...`
      );
      throw new Error(`Failed to parse final captions data: ${e.message}`);
    }

    if (
      !captionsData?.playerCaptionsTracklistRenderer?.captionTracks ||
      captionsData.playerCaptionsTracklistRenderer.captionTracks.length === 0
    ) {
      console.warn(
        `[${videoId}] Parsed captions data lacks track information or is empty.`
      );
      throw new Error(
        "Transcript tracks unavailable or disabled in parsed data"
      );
    }

    // Find the first usable transcript URL (prefer 'en' if available, otherwise take first)
    let transcriptUrl = "";
    const tracks = captionsData.playerCaptionsTracklistRenderer.captionTracks;
    const englishTrack = tracks.find(
      (track: any) => track.languageCode === "en"
    );
    transcriptUrl = englishTrack?.baseUrl || tracks[0]?.baseUrl;

    if (!transcriptUrl) {
      console.error(
        `[${videoId}] Could not find a valid baseUrl in caption tracks.`
      );
      throw new Error("No valid transcript URL found in caption tracks");
    }
    console.log(
      `[${videoId}] Using transcript URL: ${transcriptUrl.substring(0, 60)}...`
    );

    // Fetch the transcript XML/TTML
    const transcriptResponse = await fetch(transcriptUrl, {
      headers: {
        "User-Agent":
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9", // Important for getting english transcript if auto-selected
        Accept: "*/*", // Accept any content type
      },
      signal: AbortSignal.timeout(10000), // 10 second timeout
    });

    if (!transcriptResponse.ok) {
      const errorText = await transcriptResponse.text();
      console.error(
        `[${videoId}] Failed to fetch transcript content: ${
          transcriptResponse.status
        } ${transcriptResponse.statusText}. Response: ${errorText.substring(
          0,
          200
        )}`
      );
      throw new Error(
        `Failed to fetch transcript data: ${transcriptResponse.status} ${transcriptResponse.statusText}`
      );
    }

    const transcriptContent = await transcriptResponse.text();

    // Basic validation of transcript content
    if (!transcriptContent || transcriptContent.length < 50) {
      console.error(
        `[${videoId}] Invalid or empty transcript content received.`
      );
      throw new Error("Invalid or empty transcript data received");
    }
    // More robust check: look for common transcript tags
    if (
      !transcriptContent.includes("<text") &&
      !transcriptContent.includes("<p")
    ) {
      console.warn(
        `[${videoId}] Transcript content might be invalid (missing common tags): ${transcriptContent.substring(
          0,
          200
        )}...`
      );
      // Decide whether to throw error or proceed cautiously
      // throw new Error("Transcript content appears invalid (missing tags)");
    }

    // Parse the XML/TTML to extract transcript segments
    const transcript = [];
    // Handle standard XML format <text start="..." dur="...">...</text>
    const xmlRegex = /<text start="([^"]*)" dur="([^"]*)">([^<]*)<\/text>/g;
    // Handle alternative format (often in TTML) <p begin="..." end="..." ...>...</p>
    const ttmlRegex = /<p begin="([^"]*)" end="([^"]*)"[^>]*>([^<]*)<\/p>/g;

    let match;
    // Try XML first
    while ((match = xmlRegex.exec(transcriptContent)) !== null) {
      const offset = parseFloat(match[1]);
      const duration = parseFloat(match[2]);
      // Basic sanity check for parsed numbers
      if (!isNaN(offset) && !isNaN(duration)) {
        transcript.push({
          text: match[3]
            .replace(/&amp;/g, "&")
            .replace(/&lt;/g, "<")
            .replace(/&gt;/g, ">")
            .replace(/&#39;/g, "'")
            .replace(/&quot;/g, '"')
            .replace(/\\n/g, " ")
            .trim(),
          duration: duration,
          offset: offset,
        });
      } else {
        console.warn(
          `[${videoId}] Skipping segment due to invalid number format (XML): start='${match[1]}', dur='${match[2]}'`
        );
      }
    }

    // If XML parsing yielded nothing, try TTML format
    if (transcript.length === 0) {
      console.log(
        `[${videoId}] XML pattern found no segments, trying TTML pattern...`
      );
      while ((match = ttmlRegex.exec(transcriptContent)) !== null) {
        const offset = parseTimestamp(match[1]); // Need helper to parse HH:MM:SS.ms
        const end = parseTimestamp(match[2]);
        const duration = end - offset;
        // Basic sanity check for parsed numbers
        if (!isNaN(offset) && !isNaN(duration) && duration >= 0) {
          transcript.push({
            text: match[3]
              .replace(/&amp;/g, "&")
              .replace(/&lt;/g, "<")
              .replace(/&gt;/g, ">")
              .replace(/&#39;/g, "'")
              .replace(/&quot;/g, '"')
              .replace(/\\n/g, " ")
              .trim(),
            duration: duration,
            offset: offset,
          });
        } else {
          console.warn(
            `[${videoId}] Skipping segment due to invalid number format (TTML): begin='${match[1]}', end='${match[2]}'`
          );
        }
      }
    }

    if (transcript.length === 0) {
      console.error(
        `[${videoId}] Failed to parse transcript content using both XML and TTML patterns. Content snippet: ${transcriptContent.substring(
          0,
          500
        )}...`
      );
      throw new Error("Failed to parse transcript XML/TTML content");
    }

    console.log(
      `[${videoId}] Successfully fetched and parsed transcript directly: ${transcript.length} segments`
    );
    return transcript;
  } catch (error: any) {
    // Enhanced logging
    console.error(
      `[${videoId}] Direct transcript fetch failed: ${error.message || error}`
    );
    // Add more specific logging based on error message content
    if (error.message?.includes("fetch video page")) {
      console.error(
        `[${videoId}] Specific failure: Fetching main video page. Status: ${
          error.message.split(": ")[1]
        }`
      );
    } else if (error.message?.includes("No captions data found")) {
      console.error(
        `[${videoId}] Specific failure: Could not find captions JSON/patterns in HTML.`
      );
    } else if (error.message?.includes("parse final captions data")) {
      console.error(
        `[${videoId}] Specific failure: Parsing extracted captions JSON.`
      );
    } else if (error.message?.includes("fetch transcript data")) {
      console.error(
        `[${videoId}] Specific failure: Fetching the transcript content file. Status: ${
          error.message.split(": ")[1]
        }`
      );
    } else if (error.message?.includes("parse transcript XML/TTML")) {
      console.error(
        `[${videoId}] Specific failure: Parsing the final transcript content.`
      );
    } else if (error.name === "AbortError") {
      console.error(
        `[${videoId}] Direct transcript fetch sub-request timed out.`
      );
    } else if (error.message?.includes("Invalid content type")) {
      console.error(
        `[${videoId}] Specific failure: Received unexpected content type from YouTube.`
      );
    } else if (error.message?.includes("Empty or too short response")) {
      console.error(
        `[${videoId}] Specific failure: Received unusable short response from YouTube.`
      );
    }
    // Re-throw the original error to be caught by the main handler's loop
    throw error;
  }
}

// Helper to parse HH:MM:SS.ms timestamps from TTML
function parseTimestamp(timestamp: string): number {
  const parts = timestamp.split(":");
  let seconds = 0;
  if (parts.length === 3) {
    seconds += parseFloat(parts[0]) * 3600;
    seconds += parseFloat(parts[1]) * 60;
    seconds += parseFloat(parts[2]);
  } else if (parts.length === 2) {
    seconds += parseFloat(parts[0]) * 60;
    seconds += parseFloat(parts[1]);
  } else if (parts.length === 1) {
    seconds += parseFloat(parts[0]);
  }
  return isNaN(seconds) ? 0 : seconds; // Return 0 if parsing fails
}

// Helper function to fetch podcast metadata (Refactored for Summarize route)
const fetchPodcastMetadataForSummarize = async (
  videoId: string
): Promise<Partial<PodcastMetadata> | null> => {
  let metadata: Partial<PodcastMetadata> | null = null;

  // 1. Try fetching using the YouTube Data API (shared function)
  try {
    console.log(
      `[${videoId}] (Summarize) Attempting metadata fetch via YouTube API...`
    );
    metadata = await fetchMetadataFromYouTubeAPI(videoId);
    if (metadata) {
      console.log(
        `[${videoId}] (Summarize) Metadata successfully fetched via YouTube API.`
      );
      return metadata;
    } else {
      console.warn(
        `[${videoId}] (Summarize) YouTube API metadata fetch returned null, proceeding to fallback.`
      );
    }
  } catch (apiError: any) {
    console.error(
      `[${videoId}] (Summarize) Error during YouTube API metadata fetch: ${
        apiError.message || apiError
      }. Proceeding to fallback.`
    );
    if (apiError.name === "AbortError") {
      console.error(`[${videoId}] (Summarize) YouTube API request timed out.`);
    }
  }

  // 2. If API fails or returns null, try oEmbed fallback specific to this route
  if (!metadata) {
    try {
      console.log(
        `[${videoId}] (Summarize) Attempting metadata fetch via oEmbed fallback...`
      );
      metadata = await fetchOEmbedMetadataForSummarize(videoId); // Use the function defined in this file
      if (metadata) {
        console.log(
          `[${videoId}] (Summarize) Metadata successfully fetched via oEmbed.`
        );
        return metadata;
      } else {
        console.warn(
          `[${videoId}] (Summarize) oEmbed metadata fetch also returned null.`
        );
      }
    } catch (oembedError: any) {
      console.error(
        `[${videoId}] (Summarize) Error during oEmbed metadata fetch: ${
          oembedError.message || oembedError
        }`
      );
    }
  }

  // 3. If both methods fail, return null
  if (!metadata) {
    console.error(
      `[${videoId}] (Summarize) All methods failed to fetch metadata.`
    );
    return null;
  }

  return metadata;
};

// Function to fetch transcript using YouTube API (Enhanced Logging)
// Note: This often requires OAuth for non-public captions. API key might only work for public ones.
async function fetchYouTubeTranscriptViaAPI(videoId: string) {
  const apiKey = process.env.YOUTUBE_API_KEY;
  if (!apiKey) {
    console.warn(
      `[${videoId}] YouTube API key missing, cannot attempt transcript via API.`
    );
    return null;
  }
  console.log(`[${videoId}] Attempting transcript fetch via YouTube API...`);
  try {
    // Fetch caption list
    const listUrl = `https://www.googleapis.com/youtube/v3/captions?part=snippet&videoId=${videoId}&key=${apiKey}`;
    const listResponse = await fetch(listUrl, {
      signal: AbortSignal.timeout(10000),
    });

    if (!listResponse.ok) {
      const errorText = await listResponse.text();
      console.error(
        `[${videoId}] YouTube API caption list request failed: Status ${
          listResponse.status
        }. Response: ${errorText.substring(0, 200)}`
      );
      if (listResponse.status === 403) {
        // Forbidden often means captions disabled by owner or requires OAuth
        console.warn(
          `[${videoId}] API Error 403: Captions likely disabled or require owner permission (OAuth).`
        );
        throw new Error(`Captions disabled or private (API 403)`);
      } else if (listResponse.status === 404) {
        console.warn(`[${videoId}] API Error 404: Video/Captions not found.`);
        throw new Error(`Video or captions not found (API 404)`);
      } else {
        throw new Error(
          `Failed to list captions via API: ${listResponse.statusText}`
        );
      }
    }

    const listData = await listResponse.json();
    if (!listData.items || listData.items.length === 0) {
      console.warn(`[${videoId}] YouTube API returned no caption tracks.`);
      throw new Error("No caption tracks found via API");
    }

    // Prefer English, fallback to first available
    const englishTrack = listData.items.find(
      (item: any) => item.snippet?.language === "en"
    );
    const trackToDownload = englishTrack || listData.items[0];
    const captionId = trackToDownload.id;

    console.log(
      `[${videoId}] Found API caption track ID: ${captionId}, Language: ${trackToDownload.snippet?.language}`
    );

    // Download the caption track (try common formats, srt might work with API key)
    // Note: download often requires OAuth, but try standard formats. ttml is common.
    let downloadedTranscript = null;
    const formatsToTry = ["srt", "vtt", "ttml"]; // Common formats
    for (const format of formatsToTry) {
      const downloadUrl = `https://www.googleapis.com/youtube/v3/captions/${captionId}?key=${apiKey}&tfmt=${format}`;
      try {
        console.log(`[${videoId}] Attempting API download format: ${format}`);
        const downloadResponse = await fetch(downloadUrl, {
          signal: AbortSignal.timeout(10000),
        });
        if (downloadResponse.ok) {
          downloadedTranscript = await downloadResponse.text();
          console.log(
            `[${videoId}] Successfully downloaded API transcript format: ${format}`
          );
          break; // Success
        } else {
          const errorText = await downloadResponse.text();
          console.warn(
            `[${videoId}] API download failed for format ${format}: Status ${
              downloadResponse.status
            }. Response: ${errorText.substring(0, 200)}`
          );
          // Continue to next format
        }
      } catch (downloadError: any) {
        console.warn(
          `[${videoId}] Error during API download attempt for format ${format}: ${downloadError.message}`
        );
        // Continue to next format
      }
    }

    if (!downloadedTranscript) {
      console.error(
        `[${videoId}] Failed to download API transcript using formats: ${formatsToTry.join(
          ", "
        )}`
      );
      throw new Error(`Failed to download caption track using any format`);
    }

    // Simple parsing (assuming SRT or VTT-like format - needs improvement for robustness)
    // This is a placeholder - a proper SRT/VTT parser is recommended
    const lines = downloadedTranscript.split("\\n");
    const transcript = lines
      .map((line) => line.trim())
      .filter((line) => line && !line.match(/^\d+$/) && !line.includes("-->")) // Basic filter
      .map((text) => ({ text, duration: 0, offset: 0 })); // Dummy duration/offset

    if (transcript.length === 0) {
      console.error(
        `[${videoId}] Failed to parse downloaded API transcript content.`
      );
      throw new Error("Failed to parse downloaded API transcript");
    }

    console.log(
      `[${videoId}] Successfully fetched and parsed transcript via YouTube API: ${transcript.length} lines (basic parse).`
    );
    return transcript; // Return the parsed transcript
  } catch (error: any) {
    console.error(
      `[${videoId}] YouTube API transcript fetch failed: ${
        error.message || error
      }`
    );
    if (error.name === "AbortError") {
      console.error(
        `[${videoId}] YouTube API transcript fetch sub-request timed out.`
      );
    }
    // Specific logging based on error message
    if (
      error.message?.includes("Captions disabled") ||
      error.message?.includes("API 403")
    ) {
      console.warn(
        `[${videoId}] Transcript fetch via API blocked (likely disabled/private).`
      );
    } else if (error.message?.includes("No caption tracks")) {
      console.warn(`[${videoId}] No caption tracks listed by API.`);
    } else if (error.message?.includes("Failed to download")) {
      console.error(
        `[${videoId}] Critical failure during API transcript download phase.`
      );
    }
    return null; // Indicate failure to the caller to try next method
  }
}

// Function to fetch transcript using Innertube API (Enhanced Logging)
async function fetchYouTubeTranscriptViaInnertubeAPI(videoId: string) {
  console.log(`[${videoId}] Attempting transcript fetch via Innertube API...`);
  let pageContent = ""; // Store page content for debugging
  try {
    // 1. Fetch the video page
    const pageResponse = await fetch(
      `https://www.youtube.com/watch?v=${videoId}`,
      {
        headers: {
          "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
          "Accept-Language": "en-US,en;q=0.9",
        },
        signal: AbortSignal.timeout(15000),
      }
    );
    if (!pageResponse.ok) {
      throw new Error(
        `Failed to fetch video page: ${pageResponse.status} ${pageResponse.statusText}`
      );
    }
    pageContent = await pageResponse.text();
    if (!pageContent || pageContent.length < 1000) {
      throw new Error("Empty or too short response from YouTube page fetch");
    }

    // 2. Extract necessary data (API Key, Client Version, Context)
    const apiKeyMatch = pageContent.match(/"innertubeApiKey":"([^"]+)"/);
    const clientVersionMatch = pageContent.match(/"clientVersion":"([^"]+)"/);
    // Context extraction is complex, find ytInitialPlayerResponse or similar
    const playerResponseMatch = pageContent.match(
      /ytInitialPlayerResponse\s*=\s*(\{.*?\});/
    );

    if (!apiKeyMatch || !apiKeyMatch[1])
      throw new Error("Could not extract Innertube API key");
    if (!clientVersionMatch || !clientVersionMatch[1])
      throw new Error("Could not extract client version");
    if (!playerResponseMatch || !playerResponseMatch[1])
      throw new Error(
        "Could not extract player context (ytInitialPlayerResponse)"
      );

    const apiKey = apiKeyMatch[1];
    const clientVersion = clientVersionMatch[1];
    let playerResponse;
    try {
      playerResponse = JSON.parse(playerResponseMatch[1]);
    } catch (e: any) {
      console.error(
        `[${videoId}] Failed to parse ytInitialPlayerResponse JSON: ${e.message}`
      );
      throw new Error("Failed to parse player context JSON");
    }

    // Find captions URL within the player response
    const captionTracks =
      playerResponse?.captions?.playerCaptionsTracklistRenderer?.captionTracks;
    if (!captionTracks || captionTracks.length === 0) {
      throw new Error("No caption tracks found in Innertube player response");
    }

    // Prefer English track, fallback to the first one
    const englishTrack = captionTracks.find(
      (track: any) => track.languageCode === "en"
    );
    const targetTrack = englishTrack || captionTracks[0];
    const captionsUrl = targetTrack?.baseUrl;

    if (!captionsUrl) {
      throw new Error(
        "Could not find captions baseUrl in Innertube player response"
      );
    }

    console.log(
      `[${videoId}] Found Innertube captions URL for lang ${
        targetTrack.languageCode
      }: ${captionsUrl.substring(0, 60)}...`
    );

    // 3. Fetch the actual transcript from the Innertube captions URL
    const transcriptResponse = await fetch(captionsUrl, {
      headers: {
        "User-Agent":
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
      },
      signal: AbortSignal.timeout(10000),
    });

    if (!transcriptResponse.ok) {
      const errorText = await transcriptResponse.text();
      console.error(
        `[${videoId}] Innertube captions URL fetch failed: Status ${
          transcriptResponse.status
        }. Response: ${errorText.substring(0, 200)}`
      );
      throw new Error(
        `Innertube captions URL fetch failed: ${transcriptResponse.statusText}`
      );
    }

    const transcriptContent = await transcriptResponse.text();
    if (!transcriptContent || transcriptContent.length < 50) {
      throw new Error("Empty or invalid transcript content from Innertube URL");
    }

    // 4. Parse the transcript (reuse direct fetch parsing logic)
    const transcript = [];
    const xmlRegex = /<text start="([^"]*)" dur="([^"]*)">([^<]*)<\/text>/g;
    const ttmlRegex = /<p begin="([^"]*)" end="([^"]*)"[^>]*>([^<]*)<\/p>/g;
    let match;

    while ((match = xmlRegex.exec(transcriptContent)) !== null) {
      const offset = parseFloat(match[1]);
      const duration = parseFloat(match[2]);
      if (!isNaN(offset) && !isNaN(duration)) {
        transcript.push({
          text: match[3]
            .replace(/&amp;/g, "&")
            .replace(/&lt;/g, "<")
            .replace(/&gt;/g, ">")
            .replace(/&#39;/g, "'")
            .replace(/&quot;/g, '"')
            .replace(/\\n/g, " ")
            .trim(),
          duration: duration,
          offset: offset,
        });
      }
    }

    if (transcript.length === 0) {
      console.log(
        `[${videoId}] Innertube XML pattern found no segments, trying TTML pattern...`
      );
      while ((match = ttmlRegex.exec(transcriptContent)) !== null) {
        const offset = parseTimestamp(match[1]);
        const end = parseTimestamp(match[2]);
        const duration = end - offset;
        if (!isNaN(offset) && !isNaN(duration) && duration >= 0) {
          transcript.push({
            text: match[3]
              .replace(/&amp;/g, "&")
              .replace(/&lt;/g, "<")
              .replace(/&gt;/g, ">")
              .replace(/&#39;/g, "'")
              .replace(/&quot;/g, '"')
              .replace(/\\n/g, " ")
              .trim(),
            duration: duration,
            offset: offset,
          });
        }
      }
    }

    if (transcript.length === 0) {
      console.error(
        `[${videoId}] Failed to parse Innertube transcript content using both XML and TTML patterns. Content snippet: ${transcriptContent.substring(
          0,
          500
        )}...`
      );
      throw new Error("Failed to parse Innertube transcript content");
    }

    console.log(
      `[${videoId}] Successfully fetched and parsed transcript via Innertube API: ${transcript.length} segments`
    );
    return transcript;
  } catch (error: any) {
    console.error(
      `[${videoId}] Innertube API transcript fetch failed: ${
        error.message || error
      }`
    );
    // Add specific logging based on error messages
    if (error.message?.includes("fetch video page")) {
      console.error(
        `[${videoId}] Innertube Failure: Fetching initial page. Status: ${
          error.message.split(": ")[1]
        }`
      );
    } else if (error.message?.includes("extract Innertube API key")) {
      console.error(
        `[${videoId}] Innertube Failure: Extracting API key. Check page structure.`
      );
      // console.log(`[${videoId}] Innertube Page Snippet: ${pageContent?.substring(0, 500)}`); // Debugging
    } else if (error.message?.includes("extract client version")) {
      console.error(
        `[${videoId}] Innertube Failure: Extracting client version. Check page structure.`
      );
    } else if (error.message?.includes("extract player context")) {
      console.error(
        `[${videoId}] Innertube Failure: Extracting ytInitialPlayerResponse. Check page structure.`
      );
    } else if (error.message?.includes("parse player context")) {
      console.error(
        `[${videoId}] Innertube Failure: Parsing ytInitialPlayerResponse JSON.`
      );
    } else if (error.message?.includes("No caption tracks found")) {
      console.warn(
        `[${videoId}] Innertube Info: No caption tracks listed in player response.`
      );
    } else if (error.message?.includes("Could not find captions baseUrl")) {
      console.error(
        `[${videoId}] Innertube Failure: Found tracks but no baseUrl.`
      );
    } else if (error.message?.includes("Innertube captions URL fetch failed")) {
      console.error(
        `[${videoId}] Innertube Failure: Fetching final transcript content. Status: ${
          error.message.split(": ")[1]
        }`
      );
    } else if (
      error.message?.includes("Failed to parse Innertube transcript content")
    ) {
      console.error(
        `[${videoId}] Innertube Failure: Parsing final transcript content.`
      );
    } else if (error.name === "AbortError") {
      console.error(
        `[${videoId}] Innertube API transcript fetch sub-request timed out.`
      );
    } else if (error.message?.includes("Empty or too short response")) {
      console.error(
        `[${videoId}] Innertube Failure: Empty/short response fetching initial page.`
      );
    } else if (error.message?.includes("Empty or invalid transcript content")) {
      console.error(
        `[${videoId}] Innertube Failure: Empty/short response fetching final transcript.`
      );
    }
    return null; // Indicate failure to the caller to try next method
  }
}

/**
 * Handles POST requests to summarize a YouTube video
 */
export async function POST(request: Request) {
  try {
    // Extract the videoId from the request body
    let reqData;
    try {
      reqData = await request.json();
    } catch (error) {
      return NextResponse.json(
        { error: "Invalid request body" },
        { status: 400 }
      );
    }

    // Support both direct videoId and url parameter
    let videoId = reqData.videoId;

    // If videoId is not provided but url is, try to extract videoId from url
    if (!videoId && reqData.url) {
      try {
        videoId = extractVideoId(reqData.url);
        console.log(`Extracted videoId ${videoId} from URL ${reqData.url}`);
      } catch (error) {
        console.error(
          `Failed to extract videoId from URL: ${reqData.url}`,
          error
        );
        return NextResponse.json(
          { error: "Could not extract videoId from provided URL" },
          { status: 400 }
        );
      }
    }

    if (!videoId) {
      console.error("Missing videoId parameter and no URL provided");
      return NextResponse.json(
        { error: "Missing videoId parameter" },
        { status: 400 }
      );
    }

    console.log(`Summarizing video with ID: ${videoId}`);

    // Fetch video metadata and transcript in parallel
    const [metadata, transcript] = await Promise.all([
      fetchMetadataFromYouTubeAPI(videoId),
      fetchTranscript(videoId),
    ]);

    // Check if we could get the metadata
    if (!metadata) {
      console.error(`Failed to fetch metadata for video ID: ${videoId}`);
      return NextResponse.json(
        { error: "Failed to fetch video metadata" },
        { status: 500 }
      );
    }

    // Check if we could get the transcript
    if (!transcript || transcript.length === 0) {
      console.error(`Failed to fetch transcript for video ID: ${videoId}`);
      return NextResponse.json(
        { error: "Failed to fetch video transcript" },
        { status: 500 }
      );
    }

    // Combine transcript text
    const fullTranscript = transcript.map((line) => line.text).join(" ");

    // Break into chunks of 12000 characters for OpenAI limit
    const chunkSize = 12000;
    const chunks = [];
    for (let i = 0; i < fullTranscript.length; i += chunkSize) {
      chunks.push(fullTranscript.slice(i, i + chunkSize));
    }

    // Process each chunk with OpenAI
    const summaryPromises = chunks.map(async (chunk, i) => {
      const prompt = `
        You're summarizing part ${i + 1} of ${
        chunks.length
      } of a YouTube video transcript.
        
        Title: ${metadata.title}
        Creator: ${metadata.channelName}
        
        Instructions:
        1. Identify key points, ideas, and information.
        2. Focus on the most valuable insights, skipping repetitive content.
        3. Maintain the original meaning and tone.
        4. Create a coherent, well-structured summary.
        5. If this is part of a multi-part summary, focus on just this section.
        
        Transcript part ${i + 1}/${chunks.length}:
        ${chunk}
        
        Summary of this part:`;

      const response = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content:
              "You are an expert at summarizing content. Create clear, concise summaries that retain the most valuable information.",
          },
          {
            role: "user",
            content: prompt,
          },
        ],
        temperature: 0.5,
        max_tokens: 500,
      });

      return response.choices[0]?.message?.content || "";
    });

    // Wait for all chunks to be processed
    const chunkSummaries = await Promise.all(summaryPromises);

    // If we have multiple chunks, create a final combined summary
    let finalSummary = "";
    if (chunks.length > 1) {
      const combinedSummary = chunkSummaries.join("\n\n");
      const finalPrompt = `
        You're creating a final summary of a YouTube video based on summaries of multiple chunks of its transcript.
        
        Title: ${metadata.title}
        Creator: ${metadata.channelName}
        
        Instructions:
        1. Create a coherent, well-organized final summary from the separate chunk summaries.
        2. Eliminate repetition across the chunks.
        3. Identify the most important points from the entire video.
        4. Structure the summary logically, possibly with sections if appropriate.
        5. Keep the summary concise yet comprehensive.
        
        Individual chunk summaries:
        ${combinedSummary}
        
        Final complete summary:`;

      const finalResponse = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content:
              "You are an expert at synthesizing summaries into a cohesive whole, maintaining the most important information while eliminating redundancy.",
          },
          {
            role: "user",
            content: finalPrompt,
          },
        ],
        temperature: 0.5,
        max_tokens: 1000,
      });

      finalSummary = finalResponse.choices[0]?.message?.content || "";
    } else {
      // If only one chunk, use that summary
      finalSummary = chunkSummaries[0];
    }

    // Return the summary and metadata
    return NextResponse.json({
      summary: finalSummary,
      metadata: {
        title: metadata.title,
        channelName: metadata.channelName,
        thumbnails: metadata.thumbnails,
        duration: metadata.duration,
        viewCount: metadata.viewCount,
        videoId: videoId,
      },
    });
  } catch (error: any) {
    console.error("Error in summarize API:", error);
    return NextResponse.json(
      { error: error.message || "Failed to summarize video" },
      { status: 500 }
    );
  }
}

// Removed the old fetchPodcastMetadata internal fetch logic
// Added enhanced logging to catch blocks of transcript fetchers
// Updated main POST handler to try multiple transcript methods sequentially
// Updated OpenAI model and prompt structure
// Improved error handling and user messaging when transcript fetching fails completely
// Refactored metadata fetching to use shared functions
// Added TTML parsing fallback to direct fetch
// Added helper for timestamp parsing
</file>

</files>
