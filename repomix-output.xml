This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
app/
  api/
    chat/
      route.ts
    podcast-metadata/
      route.ts
    summarize/
      route.ts
  auth/
    auth-code-error/
      page.tsx
    callback/
      route.ts
  dashboard/
    page.tsx
  validate-metadata/
    page.tsx
  validate-transcript/
    page.tsx
  globals.css
  layout.tsx
  page.tsx
  page.tsx.backup
components/
  Chat.tsx
  ErrorBoundary.tsx
  Navbar.tsx
  PodcastHeader.tsx
  PodcastMetadata.tsx
  Summary.tsx
lib/
  supabase/
    client.ts
    server.ts
  supabaseClient.ts
  youtube-transcript.ts
  youtube.ts
public/
  create-screenshot.html
  README.md
.gitignore
DEPLOYMENT_VERCEL.md
middleware.ts
package.json
postcss.config.js
README.md
supabaseplan.md
tailwind.config.js
TRANSCRIPT_ISSUE_PLAN.md
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app/auth/auth-code-error/page.tsx">
import React from "react";

export default function AuthCodeError() {
  return (
    <div
      style={{ padding: "20px", fontFamily: "sans-serif", textAlign: "center" }}
    >
      <h1>Authentication Error</h1>
      <p>
        Sorry, we couldn't sign you in. There was an issue during the
        authentication process.
      </p>
      <p>Please try signing in again.</p>
      <a href="/">Go back to Home</a>
    </div>
  );
}
</file>

<file path="app/auth/callback/route.ts">
import { createServerClient, type CookieOptions } from "@supabase/ssr";
import { cookies } from "next/headers";
import { NextResponse, type NextRequest } from "next/server";

export async function GET(request: NextRequest) {
  const { searchParams, origin } = new URL(request.url);
  const code = searchParams.get("code");
  // if "next" is in param, use it as the redirect URL
  const next = searchParams.get("next") ?? "/dashboard"; // Default redirect to dashboard

  if (code) {
    const cookieStore = cookies();
    const supabase = createServerClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
      {
        cookies: {
          get(name: string) {
            return cookieStore.get(name)?.value;
          },
          set(name: string, value: string, options: CookieOptions) {
            cookieStore.set({ name, value, ...options });
          },
          remove(name: string, options: CookieOptions) {
            cookieStore.delete({ name, ...options });
          },
        },
      }
    );
    const { error } = await supabase.auth.exchangeCodeForSession(code);
    if (!error) {
      return NextResponse.redirect(`${origin}${next}`);
    }
  }

  // return the user to an error page with instructions
  console.error("Error exchanging code for session or code not found");
  return NextResponse.redirect(`${origin}/auth/auth-code-error`);
}
</file>

<file path="app/dashboard/page.tsx">
"use client";

import React, { useState, useEffect } from "react";
import { ArrowPathIcon } from "@heroicons/react/24/solid";
import toast, { Toaster } from "react-hot-toast";
import Summary from "../../components/Summary";
import Chat from "../../components/Chat";
import Navbar from "../../components/Navbar";
import PodcastHeader from "../../components/PodcastHeader";
import { PodcastMetadataProvider } from "../../components/PodcastMetadata";

export default function Dashboard() {
  const [url, setUrl] = useState("");
  const [loading, setLoading] = useState(false);
  const [validating, setValidating] = useState(false);
  const [summary, setSummary] = useState<string | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [activeTab, setActiveTab] = useState<"summary" | "chat">("summary");
  const [isUrlValidated, setIsUrlValidated] = useState(false);
  const [isSummaryLoading, setIsSummaryLoading] = useState(false);
  const [isChatLoading, setIsChatLoading] = useState(false);

  // Function to validate YouTube URL format
  const isValidYoutubeUrl = (url: string): boolean => {
    const youtubeRegex = /^(https?:\/\/)?(www\.)?(youtube\.com|youtu\.be)\/.+/;
    return youtubeRegex.test(url);
  };

  // Extract video ID from URL
  const extractVideoId = (url: string): string | null => {
    const match = url.match(
      /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
    );
    return match ? match[1] : null;
  };

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setError(null);

    if (!isValidYoutubeUrl(url)) {
      toast.error("Please enter a valid YouTube URL");
      return;
    }

    const videoId = extractVideoId(url);
    if (!videoId) {
      toast.error("Could not extract video ID from URL");
      return;
    }

    setLoading(true);
    setIsSummaryLoading(true);
    setSummary(null);

    try {
      console.log("Submitting URL:", url);
      const response = await fetch("/api/summarize", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ url }),
      });

      const data = await response.json();
      console.log("Response:", data);

      if (!response.ok) {
        throw new Error(data.error || "Failed to generate summary");
      }

      setSummary(data.summary);
      setIsUrlValidated(true);
      toast.success("Summary generated successfully!");
    } catch (error) {
      console.error("Error:", error);
      const errorMessage =
        error instanceof Error ? error.message : "Failed to generate summary";
      setError(errorMessage);
      toast.error(errorMessage);
    } finally {
      setLoading(false);
      setIsSummaryLoading(false);
    }
  };

  const validateUrl = async () => {
    if (!isValidYoutubeUrl(url)) {
      toast.error("Please enter a valid YouTube URL");
      return;
    }

    const videoId = extractVideoId(url);
    if (!videoId) {
      toast.error("Could not extract video ID from URL");
      return;
    }

    setValidating(true);
    setIsChatLoading(true);
    setError(null);

    try {
      // Just check if the transcript exists
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          url,
          question:
            "Just checking if the transcript exists. Please respond with 'Yes'.",
        }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || "Failed to validate URL");
      }

      setIsUrlValidated(true);
      setActiveTab("chat");
      toast.success("Podcast loaded successfully!");
    } catch (error) {
      console.error("Error:", error);
      const errorMessage =
        error instanceof Error ? error.message : "Failed to validate URL";
      setError(errorMessage);
      toast.error(errorMessage);
    } finally {
      setValidating(false);
      setIsChatLoading(false);
    }
  };

  const handleTabChange = (tab: "summary" | "chat") => {
    if (loading || validating) return;

    setActiveTab(tab);

    // If switching to chat and URL is not yet validated, validate it
    if (tab === "chat" && !isUrlValidated && url) {
      validateUrl();
    }

    // If switching to summary and no summary exists yet, generate it
    if (tab === "summary" && !summary && isUrlValidated) {
      setIsSummaryLoading(true);
      handleSubmit(new Event("submit") as any);
    }
  };

  // Handle URL input change
  const handleUrlChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    setUrl(e.target.value);
    // Reset validation state when URL changes
    if (isUrlValidated) {
      setIsUrlValidated(false);
      setSummary(null);
    }
  };

  return (
    <div className="min-h-screen bg-[#f7f7ff]">
      <Navbar />
      <Toaster position="bottom-right" />

      <div className="max-w-7xl mx-auto px-4">
        <div className="pt-8 pb-16 md:pt-12 md:pb-24 text-center">
          <h1 className="text-5xl md:text-6xl font-bold text-purple-700 mb-6">
            Transform Podcasts into
            <br />
            Interactive Conversations
          </h1>
          <p className="text-lg text-gray-600 max-w-3xl mx-auto mb-10">
            Paste any podcast URL and let AI create summaries and engage in
            meaningful conversations about the content
          </p>

          <div className="max-w-3xl mx-auto bg-white p-8 rounded-2xl shadow-md">
            <div className="flex items-center mb-4">
              <div className="w-6 h-6 mr-2">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                  className="text-purple-600"
                >
                  <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
                  <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
                </svg>
              </div>
              <h2 className="text-xl font-semibold">Enter Podcast URL</h2>
            </div>

            <form onSubmit={handleSubmit}>
              <div className="flex flex-col gap-4">
                <div className="flex flex-col md:flex-row gap-2">
                  <input
                    id="youtube-url"
                    type="text"
                    value={url}
                    onChange={handleUrlChange}
                    placeholder="https://podcast-url.com/episode"
                    className="flex-1 p-3 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-purple-500"
                    disabled={loading || validating}
                    aria-label="YouTube URL"
                  />
                  <button
                    type="submit"
                    disabled={loading || validating || !url.trim()}
                    className="px-6 py-3 bg-purple-600 text-white rounded-lg hover:bg-purple-700 disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center"
                    aria-label={loading ? "Processing..." : "Analyze"}
                  >
                    {loading ? (
                      <div className="flex items-center">
                        <ArrowPathIcon className="w-5 h-5 animate-spin mr-2" />
                        <span>Processing...</span>
                      </div>
                    ) : (
                      <div className="flex items-center">
                        <svg
                          xmlns="http://www.w3.org/2000/svg"
                          viewBox="0 0 24 24"
                          fill="currentColor"
                          className="w-5 h-5 mr-2"
                        >
                          <path d="M18.375 2.25c-1.035 0-1.875.84-1.875 1.875v15.75c0 1.035.84 1.875 1.875 1.875h.75c1.035 0 1.875-.84 1.875-1.875V4.125c0-1.036-.84-1.875-1.875-1.875h-.75zM9.75 8.625c0-1.036.84-1.875 1.875-1.875h.75c1.036 0 1.875.84 1.875 1.875v11.25c0 1.035-.84 1.875-1.875 1.875h-.75c-1.036 0-1.875-.84-1.875-1.875V8.625zM3 13.125c0-1.036.84-1.875 1.875-1.875h.75c1.036 0 1.875.84 1.875 1.875v6.75c0 1.035-.84 1.875-1.875 1.875h-.75C3.84 21.75 3 20.91 3 19.875v-6.75z" />
                        </svg>
                        Analyze
                      </div>
                    )}
                  </button>
                </div>
                {error && (
                  <div className="text-red-600 text-sm p-2 bg-red-50 rounded">
                    Error: {error}
                  </div>
                )}
              </div>
            </form>
          </div>
        </div>

        {/* Feature Cards Section - only shown when no podcast summary is generated */}
        {!isUrlValidated && (
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-8 max-w-7xl mx-auto px-4 pb-20">
            {/* Smart Summaries Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M12 6.042A8.967 8.967 0 006 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 016 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 016-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0018 18a8.967 8.967 0 00-6 2.292m0-14.25v14.25" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Smart Summaries
              </h3>
              <p className="text-gray-600">
                Get comprehensive summaries that capture the key points and
                insights from any podcast.
              </p>
            </div>

            {/* Interactive Chat Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M4.913 2.658c2.075-.27 4.19-.408 6.337-.408 2.147 0 4.262.139 6.337.408 1.922.25 3.291 1.861 3.405 3.727a4.403 4.403 0 00-1.032-.211 50.89 50.89 0 00-8.42 0c-2.358.196-4.04 2.19-4.04 4.434v4.286a4.47 4.47 0 002.433 3.984L7.28 21.53A.75.75 0 016 21v-4.03a48.527 48.527 0 01-1.087-.128C2.905 16.58 1.5 14.833 1.5 12.862V6.638c0-1.97 1.405-3.718 3.413-3.979z" />
                  <path d="M15.75 7.5c-1.376 0-2.739.057-4.086.169C10.124 7.797 9 9.103 9 10.609v4.285c0 1.507 1.128 2.814 2.67 2.94 1.243.102 2.5.157 3.768.165l2.782 2.781a.75.75 0 001.28-.53v-2.39l.33-.026c1.542-.125 2.67-1.433 2.67-2.94v-4.286c0-1.505-1.125-2.811-2.664-2.94A49.392 49.392 0 0015.75 7.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Interactive Chat
              </h3>
              <p className="text-gray-600">
                Ask questions about the podcast content and get detailed answers
                from our AI assistant.
              </p>
            </div>

            {/* Works Everywhere Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
                  <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Works Everywhere
              </h3>
              <p className="text-gray-600">
                Compatible with podcasts from Spotify, Apple Podcasts, YouTube,
                and many more platforms.
              </p>
            </div>

            {/* AI-Powered Insights Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M21.731 2.269a2.625 2.625 0 00-3.712 0l-1.157 1.157 3.712 3.712 1.157-1.157a2.625 2.625 0 000-3.712zM19.513 8.199l-3.712-3.712-8.4 8.4a5.25 5.25 0 00-1.32 2.214l-.8 2.685a.75.75 0 00.933.933l2.685-.8a5.25 5.25 0 002.214-1.32l8.4-8.4z" />
                  <path d="M5.25 5.25a3 3 0 00-3 3v10.5a3 3 0 003 3h10.5a3 3 0 003-3V13.5a.75.75 0 00-1.5 0v5.25a1.5 1.5 0 01-1.5 1.5H5.25a1.5 1.5 0 01-1.5-1.5V8.25a1.5 1.5 0 011.5-1.5h5.25a.75.75 0 000-1.5H5.25z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                AI-Powered Insights
              </h3>
              <p className="text-gray-600">
                Advanced AI analyzes podcasts to extract the most relevant
                information and context.
              </p>
            </div>
          </div>
        )}

        {isUrlValidated && (
          <div className="max-w-5xl mx-auto">
            <PodcastMetadataProvider videoUrl={url}>
              {(metadata, metadataLoading) => (
                <>
                  {metadata && (
                    <PodcastHeader
                      videoId={metadata.videoId}
                      title={metadata.title}
                      channelName={metadata.channelName}
                      duration={metadata.duration}
                      viewCount={metadata.viewCount}
                      likeCount={metadata.likeCount}
                      publishedAt={metadata.publishedAt}
                      onChatClick={() => handleTabChange("chat")}
                      activeTab={activeTab}
                    />
                  )}

                  <div className="bg-white rounded-lg mb-8">
                    <div className="flex border-b">
                      <button
                        onClick={() => handleTabChange("summary")}
                        className={`flex-1 py-4 font-medium text-center transition-colors ${
                          activeTab === "summary"
                            ? "text-purple-600 border-b-2 border-purple-600 bg-purple-50"
                            : "text-gray-500 hover:text-gray-700 hover:bg-gray-50"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Summary tab"
                        aria-selected={activeTab === "summary"}
                        role="tab"
                      >
                        <div className="flex items-center justify-center">
                          <svg
                            xmlns="http://www.w3.org/2000/svg"
                            className="h-5 w-5 mr-2"
                            fill="none"
                            viewBox="0 0 24 24"
                            stroke="currentColor"
                          >
                            <path
                              strokeLinecap="round"
                              strokeLinejoin="round"
                              strokeWidth={2}
                              d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-14.25C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.247 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"
                            />
                          </svg>
                          <span>Summary</span>
                          {isSummaryLoading && (
                            <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                          )}
                        </div>
                      </button>
                      <button
                        onClick={() => handleTabChange("chat")}
                        className={`flex-1 py-4 font-medium text-center transition-colors ${
                          activeTab === "chat"
                            ? "text-purple-600 border-b-2 border-purple-600 bg-purple-50"
                            : "text-gray-500 hover:text-gray-700 hover:bg-gray-50"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Chat Assistant tab"
                        aria-selected={activeTab === "chat"}
                        role="tab"
                      >
                        <div className="flex items-center justify-center">
                          <svg
                            xmlns="http://www.w3.org/2000/svg"
                            className="h-5 w-5 mr-2"
                            fill="none"
                            viewBox="0 0 24 24"
                            stroke="currentColor"
                          >
                            <path
                              strokeLinecap="round"
                              strokeLinejoin="round"
                              strokeWidth={2}
                              d="M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z"
                            />
                          </svg>
                          <span>Chat Assistant</span>
                          {isChatLoading && (
                            <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                          )}
                        </div>
                      </button>
                    </div>

                    <div
                      role="tabpanel"
                      aria-label="Tab Content"
                      className="w-full"
                    >
                      <div className="max-w-full">
                        {activeTab === "summary" && (
                          <Summary summary={summary || ""} videoUrl={url} />
                        )}
                        {activeTab === "chat" && <Chat videoUrl={url} />}
                      </div>
                    </div>
                  </div>
                </>
              )}
            </PodcastMetadataProvider>
          </div>
        )}
      </div>
    </div>
  );
}
</file>

<file path="app/validate-metadata/page.tsx">
"use client";

import { useState } from "react";

export default function ValidateMetadata() {
  const [url, setUrl] = useState("");
  const [metadata, setMetadata] = useState<any>(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState("");
  const [showFullDescription, setShowFullDescription] = useState(false);

  const validateMetadata = async () => {
    setLoading(true);
    setError("");
    setShowFullDescription(false); // Reset description state on new validation
    try {
      const response = await fetch("/api/podcast-metadata", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          url,
          includeFull: true, // Always request the full description in the API
        }),
      });

      const data = await response.json();
      if (!response.ok) {
        throw new Error(data.error || "Failed to validate metadata");
      }

      setMetadata(data.metadata);
    } catch (err: any) {
      setError(err.message || "An error occurred");
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="p-6 max-w-4xl mx-auto">
      <h1 className="text-2xl font-bold mb-4">YouTube Metadata Validator</h1>
      <div className="mb-4">
        <input
          type="text"
          value={url}
          onChange={(e) => setUrl(e.target.value)}
          placeholder="Enter YouTube URL"
          className="w-full p-2 border rounded"
        />
      </div>
      <button
        onClick={validateMetadata}
        disabled={loading}
        className="px-4 py-2 bg-blue-500 text-white rounded disabled:bg-blue-300"
      >
        {loading ? "Validating..." : "Validate Metadata"}
      </button>

      {error && (
        <div className="mt-4 p-3 bg-red-100 text-red-700 rounded">{error}</div>
      )}

      {metadata && (
        <div className="mt-6">
          <h2 className="text-xl font-semibold mb-2">Metadata Results:</h2>

          <div className="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
            <div className="bg-gray-50 p-4 rounded shadow">
              <h3 className="font-medium text-lg mb-2">Basic Information</h3>
              <div className="space-y-2">
                <p>
                  <span className="font-semibold">Title:</span> {metadata.title}
                </p>
                <p>
                  <span className="font-semibold">Channel:</span>{" "}
                  {metadata.channelName}
                </p>
                <p>
                  <span className="font-semibold">Duration:</span>{" "}
                  {metadata.duration}
                </p>
                <p>
                  <span className="font-semibold">Video ID:</span>{" "}
                  {metadata.videoId}
                </p>
                {metadata.publishedAt && (
                  <p>
                    <span className="font-semibold">Published:</span>{" "}
                    {new Date(metadata.publishedAt).toLocaleDateString()}
                  </p>
                )}
                {metadata.viewCount && (
                  <p>
                    <span className="font-semibold">Views:</span>{" "}
                    {parseInt(metadata.viewCount).toLocaleString()}
                  </p>
                )}
                {metadata.likeCount && (
                  <p>
                    <span className="font-semibold">Likes:</span>{" "}
                    {parseInt(metadata.likeCount).toLocaleString()}
                  </p>
                )}
              </div>
            </div>

            <div className="bg-gray-50 p-4 rounded shadow">
              <h3 className="font-medium text-lg mb-2">Description</h3>
              <div className="max-h-60 overflow-y-auto">
                <p className="whitespace-pre-wrap">
                  {showFullDescription
                    ? metadata.fullDescription
                    : metadata.description}
                </p>
                {metadata.descriptionTruncated && (
                  <button
                    onClick={() => setShowFullDescription(!showFullDescription)}
                    className="mt-2 text-blue-500 hover:text-blue-700 text-sm font-medium"
                  >
                    {showFullDescription ? "Show less" : "Show more"}
                  </button>
                )}
              </div>
            </div>
          </div>

          {metadata.thumbnails && (
            <div className="bg-gray-50 p-4 rounded shadow">
              <h3 className="font-medium text-lg mb-2">Thumbnails</h3>
              <div className="grid grid-cols-2 md:grid-cols-4 gap-4">
                {Object.entries(metadata.thumbnails).map(
                  ([key, thumb]: [string, any]) => (
                    <div key={key} className="text-center">
                      <img
                        src={thumb.url}
                        alt={`${key} thumbnail`}
                        className="mx-auto mb-2 rounded"
                      />
                      <p className="text-sm">
                        {key}: {thumb.width}x{thumb.height}
                      </p>
                    </div>
                  )
                )}
              </div>
            </div>
          )}

          <div className="mt-4 bg-gray-100 p-4 rounded overflow-auto max-h-96">
            <h3 className="font-medium text-lg mb-2">Raw JSON Data</h3>
            <pre className="text-xs">{JSON.stringify(metadata, null, 2)}</pre>
          </div>
        </div>
      )}
    </div>
  );
}
</file>

<file path="app/validate-transcript/page.tsx">
"use client";

import { useState } from "react";
import { useRouter } from "next/navigation";

export default function ValidateTranscript() {
  const [url, setUrl] = useState("");
  const [error, setError] = useState<string | null>(null);
  const [transcript, setTranscript] = useState<string | null>(null);
  const [loading, setLoading] = useState(false);
  const router = useRouter();

  const validateTranscript = async () => {
    setLoading(true);
    setError(null);
    setTranscript(null);

    try {
      // Make a simple request to our chat API with a minimal question
      // This will test the transcript fetching functionality
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          url,
          question:
            "Just testing the transcript. Please say 'Transcript fetched successfully!'",
        }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || "Failed to validate transcript");
      }

      setTranscript(data.answer);
    } catch (error) {
      console.error("Error:", error);
      const errorMessage =
        error instanceof Error
          ? error.message
          : "Failed to validate transcript";
      setError(errorMessage);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="p-6 max-w-4xl mx-auto">
      <h1 className="text-2xl font-bold mb-4">YouTube Transcript Validator</h1>
      <div className="mb-4">
        <input
          type="text"
          value={url}
          onChange={(e) => setUrl(e.target.value)}
          placeholder="Enter YouTube URL"
          className="w-full p-2 border rounded"
        />
      </div>
      <button
        onClick={validateTranscript}
        disabled={loading}
        className="px-4 py-2 bg-blue-500 text-white rounded disabled:bg-blue-300"
      >
        {loading ? "Validating..." : "Validate Transcript"}
      </button>

      {error && (
        <div className="mt-4 p-3 bg-red-100 text-red-700 rounded">{error}</div>
      )}

      {transcript && (
        <div className="mt-6">
          <h2 className="text-xl font-semibold mb-2">Transcript Result:</h2>
          <div className="p-4 bg-gray-50 rounded-lg border">{transcript}</div>
        </div>
      )}

      <div className="mt-4">
        <button
          onClick={() => router.push("/")}
          className="px-4 py-2 bg-gray-200 rounded"
        >
          Back to Home
        </button>
      </div>
    </div>
  );
}
</file>

<file path="app/globals.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --foreground-rgb: 0, 0, 0;
  --background-start-rgb: 214, 219, 220;
  --background-end-rgb: 255, 255, 255;
}

body {
  color: rgb(var(--foreground-rgb));
  background: linear-gradient(
      to bottom,
      transparent,
      rgb(var(--background-end-rgb))
    )
    rgb(var(--background-start-rgb));
}

@layer utilities {
  .prose {
    max-width: 65ch;
    line-height: 1.6;
  }

  .prose p {
    margin-bottom: 1.25em;
  }
}

/* Markdown styling */
pre {
  font-family: ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont,
    "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  line-height: 1.6;
  font-size: 1rem;
  background-color: #f3f4f6;
  padding: 1em;
  border-radius: 0.5em;
  overflow-x: auto;
  margin: 1.5em 0;
}

pre code {
  background-color: transparent !important;
  padding: 0 !important;
  border-radius: 0 !important;
}

/* Enhanced Markdown Content Styling */
.markdown-content {
  font-family: ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont,
    "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  line-height: 1.6;
  font-size: 1rem;
  color: #374151;
}

.markdown-content h1,
.markdown-content h2,
.markdown-content h3,
.markdown-content h4,
.markdown-content h5,
.markdown-content h6 {
  font-weight: 600;
  margin-top: 1.5em;
  margin-bottom: 0.75em;
  line-height: 1.3;
  color: #111827;
}

.markdown-content h1 {
  font-size: 1.875rem;
  margin-top: 0;
}

.markdown-content h2 {
  font-size: 1.5rem;
  border-bottom: 1px solid #e5e7eb;
  padding-bottom: 0.5em;
  margin-top: 2em;
}

.markdown-content h3 {
  font-size: 1.25rem;
}

.markdown-content p {
  margin-bottom: 1.25em;
}

.markdown-content ul,
.markdown-content ol {
  padding-left: 1.75em;
  margin: 1em 0;
}

.markdown-content li {
  margin-bottom: 0.5em;
  position: relative;
}

.markdown-content li p {
  margin-bottom: 0.5em;
}

.markdown-content code {
  font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas,
    "Liberation Mono", "Courier New", monospace;
  background-color: #f3f4f6;
  padding: 0.2em 0.4em;
  border-radius: 0.25em;
  font-size: 0.875em;
}

.markdown-content blockquote {
  border-left: 4px solid #e5e7eb;
  padding: 0.5em 1em;
  margin: 1.5em 0;
  background-color: #f9fafb;
  color: #4b5563;
}

.markdown-content a {
  color: #2563eb;
  text-decoration: underline;
  text-underline-offset: 2px;
}

.markdown-content a:hover {
  color: #1d4ed8;
}

.markdown-content hr {
  border: 0;
  border-top: 1px solid #e5e7eb;
  margin: 2em 0;
}

.markdown-content strong {
  font-weight: 600;
  color: #111827;
}

.markdown-content em {
  font-style: italic;
}

/* Timestamp styling */
.markdown-content p:has(code) {
  margin-top: 1.5em;
  font-weight: 500;
}

/* List item spacing */
.markdown-content ul li,
.markdown-content ol li {
  margin-bottom: 0.75em;
}

/* Nested lists */
.markdown-content ul ul,
.markdown-content ol ol,
.markdown-content ul ol,
.markdown-content ol ul {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}

/* Table styling */
.markdown-content table {
  width: 100%;
  border-collapse: collapse;
  margin: 1.5em 0;
}

.markdown-content th,
.markdown-content td {
  padding: 0.75em;
  border: 1px solid #e5e7eb;
}

.markdown-content th {
  background-color: #f9fafb;
  font-weight: 600;
  text-align: left;
}

.markdown-content tr:nth-child(even) {
  background-color: #f9fafb;
}

/* Image styling */
.markdown-content img {
  max-width: 100%;
  height: auto;
  border-radius: 0.375em;
  margin: 1.5em 0;
}
</file>

<file path="app/page.tsx.backup">
"use client";

import React, { useState, useEffect } from "react";
import { ArrowPathIcon } from "@heroicons/react/24/solid";
import toast, { Toaster } from "react-hot-toast";
import Summary from "../components/Summary";
import Chat from "../components/Chat";
import Navbar from "../components/Navbar";
import PodcastHeader from "../components/PodcastHeader";
import { PodcastMetadataProvider } from "../components/PodcastMetadata";

export default function Home() {
  const [url, setUrl] = useState("");
  const [loading, setLoading] = useState(false);
  const [validating, setValidating] = useState(false);
  const [summary, setSummary] = useState<string | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [activeTab, setActiveTab] = useState<"summary" | "chat">("summary");
  const [isUrlValidated, setIsUrlValidated] = useState(false);
  const [isSummaryLoading, setIsSummaryLoading] = useState(false);
  const [isChatLoading, setIsChatLoading] = useState(false);

  // Function to validate YouTube URL format
  const isValidYoutubeUrl = (url: string): boolean => {
    const youtubeRegex = /^(https?:\/\/)?(www\.)?(youtube\.com|youtu\.be)\/.+/;
    return youtubeRegex.test(url);
  };

  // Extract video ID from URL
  const extractVideoId = (url: string): string | null => {
    const match = url.match(
      /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
    );
    return match ? match[1] : null;
  };

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setError(null);

    if (!isValidYoutubeUrl(url)) {
      toast.error("Please enter a valid YouTube URL");
      return;
    }

    const videoId = extractVideoId(url);
    if (!videoId) {
      toast.error("Could not extract video ID from URL");
      return;
    }

    setLoading(true);
    setIsSummaryLoading(true);
    setSummary(null);

    try {
      console.log("Submitting URL:", url);
      const response = await fetch("/api/summarize", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ url }),
      });

      const data = await response.json();
      console.log("Response:", data);

      if (!response.ok) {
        throw new Error(data.error || "Failed to generate summary");
      }

      setSummary(data.summary);
      setIsUrlValidated(true);
      toast.success("Summary generated successfully!");
    } catch (error) {
      console.error("Error:", error);
      const errorMessage =
        error instanceof Error ? error.message : "Failed to generate summary";
      setError(errorMessage);
      toast.error(errorMessage);
    } finally {
      setLoading(false);
      setIsSummaryLoading(false);
    }
  };

  const validateUrl = async () => {
    if (!isValidYoutubeUrl(url)) {
      toast.error("Please enter a valid YouTube URL");
      return;
    }

    const videoId = extractVideoId(url);
    if (!videoId) {
      toast.error("Could not extract video ID from URL");
      return;
    }

    setValidating(true);
    setIsChatLoading(true);
    setError(null);

    try {
      // Just check if the transcript exists
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          url,
          question:
            "Just checking if the transcript exists. Please respond with 'Yes'.",
        }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || "Failed to validate URL");
      }

      setIsUrlValidated(true);
      setActiveTab("chat");
      toast.success("Podcast loaded successfully!");
    } catch (error) {
      console.error("Error:", error);
      const errorMessage =
        error instanceof Error ? error.message : "Failed to validate URL";
      setError(errorMessage);
      toast.error(errorMessage);
    } finally {
      setValidating(false);
      setIsChatLoading(false);
    }
  };

  const handleTabChange = (tab: "summary" | "chat") => {
    if (loading || validating) return;

    setActiveTab(tab);

    // If switching to chat and URL is not yet validated, validate it
    if (tab === "chat" && !isUrlValidated && url) {
      validateUrl();
    }

    // If switching to summary and no summary exists yet, generate it
    if (tab === "summary" && !summary && isUrlValidated) {
      setIsSummaryLoading(true);
      handleSubmit(new Event("submit") as any);
    }
  };

  // Handle URL input change
  const handleUrlChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    setUrl(e.target.value);
    // Reset validation state when URL changes
    if (isUrlValidated) {
      setIsUrlValidated(false);
      setSummary(null);
    }
  };

  return (
    <div className="min-h-screen bg-[#f7f7ff]">
      <Navbar />
      <Toaster position="bottom-right" />

      <div className="max-w-7xl mx-auto px-4">
        <div className="pt-8 pb-16 md:pt-12 md:pb-24 text-center">
          <h1 className="text-5xl md:text-6xl font-bold text-purple-700 mb-6">
            Transform Podcasts into
            <br />
            Interactive Conversations
          </h1>
          <p className="text-lg text-gray-600 max-w-3xl mx-auto mb-10">
            Paste any podcast URL and let AI create summaries and engage in
            meaningful conversations about the content
          </p>

          <div className="max-w-3xl mx-auto bg-white p-8 rounded-2xl shadow-md">
            <div className="flex items-center mb-4">
              <div className="w-6 h-6 mr-2">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                  className="text-purple-600"
                >
                  <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
                  <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
                </svg>
              </div>
              <h2 className="text-xl font-semibold">Enter Podcast URL</h2>
            </div>

            <form onSubmit={handleSubmit}>
              <div className="flex flex-col gap-4">
                <div className="flex flex-col md:flex-row gap-2">
                  <input
                    id="youtube-url"
                    type="text"
                    value={url}
                    onChange={handleUrlChange}
                    placeholder="https://podcast-url.com/episode"
                    className="flex-1 p-3 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-purple-500"
                    disabled={loading || validating}
                    aria-label="YouTube URL"
                  />
                  <button
                    type="submit"
                    disabled={loading || validating || !url.trim()}
                    className="px-6 py-3 bg-purple-600 text-white rounded-lg hover:bg-purple-700 disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center"
                    aria-label={loading ? "Processing..." : "Analyze"}
                  >
                    {loading ? (
                      <div className="flex items-center">
                        <ArrowPathIcon className="w-5 h-5 animate-spin mr-2" />
                        <span>Processing...</span>
                      </div>
                    ) : (
                      <div className="flex items-center">
                        <svg
                          xmlns="http://www.w3.org/2000/svg"
                          viewBox="0 0 24 24"
                          fill="currentColor"
                          className="w-5 h-5 mr-2"
                        >
                          <path d="M18.375 2.25c-1.035 0-1.875.84-1.875 1.875v15.75c0 1.035.84 1.875 1.875 1.875h.75c1.035 0 1.875-.84 1.875-1.875V4.125c0-1.036-.84-1.875-1.875-1.875h-.75zM9.75 8.625c0-1.036.84-1.875 1.875-1.875h.75c1.036 0 1.875.84 1.875 1.875v11.25c0 1.035-.84 1.875-1.875 1.875h-.75c-1.036 0-1.875-.84-1.875-1.875V8.625zM3 13.125c0-1.036.84-1.875 1.875-1.875h.75c1.036 0 1.875.84 1.875 1.875v6.75c0 1.035-.84 1.875-1.875 1.875h-.75C3.84 21.75 3 20.91 3 19.875v-6.75z" />
                        </svg>
                        Analyze
                      </div>
                    )}
                  </button>
                </div>
                {error && (
                  <div className="text-red-600 text-sm p-2 bg-red-50 rounded">
                    Error: {error}
                  </div>
                )}
              </div>
            </form>
          </div>
        </div>

        {/* Feature Cards Section - only shown when no podcast summary is generated */}
        {!isUrlValidated && (
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-8 max-w-7xl mx-auto px-4 pb-20">
            {/* Smart Summaries Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M12 6.042A8.967 8.967 0 006 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 016 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 016-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0018 18a8.967 8.967 0 00-6 2.292m0-14.25v14.25" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Smart Summaries
              </h3>
              <p className="text-gray-600">
                Get comprehensive summaries that capture the key points and
                insights from any podcast.
              </p>
            </div>

            {/* Interactive Chat Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M4.913 2.658c2.075-.27 4.19-.408 6.337-.408 2.147 0 4.262.139 6.337.408 1.922.25 3.291 1.861 3.405 3.727a4.403 4.403 0 00-1.032-.211 50.89 50.89 0 00-8.42 0c-2.358.196-4.04 2.19-4.04 4.434v4.286a4.47 4.47 0 002.433 3.984L7.28 21.53A.75.75 0 016 21v-4.03a48.527 48.527 0 01-1.087-.128C2.905 16.58 1.5 14.833 1.5 12.862V6.638c0-1.97 1.405-3.718 3.413-3.979z" />
                  <path d="M15.75 7.5c-1.376 0-2.739.057-4.086.169C10.124 7.797 9 9.103 9 10.609v4.285c0 1.507 1.128 2.814 2.67 2.94 1.243.102 2.5.157 3.768.165l2.782 2.781a.75.75 0 001.28-.53v-2.39l.33-.026c1.542-.125 2.67-1.433 2.67-2.94v-4.286c0-1.505-1.125-2.811-2.664-2.94A49.392 49.392 0 0015.75 7.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Interactive Chat
              </h3>
              <p className="text-gray-600">
                Ask questions about the podcast content and get detailed answers
                from our AI assistant.
              </p>
            </div>

            {/* Works Everywhere Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
                  <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Works Everywhere
              </h3>
              <p className="text-gray-600">
                Compatible with podcasts from Spotify, Apple Podcasts, YouTube,
                and many more platforms.
              </p>
            </div>

            {/* AI-Powered Insights Card */}
            <div className="bg-white p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M21.731 2.269a2.625 2.625 0 00-3.712 0l-1.157 1.157 3.712 3.712 1.157-1.157a2.625 2.625 0 000-3.712zM19.513 8.199l-3.712-3.712-8.4 8.4a5.25 5.25 0 00-1.32 2.214l-.8 2.685a.75.75 0 00.933.933l2.685-.8a5.25 5.25 0 002.214-1.32l8.4-8.4z" />
                  <path d="M5.25 5.25a3 3 0 00-3 3v10.5a3 3 0 003 3h10.5a3 3 0 003-3V13.5a.75.75 0 00-1.5 0v5.25a1.5 1.5 0 01-1.5 1.5H5.25a1.5 1.5 0 01-1.5-1.5V8.25a1.5 1.5 0 011.5-1.5h5.25a.75.75 0 000-1.5H5.25z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                AI-Powered Insights
              </h3>
              <p className="text-gray-600">
                Advanced AI analyzes podcasts to extract the most relevant
                information and context.
              </p>
            </div>
          </div>
        )}

        {isUrlValidated && (
          <div className="max-w-5xl mx-auto">
            <PodcastMetadataProvider videoUrl={url}>
              {(metadata, metadataLoading) => (
                <>
                  {metadata && (
                    <PodcastHeader
                      videoId={metadata.videoId}
                      title={metadata.title}
                      channelName={metadata.channelName}
                      duration={metadata.duration}
                      description={metadata.description}
                      onChatClick={() => handleTabChange("chat")}
                      activeTab={activeTab}
                    />
                  )}

                  <div className="bg-white rounded-lg shadow-md mb-8">
                    <div className="flex border-b">
                      <button
                        onClick={() => handleTabChange("summary")}
                        className={`px-4 py-3 font-medium ${
                          activeTab === "summary"
                            ? "text-purple-600 border-b-2 border-purple-600"
                            : "text-gray-500 hover:text-gray-700"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Summary tab"
                        aria-selected={activeTab === "summary"}
                        role="tab"
                      >
                        Summary{" "}
                        {isSummaryLoading && (
                          <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                        )}
                      </button>
                      <button
                        onClick={() => handleTabChange("chat")}
                        className={`px-4 py-3 font-medium ${
                          activeTab === "chat"
                            ? "text-purple-600 border-b-2 border-purple-600"
                            : "text-gray-500 hover:text-gray-700"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Chat Assistant tab"
                        aria-selected={activeTab === "chat"}
                        role="tab"
                      >
                        Chat Assistant{" "}
                        {isChatLoading && (
                          <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                        )}
                      </button>
                    </div>

                    <div className="flex border-b space-x-8 px-4">
                      <button
                        onClick={() => handleTabChange("summary")}
                        className={`pt-4 pb-3 font-medium text-base ${
                          activeTab === "summary"
                            ? "text-purple-600 border-b-2 border-purple-600"
                            : "text-gray-500 hover:text-gray-700"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Summary tab"
                        aria-selected={activeTab === "summary"}
                        role="tab"
                      >
                        <div className="flex items-center">
                          <span>Summary</span>
                          {isSummaryLoading && (
                            <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                          )}
                        </div>
                      </button>
                      <button
                        onClick={() => handleTabChange("chat")}
                        className={`pt-4 pb-3 font-medium text-base ${
                          activeTab === "chat"
                            ? "text-purple-600 border-b-2 border-purple-600"
                            : "text-gray-500 hover:text-gray-700"
                        }`}
                        disabled={loading || validating}
                        aria-label="Switch to Chat Assistant tab"
                        aria-selected={activeTab === "chat"}
                        role="tab"
                      >
                        <div className="flex items-center">
                          <span>Chat Assistant</span>
                          {isChatLoading && (
                            <span className="ml-2 inline-block w-4 h-4 border-2 border-purple-600 border-t-transparent rounded-full animate-spin"></span>
                          )}
                        </div>
                      </button>
                    </div>

                    <div
                      role="tabpanel"
                      aria-label="Tab Content"
                      className="p-8"
                    >
                      {activeTab === "summary" && (
                        <Summary summary={summary || ""} videoUrl={url} />
                      )}
                      {activeTab === "chat" && <Chat videoUrl={url} />}
                    </div>
                  </div>
                </>
              )}
            </PodcastMetadataProvider>
          </div>
        )}
      </div>
    </div>
  );
}
</file>

<file path="components/ErrorBoundary.tsx">
"use client";

import React, { Component, ErrorInfo, ReactNode } from "react";

interface Props {
  children: ReactNode;
  fallback?: ReactNode;
}

interface State {
  hasError: boolean;
  error: Error | null;
}

class ErrorBoundary extends Component<Props, State> {
  constructor(props: Props) {
    super(props);
    this.state = {
      hasError: false,
      error: null,
    };
  }

  static getDerivedStateFromError(error: Error): State {
    return {
      hasError: true,
      error,
    };
  }

  componentDidCatch(error: Error, errorInfo: ErrorInfo): void {
    console.error("Error caught by ErrorBoundary:", error, errorInfo);
  }

  render(): ReactNode {
    if (this.state.hasError) {
      if (this.props.fallback) {
        return this.props.fallback;
      }

      return (
        <div className="min-h-screen flex items-center justify-center bg-gray-50 p-4">
          <div className="bg-white p-8 rounded-lg shadow-md max-w-md w-full">
            <h2 className="text-2xl font-bold text-red-600 mb-4">
              Something went wrong
            </h2>
            <p className="text-gray-700 mb-4">
              We're sorry, but there was an error processing your request.
              Please try again later.
            </p>
            <div className="bg-gray-100 p-3 rounded text-sm text-gray-800 font-mono overflow-auto max-h-32 mb-4">
              {this.state.error?.message || "Unknown error"}
            </div>
            <button
              onClick={() => window.location.reload()}
              className="w-full py-2 px-4 bg-blue-600 text-white rounded hover:bg-blue-700 transition-colors"
            >
              Reload Page
            </button>
          </div>
        </div>
      );
    }

    return this.props.children;
  }
}

export default ErrorBoundary;
</file>

<file path="components/PodcastHeader.tsx">
import React, { useState } from "react";
import {
  ClockIcon,
  ChatBubbleLeftRightIcon,
  ShareIcon,
  EyeIcon,
  HandThumbUpIcon,
  CalendarIcon,
} from "@heroicons/react/24/outline";

interface PodcastHeaderProps {
  videoId: string;
  title: string;
  channelName: string;
  duration: string;
  viewCount?: string | null;
  likeCount?: string | null;
  publishedAt?: string | null;
  onChatClick: () => void;
  activeTab: "summary" | "chat";
}

const PodcastHeader: React.FC<PodcastHeaderProps> = ({
  videoId,
  title,
  channelName,
  duration,
  viewCount,
  likeCount,
  publishedAt,
  onChatClick,
  activeTab,
}) => {
  // Use the highest quality thumbnail available (maxresdefault is best quality)
  // With fallback to hqdefault if maxresdefault isn't available
  const thumbnailUrl = `https://i.ytimg.com/vi/${videoId}/maxresdefault.jpg`;
  const fallbackThumbnailUrl = `https://i.ytimg.com/vi/${videoId}/hqdefault.jpg`;
  const [imgSrc, setImgSrc] = useState(thumbnailUrl);

  // Format the published date
  const formatDate = (dateString: string | null) => {
    if (!dateString) return "Unknown date";

    const date = new Date(dateString);
    return date.toLocaleDateString("en-US", {
      year: "numeric",
      month: "long",
      day: "numeric",
    });
  };

  const handleShare = () => {
    if (navigator.share) {
      navigator
        .share({
          title: title,
          text: `Check out this podcast: ${title}`,
          url: `https://www.youtube.com/watch?v=${videoId}`,
        })
        .catch((err) => console.error("Error sharing:", err));
    } else {
      const url = `https://www.youtube.com/watch?v=${videoId}`;
      navigator.clipboard.writeText(url);
      alert("Link copied to clipboard!");
    }
  };

  return (
    <div className="bg-white rounded-xl shadow-md overflow-hidden mb-6">
      <div className="md:flex">
        {/* Fixed width container that matches 16:9 aspect ratio at 420x240 */}
        <div className="md:w-[420px] md:flex-shrink-0 relative">
          {/* Mobile: Dynamic 16:9 aspect ratio with padding trick */}
          {/* Desktop: Fixed height matching 16:9 ratio of width */}
          <div className="w-full pt-[56.25%] md:pt-0 md:h-[236px]">
            <img
              className="absolute inset-0 w-full h-full object-cover"
              src={imgSrc}
              alt={title}
              onError={() => setImgSrc(fallbackThumbnailUrl)}
            />
          </div>
        </div>
        <div className="p-6 flex flex-col justify-between w-full max-w-full">
          <div>
            <p className="text-sm text-purple-600 font-semibold uppercase tracking-wide">
              {channelName}
            </p>
            <h1 className="text-2xl font-bold text-gray-900 mt-1 mb-4">
              {title}
            </h1>

            {/* Metadata stats */}
            <div className="grid grid-cols-1 md:grid-cols-2 gap-y-2 gap-x-4 mb-4">
              <div className="flex items-center text-gray-600">
                <ClockIcon className="h-5 w-5 mr-1.5 text-gray-500" />
                <span>{duration}</span>
              </div>

              {publishedAt && (
                <div className="flex items-center text-gray-600">
                  <CalendarIcon className="h-5 w-5 mr-1.5 text-gray-500" />
                  <span>{formatDate(publishedAt)}</span>
                </div>
              )}

              {viewCount && (
                <div className="flex items-center text-gray-600">
                  <EyeIcon className="h-5 w-5 mr-1.5 text-gray-500" />
                  <span>{parseInt(viewCount).toLocaleString()} views</span>
                </div>
              )}

              {likeCount && (
                <div className="flex items-center text-gray-600">
                  <HandThumbUpIcon className="h-5 w-5 mr-1.5 text-gray-500" />
                  <span>{parseInt(likeCount).toLocaleString()} likes</span>
                </div>
              )}
            </div>
          </div>

          <div className="mt-4 flex gap-3">
            <button
              onClick={onChatClick}
              className={`flex items-center px-4 py-2 rounded-lg text-sm font-medium transition-colors ${
                activeTab === "chat"
                  ? "bg-purple-600 text-white"
                  : "bg-purple-100 text-purple-700 hover:bg-purple-200"
              }`}
            >
              <ChatBubbleLeftRightIcon className="h-4 w-4 mr-2" />
              Chat About This
            </button>
            <button
              onClick={handleShare}
              className="flex items-center px-4 py-2 bg-gray-100 text-gray-700 rounded-lg text-sm font-medium hover:bg-gray-200 transition-colors"
            >
              <ShareIcon className="h-4 w-4 mr-2" />
              Share
            </button>
          </div>
        </div>
      </div>
    </div>
  );
};

export default PodcastHeader;
</file>

<file path="components/PodcastMetadata.tsx">
import React, { useEffect, useState } from "react";

export interface PodcastMetadata {
  title: string;
  channelName: string;
  duration: string;
  videoId: string;
  description?: string;
  fullDescription?: string;
  descriptionTruncated?: boolean;
  publishedAt?: string | null;
  viewCount?: string | null;
  likeCount?: string | null;
  thumbnails?: {
    [key: string]: {
      url: string;
      width: number;
      height: number;
    };
  } | null;
}

interface PodcastMetadataProviderProps {
  videoUrl: string;
  children: (
    metadata: PodcastMetadata | null,
    loading: boolean
  ) => React.ReactNode;
}

const extractVideoId = (url: string): string | null => {
  const match = url.match(
    /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
  );
  return match ? match[1] : null;
};

const formatDuration = (seconds: number): string => {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);

  if (hours > 0) {
    return `${hours}:${minutes.toString().padStart(2, "0")}:${secs
      .toString()
      .padStart(2, "0")}`;
  }

  return `${minutes}:${secs.toString().padStart(2, "0")}`;
};

export const PodcastMetadataProvider: React.FC<
  PodcastMetadataProviderProps
> = ({ videoUrl, children }) => {
  const [metadata, setMetadata] = useState<PodcastMetadata | null>(null);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    const fetchMetadata = async () => {
      try {
        setLoading(true);
        const videoId = extractVideoId(videoUrl);

        if (!videoId) {
          throw new Error("Invalid YouTube URL");
        }

        // Call our metadata API endpoint
        const response = await fetch("/api/podcast-metadata", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify({
            url: videoUrl,
            includeFull: true, // Always request full description
          }),
        });

        if (!response.ok) {
          const errorData = await response.json();
          throw new Error(errorData.error || "Failed to fetch metadata");
        }

        const data = await response.json();
        setMetadata(data.metadata);
      } catch (error) {
        console.error("Error fetching metadata:", error);

        // Fallback metadata using the video ID
        const videoId = extractVideoId(videoUrl);
        if (videoId) {
          setMetadata({
            title: "YouTube Podcast",
            channelName: "Unknown Channel",
            duration: "00:00",
            videoId: videoId,
            description: "No description available for this podcast.",
            fullDescription: "No description available for this podcast.",
            descriptionTruncated: false,
          });
        } else {
          setMetadata(null);
        }
      } finally {
        setLoading(false);
      }
    };

    if (videoUrl) {
      fetchMetadata();
    }
  }, [videoUrl]);

  return <>{children(metadata, loading)}</>;
};
</file>

<file path="lib/supabase/client.ts">
import { createBrowserClient } from "@supabase/ssr";

export function createClient() {
  // Create a supabase client on the browser with project's credentials
  return createBrowserClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
  );
}
</file>

<file path="lib/supabase/server.ts">
import { createServerClient, type CookieOptions } from "@supabase/ssr";
import { cookies } from "next/headers";

export function createClient() {
  const cookieStore = cookies();

  return createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        get(name: string) {
          return cookieStore.get(name)?.value;
        },
        set(name: string, value: string, options: CookieOptions) {
          try {
            cookieStore.set({ name, value, ...options });
          } catch (error) {
            // The `set` method was called from a Server Component.
            // This can be ignored if you have middleware refreshing
            // user sessions.
          }
        },
        remove(name: string, options: CookieOptions) {
          try {
            cookieStore.set({ name, value: "", ...options });
          } catch (error) {
            // The `delete` method was called from a Server Component.
            // This can be ignored if you have middleware refreshing
            // user sessions.
          }
        },
      },
    }
  );
}
</file>

<file path="lib/supabaseClient.ts">
import { createClient } from "@supabase/supabase-js";

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
const supabaseAnonKey = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY;

if (!supabaseUrl || !supabaseAnonKey) {
  throw new Error("Missing Supabase URL or Anon Key in environment variables");
}

export const supabase = createClient(supabaseUrl, supabaseAnonKey);
</file>

<file path="lib/youtube.ts">
import { PodcastMetadata } from "@/components/PodcastMetadata"; // Corrected import path

const YOUTUBE_API_KEY = process.env.YOUTUBE_API_KEY;
const YOUTUBE_API_URL = "https://www.googleapis.com/youtube/v3/videos";

/**
 * Fetches video metadata directly from the YouTube Data API v3.
 *
 * @param videoId - The ID of the YouTube video.
 * @returns A promise that resolves to the podcast metadata or null if fetching fails.
 */
export async function fetchMetadataFromYouTubeAPI(
  videoId: string
): Promise<Partial<PodcastMetadata> | null> {
  if (!YOUTUBE_API_KEY) {
    console.warn("YouTube API key is missing. Cannot fetch metadata via API.");
    return null;
  }

  const url = `${YOUTUBE_API_URL}?part=snippet,contentDetails,statistics&id=${videoId}&key=${YOUTUBE_API_KEY}`;

  try {
    console.log(`Fetching metadata from YouTube API for video ID: ${videoId}`);
    const response = await fetch(url, {
      headers: {
        Accept: "application/json",
      },
      next: { revalidate: 3600 }, // Cache for 1 hour
      signal: AbortSignal.timeout(10000), // 10 second timeout
    });

    if (!response.ok) {
      const errorData = await response.text();
      console.error(
        `YouTube API request failed with status ${response.status}: ${errorData}`
      );
      // Distinguish between common errors
      if (response.status === 401 || response.status === 403) {
        console.error(
          "Potential YouTube API Key issue (invalid, restricted, or quota exceeded)."
        );
      } else if (response.status === 404) {
        console.error(`Video with ID ${videoId} not found via YouTube API.`);
      }
      return null; // Indicate failure to the caller
    }

    const data = await response.json();

    if (!data.items || data.items.length === 0) {
      console.warn(
        `No items found in YouTube API response for video ID: ${videoId}`
      );
      return null;
    }

    const item = data.items[0];
    const snippet = item.snippet;
    const contentDetails = item.contentDetails;
    const statistics = item.statistics;

    // Basic validation
    if (!snippet || !contentDetails || !statistics) {
      console.warn(
        `Incomplete data received from YouTube API for video ID: ${videoId}`
      );
      return null;
    }

    // Construct the thumbnails object expected by the interface
    const bestThumbnail =
      snippet.thumbnails?.maxres ||
      snippet.thumbnails?.high ||
      snippet.thumbnails?.medium ||
      snippet.thumbnails?.default;
    const thumbnailsData = bestThumbnail
      ? {
          high: {
            url: bestThumbnail.url,
            width: bestThumbnail.width,
            height: bestThumbnail.height,
          },
        }
      : null;

    // Format duration if needed (implement formatISODuration)
    const formattedDuration = contentDetails.duration
      ? formatISODuration(contentDetails.duration)
      : "0:00";

    const metadata: Partial<PodcastMetadata> = {
      videoId: videoId,
      title: snippet.title || "Untitled Podcast",
      channelName: snippet.channelTitle || "Unknown Channel",
      thumbnails: thumbnailsData,
      publishedAt: snippet.publishedAt || null,
      description: snippet.description || null,
      fullDescription: snippet.description || null,
      duration: formattedDuration,
      viewCount: statistics.viewCount || null,
      likeCount: statistics.likeCount || null,
      descriptionTruncated: snippet.description
        ? snippet.description.length > 300
        : false,
    };

    console.log(
      `Successfully fetched metadata from YouTube API for video ID: ${videoId}`
    );
    return metadata;
  } catch (error: any) {
    console.error(
      `Error fetching metadata from YouTube API for video ID ${videoId}:`,
      error.message || error
    );
    // Add more specific error logging if needed
    if (error.name === "AbortError") {
      console.error("YouTube API request timed out.");
    } else if (error instanceof TypeError) {
      console.error("Network error or issue constructing YouTube API request.");
    }
    return null; // Indicate failure
  }
}

// Helper function to format ISO 8601 duration to readable time (HH:MM:SS or MM:SS)
// This should match the format expected by PodcastMetadata interface
const formatISODuration = (isoDuration: string): string => {
  const regex = /PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?/;
  const matches = isoDuration.match(regex);

  if (!matches) {
    return "0:00"; // Default or error format
  }

  const hours = matches[1] ? parseInt(matches[1]) : 0;
  const minutes = matches[2] ? parseInt(matches[2]) : 0;
  const seconds = matches[3] ? parseInt(matches[3]) : 0;

  if (hours > 0) {
    return `${hours}:${minutes.toString().padStart(2, "0")}:${seconds
      .toString()
      .padStart(2, "0")}`;
  }

  return `${minutes}:${seconds.toString().padStart(2, "0")}`;
};

// Define PodcastMetadata type if not imported globally or adjust import path
// interface PodcastMetadata {
//   videoId: string;
//   title: string;
//   channelName: string;
//   thumbnailUrl: string;
// //   duration?: number; // In seconds - optional
// }
</file>

<file path="public/create-screenshot.html">
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>App Screenshot Generator</title>
    <style>
      body {
        margin: 0;
        padding: 0;
        font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
          Roboto, "Helvetica Neue", Arial, sans-serif;
      }
      .app-container {
        width: 1200px;
        height: 800px;
        overflow: hidden;
        background-color: #f7f7ff;
        position: relative;
      }
      .app-header {
        background-color: white;
        padding: 20px;
        border-bottom: 1px solid #e5e7eb;
        display: flex;
        align-items: center;
      }
      .app-logo {
        width: 40px;
        height: 40px;
        background-color: #8b5cf6;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        margin-right: 12px;
      }
      .app-title {
        font-weight: 700;
        font-size: 22px;
        color: #8b5cf6;
      }
      .content {
        max-width: 1000px;
        margin: 0 auto;
        padding: 30px 20px;
      }
      .podcast-card {
        background-color: white;
        border-radius: 12px;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        overflow: hidden;
      }
      .podcast-header {
        display: flex;
        padding: 24px;
        border-bottom: 1px solid #e5e7eb;
      }
      .podcast-thumbnail {
        width: 180px;
        height: 100px;
        background-color: #e0e7ff;
        border-radius: 8px;
        margin-right: 20px;
        flex-shrink: 0;
        display: flex;
        align-items: center;
        justify-content: center;
      }
      .podcast-info {
        flex: 1;
      }
      .podcast-title {
        font-size: 20px;
        font-weight: 600;
        margin-bottom: 8px;
        color: #1f2937;
      }
      .podcast-author {
        font-size: 16px;
        color: #6b7280;
        margin-bottom: 12px;
      }
      .podcast-metadata {
        display: flex;
        gap: 16px;
        font-size: 14px;
        color: #9ca3af;
      }
      .tabs {
        display: flex;
        border-bottom: 1px solid #e5e7eb;
      }
      .tab {
        flex: 1;
        padding: 16px;
        text-align: center;
        font-weight: 500;
        cursor: pointer;
      }
      .tab.active {
        color: #8b5cf6;
        border-bottom: 2px solid #8b5cf6;
        background-color: #f5f3ff;
      }
      .tab-content {
        padding: 24px;
      }
      .chat-container {
        display: flex;
        flex-direction: column;
        height: 500px;
      }
      .chat-messages {
        flex: 1;
        overflow-y: auto;
        padding: 16px;
        display: flex;
        flex-direction: column;
        gap: 16px;
      }
      .message-pair {
        display: flex;
        flex-direction: column;
        gap: 16px;
        margin-bottom: 24px;
      }
      .message {
        max-width: 85%;
        border-radius: 12px;
        overflow: hidden;
      }
      .message-avatar {
        width: 36px;
        height: 36px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        margin-right: 12px;
        flex-shrink: 0;
      }
      .user-avatar {
        background-color: #e0e7ff;
        color: #4338ca;
      }
      .ai-avatar {
        background-color: #ede9fe;
        color: #8b5cf6;
      }
      .message-content {
        flex: 1;
      }
      .message-wrapper {
        display: flex;
        align-items: flex-start;
      }
      .user-message .message-content {
        background-color: #f3f4f6;
        padding: 16px;
        border-radius: 12px;
        color: #1f2937;
      }
      .ai-message .message-content {
        background-color: #ede9fe;
        padding: 16px;
        border-radius: 12px;
        color: #1f2937;
      }
      .chat-input {
        display: flex;
        padding: 16px;
        border-top: 1px solid #e5e7eb;
      }
      .chat-input input {
        flex: 1;
        padding: 12px 16px;
        border: 1px solid #d1d5db;
        border-radius: 8px;
        margin-right: 12px;
        font-size: 14px;
      }
      .chat-input button {
        padding: 0 16px;
        background-color: #8b5cf6;
        color: white;
        border: none;
        border-radius: 8px;
        font-weight: 500;
      }
      .message-list {
        margin: 0;
        padding-left: 20px;
      }
      .message-list li {
        margin-bottom: 10px;
      }
    </style>
  </head>
  <body>
    <div class="app-container">
      <div class="app-header">
        <div class="app-logo">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="white"
          >
            <path
              d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z"
            />
            <path
              d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z"
            />
          </svg>
        </div>
        <div class="app-title">PodAI</div>
      </div>

      <div class="content">
        <div class="podcast-card">
          <div class="podcast-header">
            <div class="podcast-thumbnail">
              <svg
                xmlns="http://www.w3.org/2000/svg"
                width="32"
                height="32"
                viewBox="0 0 24 24"
                fill="#8b5cf6"
              >
                <path
                  d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z"
                />
                <path
                  d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z"
                />
              </svg>
            </div>
            <div class="podcast-info">
              <div class="podcast-title">
                The AI Economy: Future Trends with Dr. Sophia Chen
              </div>
              <div class="podcast-author">Economics Insights Podcast</div>
              <div class="podcast-metadata">
                <span>58:45</span>
                <span>1.2M views</span>
                <span>87K likes</span>
                <span>June 10, 2023</span>
              </div>
            </div>
          </div>

          <div class="tabs">
            <div class="tab">Summary</div>
            <div class="tab active">Chat Assistant</div>
          </div>

          <div class="tab-content">
            <div class="chat-container">
              <div class="chat-messages">
                <div class="message-pair">
                  <div class="message-wrapper user-message">
                    <div class="message-avatar user-avatar">
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="20"
                        height="20"
                        viewBox="0 0 24 24"
                        fill="currentColor"
                      >
                        <path
                          d="M18.685 19.097A9.723 9.723 0 0021.75 12c0-5.385-4.365-9.75-9.75-9.75S2.25 6.615 2.25 12a9.723 9.723 0 003.065 7.097A9.716 9.716 0 0012 21.75a9.716 9.716 0 006.685-2.653zm-12.54-1.285A7.486 7.486 0 0112 15a7.486 7.486 0 015.855 2.812A8.224 8.224 0 0112 20.25a8.224 8.224 0 01-5.855-2.438zM15.75 9a3.75 3.75 0 11-7.5 0 3.75 3.75 0 017.5 0z"
                        />
                      </svg>
                    </div>
                    <div class="message-content">
                      What were the main economic impacts discussed in the
                      podcast?
                    </div>
                  </div>

                  <div class="message-wrapper ai-message">
                    <div class="message-avatar ai-avatar">
                      <svg
                        xmlns="http://www.w3.org/2000/svg"
                        width="20"
                        height="20"
                        viewBox="0 0 24 24"
                        fill="currentColor"
                      >
                        <path
                          d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z"
                        />
                        <path
                          d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z"
                        />
                      </svg>
                    </div>
                    <div class="message-content">
                      <p>
                        The podcast highlighted three main economic impacts:
                      </p>
                      <ol class="message-list">
                        <li>
                          AI automation could increase global GDP by up to 14%
                          by 2030
                        </li>
                        <li>
                          Job market disruption with an estimated 15% of jobs
                          being redefined
                        </li>
                        <li>
                          New industries emerging that could create 97 million
                          new roles
                        </li>
                      </ol>
                      <p>
                        Dr. Chen specifically emphasized the need for reskilling
                        programs as this transition occurs.
                      </p>
                    </div>
                  </div>
                </div>

                <div class="message-wrapper user-message">
                  <div class="message-avatar user-avatar">
                    <svg
                      xmlns="http://www.w3.org/2000/svg"
                      width="20"
                      height="20"
                      viewBox="0 0 24 24"
                      fill="currentColor"
                    >
                      <path
                        d="M18.685 19.097A9.723 9.723 0 0021.75 12c0-5.385-4.365-9.75-9.75-9.75S2.25 6.615 2.25 12a9.723 9.723 0 003.065 7.097A9.716 9.716 0 0012 21.75a9.716 9.716 0 006.685-2.653zm-12.54-1.285A7.486 7.486 0 0112 15a7.486 7.486 0 015.855 2.812A8.224 8.224 0 0112 20.25a8.224 8.224 0 01-5.855-2.438zM15.75 9a3.75 3.75 0 11-7.5 0 3.75 3.75 0 017.5 0z"
                      />
                    </svg>
                  </div>
                  <div class="message-content">
                    What sectors did she mention would be most affected?
                  </div>
                </div>
              </div>

              <div class="chat-input">
                <input
                  type="text"
                  placeholder="Ask a question about the podcast content..."
                />
                <button>
                  <svg
                    xmlns="http://www.w3.org/2000/svg"
                    width="20"
                    height="20"
                    viewBox="0 0 24 24"
                    fill="none"
                    stroke="currentColor"
                    stroke-width="2"
                    stroke-linecap="round"
                    stroke-linejoin="round"
                  >
                    <line x1="22" y1="2" x2="11" y2="13"></line>
                    <polygon points="22 2 15 22 11 13 2 9 22 2"></polygon>
                  </svg>
                </button>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Instructions -->
    <div style="margin: 20px; font-family: sans-serif">
      <h1>App Screenshot Generator</h1>
      <p>
        Take a screenshot of the above app interface and save it as
        "app-screenshot.png" in the public directory.
      </p>
      <p>
        On macOS: Press Command + Shift + 4, then select the app interface area.
      </p>
      <p>
        On Windows: Use the Snipping Tool or press Windows + Shift + S to
        capture the interface.
      </p>
    </div>
  </body>
</html>
</file>

<file path="public/README.md">
# App Screenshot Instructions

This directory contains a file `create-screenshot.html` that can be used to generate a screenshot for the landing page.

## How to generate the screenshot:

1. Open the `create-screenshot.html` file in a web browser
2. Take a screenshot of the application interface (not including the instructions)
3. Save the screenshot as `app-screenshot.png` in this directory (public/)

## Taking a screenshot on different operating systems:

- **macOS**: Press `Command + Shift + 4`, then select the area you want to capture
- **Windows**: Use the Snipping Tool or press `Windows + Shift + S` to capture a specific area
- **Linux**: Use a tool like GNOME Screenshot or press `PrtScn` key

The screenshot will be used on the landing page to show visitors what the application looks like.
</file>

<file path=".gitignore">
# dependencies
/node_modules
/.pnp
.pnp.js

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# local env files
.env*.local

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

# IDE
.idea/
.vscode/
*.swp
*.swo
</file>

<file path="middleware.ts">
import { createServerClient, type CookieOptions } from "@supabase/ssr";
import { NextResponse, type NextRequest } from "next/server";

export async function middleware(request: NextRequest) {
  let response = NextResponse.next({
    request: {
      headers: request.headers,
    },
  });

  const supabase = createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        get(name: string) {
          return request.cookies.get(name)?.value;
        },
        set(name: string, value: string, options: CookieOptions) {
          request.cookies.set({
            name,
            value,
            ...options,
          });
          response = NextResponse.next({
            request: {
              headers: request.headers,
            },
          });
          response.cookies.set({
            name,
            value,
            ...options,
          });
        },
        remove(name: string, options: CookieOptions) {
          request.cookies.set({
            name,
            value: "",
            ...options,
          });
          response = NextResponse.next({
            request: {
              headers: request.headers,
            },
          });
          response.cookies.set({
            name,
            value: "",
            ...options,
          });
        },
      },
    }
  );

  const {
    data: { user },
    error,
  } = await supabase.auth.getUser();

  if (error && error.message !== "Auth session missing!") {
    console.error("[Middleware] Error getting user:", error.message);
  }

  // Protect the /dashboard route
  if (!user && request.nextUrl.pathname.startsWith("/dashboard")) {
    return NextResponse.redirect(new URL("/", request.url));
  }

  // Redirect authenticated users from the landing page to the dashboard
  if (user && request.nextUrl.pathname === "/") {
    return NextResponse.redirect(new URL("/dashboard", request.url));
  }

  return response;
}

export const config = {
  matcher: [
    /*
     * Match all request paths except for the ones starting with:
     * - _next/static (static files)
     * - _next/image (image optimization files)
     * - favicon.ico (favicon file)
     * Feel free to modify this pattern to include more paths.
     */
    "/((?!_next/static|_next/image|favicon.ico|.*\\.(?:svg|png|jpg|jpeg|gif|webp)$).*)",
  ],
};
</file>

<file path="postcss.config.js">
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};
</file>

<file path="supabaseplan.md">
# Supabase Google Authentication Implementation Plan

## 1. Set up Supabase project

- [x] Create a new Supabase project (Used existing: PodAI, ID: wjkqshfnsqkmhmqsiaol)
- [ ] Configure database schema for user authentication (Using default `auth.users` for now)
- [ ] Set up Row Level Security (RLS) policies (Will implement when user-specific tables are added)

## 2. Configure Google OAuth credentials

- [x] Create a Google Cloud project
- [x] Set up OAuth consent screen
- [x] Generate OAuth client ID and secret
- [x] Add authorized redirect URIs for your application (`https://wjkqshfnsqkmhmqsiaol.supabase.co/auth/v1/callback`)

## 3. Configure Supabase Auth with Google provider

- [x] Add Google OAuth credentials to Supabase Auth settings
- [x] Configure Supabase redirect URLs (Verified match)

## 4. Implement frontend authentication flow

- [x] Install Supabase client library (`@supabase/ssr` and `@supabase/supabase-js`)
- [x] Create authentication components (Updated `Navbar.tsx` & `app/page.tsx` with state, sign-in/out)
- [x] Implement sign-in, sign-out, and session management (Done via `Navbar.tsx`, `app/page.tsx` and `middleware.ts`)
- [x] Set up protected routes for authenticated users (Done via `middleware.ts`)

## 5. Update application to use authentication context

- [x] Modify existing components to respect authentication state (`app/page.tsx`, `middleware.ts`)
- [x] Ensure dashboard is only accessible to authenticated users (Done via `middleware.ts`)
- [x] Redirect unauthenticated users to landing page (Done via `middleware.ts`)

## 6. Test authentication flow

- [x] Verify sign-in process works correctly
- [x] Test session persistence
- [x] Ensure protected routes are properly secured
</file>

<file path="app/api/podcast-metadata/route.ts">
import { NextRequest, NextResponse } from "next/server";
import { fetchMetadataFromYouTubeAPI } from "@/lib/youtube"; // Import the shared function
import { PodcastMetadata } from "@/components/PodcastMetadata"; // Use correct import path

// Define a simple type for oEmbed response
interface OEmbedResponse {
  title?: string;
  author_name?: string;
  thumbnail_url?: string;
  // Add other fields if needed (e.g., width, height for thumbnail)
}

// Simple oEmbed fetch function - returns only basic fields matching PodcastMetadata structure
async function fetchOEmbedMetadata(
  videoId: string
): Promise<Partial<PodcastMetadata> | null> {
  const url = `https://www.youtube.com/oembed?url=https://www.youtube.com/watch?v=${videoId}&format=json`;
  try {
    console.log(`Attempting oEmbed fallback for video ID: ${videoId}`);
    const response = await fetch(url, {
      next: { revalidate: 3600 },
      signal: AbortSignal.timeout(8000),
    }); // Cache & 8s timeout
    if (!response.ok) {
      console.error(
        `oEmbed request failed with status ${response.status} ${response.statusText}`
      );
      return null;
    }
    const data: OEmbedResponse = await response.json();

    // Map oEmbed response to PodcastMetadata structure
    const metadata: Partial<PodcastMetadata> = {
      videoId: videoId,
      title: data.title || "YouTube Video (oEmbed)",
      channelName: data.author_name || "Unknown Channel (oEmbed)",
      // Construct a basic thumbnail object if URL exists
      thumbnails: data.thumbnail_url
        ? { default: { url: data.thumbnail_url, width: 0, height: 0 } }
        : null,
      duration: "0:00", // oEmbed doesn't provide duration
      description: "Description unavailable via oEmbed.",
      fullDescription: "Description unavailable via oEmbed.",
      descriptionTruncated: false,
      // Other fields like viewCount, likeCount, publishedAt are not available via oEmbed
    };
    console.log(
      `Successfully fetched partial metadata via oEmbed fallback for video ID: ${videoId}`
    );
    return metadata;
  } catch (error: any) {
    if (error.name === "AbortError") {
      console.error(`oEmbed request timed out for video ID ${videoId}.`);
    } else {
      console.error(
        `Error fetching oEmbed metadata for video ID ${videoId}:`,
        error.message || error
      );
    }
    return null;
  }
}

export async function POST(request: NextRequest) {
  let videoId = ""; // For logging scope
  try {
    // includeFull is no longer needed as API function gets full description anyway
    const { url } = await request.json();

    // Extract video ID from URL
    const videoIdMatch = url.match(
      /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
    );

    if (!videoIdMatch) {
      return NextResponse.json(
        { error: "Could not extract video ID from URL" },
        { status: 400 }
      );
    }

    videoId = videoIdMatch[1];
    console.log(`[${videoId}] Received request for metadata.`);

    // Attempt to fetch metadata using the shared YouTube API function
    let metadata = await fetchMetadataFromYouTubeAPI(videoId);

    // If API fetch fails or returns null, attempt oEmbed fallback
    if (!metadata) {
      console.warn(
        `[${videoId}] YouTube API metadata fetch failed or returned null, attempting oEmbed fallback.`
      );
      metadata = await fetchOEmbedMetadata(videoId);
    }

    // If both methods fail, return an error
    if (!metadata) {
      console.error(`[${videoId}] All methods failed to fetch metadata.`);
      // Return a more specific error message
      return NextResponse.json(
        {
          error: `Failed to fetch podcast metadata for video ${videoId} using all available methods.`,
        },
        { status: 500 }
      );
    }

    // Return the successful metadata (either from API or oEmbed)
    console.log(`[${videoId}] Successfully returning metadata.`);
    // Ensure the returned object matches the expected structure (even if partial)
    return NextResponse.json({ metadata: metadata as PodcastMetadata }); // Cast to full type if confident, otherwise handle partial data downstream
  } catch (error: any) {
    // Catch potential errors during request parsing or ID extraction
    const idSuffix = videoId ? ` for video ${videoId}` : "";
    console.error(
      `Error processing podcast metadata request${idSuffix}:`,
      error.message || error
    );
    return NextResponse.json(
      { error: `Internal server error processing metadata request${idSuffix}` },
      { status: 500 }
    );
  }
}
</file>

<file path="app/layout.tsx">
import type { Metadata } from "next";
import { Poppins } from "next/font/google";
import "./globals.css";
import ErrorBoundary from "../components/ErrorBoundary";

const poppins = Poppins({
  weight: ["400", "500", "600", "700"],
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "YouTube AI Podcast Assistant",
  description: "Summarize and chat with YouTube podcasts using AI",
  keywords: "YouTube, podcast, AI, summarizer, chat, assistant, transcript",
  authors: [{ name: "Your Name" }],
  viewport: "width=device-width, initial-scale=1",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" suppressHydrationWarning>
      <body className={poppins.className}>
        <ErrorBoundary>{children}</ErrorBoundary>
      </body>
    </html>
  );
}
</file>

<file path="app/page.tsx">
"use client";

import React, { useEffect, useState } from "react";
import { useRouter } from "next/navigation";
import Link from "next/link";
import Image from "next/image";
import { createClient } from "@/lib/supabase/client";

// NoSSR wrapper component to prevent hydration mismatches
function NoSSR({ children }: { children: React.ReactNode }) {
  const [mounted, setMounted] = useState(false);

  useEffect(() => {
    setMounted(true);
  }, []);

  return mounted ? <>{children}</> : null;
}

export default function LandingPage() {
  const router = useRouter();
  const [mounted, setMounted] = useState(false);
  const [loading, setLoading] = useState(false);
  const supabase = createClient();

  useEffect(() => {
    setMounted(true);
  }, []);

  const handleNavigation = (path: string) => {
    if (mounted) {
      router.push(path);
    }
  };

  const handleSignIn = async () => {
    setLoading(true);
    const { error } = await supabase.auth.signInWithOAuth({
      provider: "google",
      options: {
        redirectTo: `${window.location.origin}/auth/callback`,
      },
    });
    if (error) {
      console.error("Error signing in:", error.message);
    }
  };

  return (
    <div className="min-h-screen bg-[#f7f7ff]">
      {/* Navigation */}
      <nav className="bg-white shadow-sm">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
          <div className="flex justify-between h-16">
            <div className="flex items-center">
              <div className="text-purple-700 font-bold text-xl">
                YouTube AI Podcast Assistant
              </div>
            </div>
            <div className="flex items-center space-x-4">
              <button
                onClick={handleSignIn}
                disabled={loading}
                className="px-4 py-2 bg-purple-600 text-white rounded-lg hover:bg-purple-700 disabled:opacity-50"
              >
                {loading ? "Signing In..." : "Sign In"}
              </button>
            </div>
          </div>
        </div>
      </nav>

      {/* Hero Section - Two Column Layout */}
      <div className="max-w-7xl mx-auto px-4 pt-16 pb-12 sm:pt-24 sm:pb-20">
        <div className="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
          {/* Left Column - Text Content */}
          <div className="text-left">
            <h1 className="text-4xl md:text-5xl lg:text-6xl font-bold text-purple-700 mb-6">
              Transform Podcasts into Interactive Conversations
            </h1>
            <p className="text-lg text-gray-600 mb-10">
              Paste any podcast URL and let AI create summaries and engage in
              meaningful conversations about the content
            </p>
            <div className="flex flex-col sm:flex-row gap-4 sm:gap-6">
              <button
                onClick={handleSignIn}
                disabled={loading}
                className="px-8 py-3 bg-purple-600 text-white rounded-lg hover:bg-purple-700 text-lg font-medium disabled:opacity-50"
              >
                {loading ? "Redirecting..." : "Get Started"}
              </button>
              <a
                href="#how-it-works"
                className="px-8 py-3 bg-purple-100 text-purple-700 rounded-lg hover:bg-purple-200 text-lg font-medium text-center"
              >
                Learn More
              </a>
            </div>
          </div>

          {/* Right Column - Screenshot */}
          <div className="relative flex justify-center md:justify-end">
            <div
              className="relative w-full rounded-xl shadow-2xl overflow-hidden border-8 border-white"
              style={{ maxHeight: "80vh" }}
            >
              <div
                style={{
                  position: "relative",
                  width: "100%",
                  paddingTop: "64.3%",
                }}
              >
                <NoSSR>
                  <Image
                    src="/app-screenshot-new.png"
                    alt="AI chat interface analyzing economic impacts from a podcast"
                    fill
                    sizes="(max-width: 768px) 100vw, 50vw"
                    priority
                    className="object-cover rounded-lg"
                    style={{ objectFit: "cover" }}
                  />
                </NoSSR>
              </div>
            </div>
          </div>
        </div>
      </div>

      {/* Feature Cards Section */}
      <div className="bg-white py-16">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
          <div className="text-center mb-12">
            <h2 className="text-3xl font-bold text-gray-900">Key Features</h2>
            <p className="mt-4 text-lg text-gray-600">
              Everything you need to get more from your podcast listening
              experience
            </p>
          </div>
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-8">
            {/* Smart Summaries Card */}
            <div className="bg-[#f7f7ff] p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M12 6.042A8.967 8.967 0 006 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 016 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 016-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0018 18a8.967 8.967 0 00-6 2.292m0-14.25v14.25" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Smart Summaries
              </h3>
              <p className="text-gray-600">
                Get comprehensive summaries that capture the key points and
                insights from any podcast.
              </p>
            </div>

            {/* Interactive Chat Card */}
            <div className="bg-[#f7f7ff] p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M4.913 2.658c2.075-.27 4.19-.408 6.337-.408 2.147 0 4.262.139 6.337.408 1.922.25 3.291 1.861 3.405 3.727a4.403 4.403 0 00-1.032-.211 50.89 50.89 0 00-8.42 0c-2.358.196-4.04 2.19-4.04 4.434v4.286a4.47 4.47 0 002.433 3.984L7.28 21.53A.75.75 0 016 21v-4.03a48.527 48.527 0 01-1.087-.128C2.905 16.58 1.5 14.833 1.5 12.862V6.638c0-1.97 1.405-3.718 3.413-3.979z" />
                  <path d="M15.75 7.5c-1.376 0-2.739.057-4.086.169C10.124 7.797 9 9.103 9 10.609v4.285c0 1.507 1.128 2.814 2.67 2.94 1.243.102 2.5.157 3.768.165l2.782 2.781a.75.75 0 001.28-.53v-2.39l.33-.026c1.542-.125 2.67-1.433 2.67-2.94v-4.286c0-1.505-1.125-2.811-2.664-2.94A49.392 49.392 0 0015.75 7.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Interactive Chat
              </h3>
              <p className="text-gray-600">
                Ask questions about the podcast content and get detailed answers
                from our AI assistant.
              </p>
            </div>

            {/* Works Everywhere Card */}
            <div className="bg-[#f7f7ff] p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
                  <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Works Everywhere
              </h3>
              <p className="text-gray-600">
                Compatible with podcasts from YouTube and many more platforms.
              </p>
            </div>

            {/* AI-Powered Insights Card */}
            <div className="bg-[#f7f7ff] p-8 rounded-2xl shadow-md flex flex-col items-start h-full">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5">
                <svg
                  xmlns="http://www.w3.org/2000/svg"
                  className="h-8 w-8 text-purple-600"
                  viewBox="0 0 24 24"
                  fill="currentColor"
                >
                  <path d="M21.731 2.269a2.625 2.625 0 00-3.712 0l-1.157 1.157 3.712 3.712 1.157-1.157a2.625 2.625 0 000-3.712zM19.513 8.199l-3.712-3.712-8.4 8.4a5.25 5.25 0 00-1.32 2.214l-.8 2.685a.75.75 0 00.933.933l2.685-.8a5.25 5.25 0 002.214-1.32l8.4-8.4z" />
                  <path d="M5.25 5.25a3 3 0 00-3 3v10.5a3 3 0 003 3h10.5a3 3 0 003-3V13.5a.75.75 0 00-1.5 0v5.25a1.5 1.5 0 01-1.5 1.5H5.25a1.5 1.5 0 01-1.5-1.5V8.25a1.5 1.5 0 011.5-1.5h5.25a.75.75 0 000-1.5H5.25z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                AI-Powered Insights
              </h3>
              <p className="text-gray-600">
                Advanced AI analyzes podcasts to extract the most relevant
                information and context.
              </p>
            </div>
          </div>
        </div>
      </div>

      {/* How It Works Section */}
      <div id="how-it-works" className="py-16">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
          <div className="text-center mb-12">
            <h2 className="text-3xl font-bold text-gray-900">How It Works</h2>
            <p className="mt-4 text-lg text-gray-600">
              Simple process, powerful results
            </p>
          </div>
          <div className="grid grid-cols-1 md:grid-cols-3 gap-8">
            {/* Step 1 */}
            <div className="text-center">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5 mx-auto">
                <span className="text-2xl font-bold text-purple-600">1</span>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Paste YouTube URL
              </h3>
              <p className="text-gray-600">
                Simply paste the URL of any YouTube podcast you want to analyze
              </p>
            </div>

            {/* Step 2 */}
            <div className="text-center">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5 mx-auto">
                <span className="text-2xl font-bold text-purple-600">2</span>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                AI Processes the Content
              </h3>
              <p className="text-gray-600">
                Our advanced AI analyzes the audio transcript to extract key
                information
              </p>
            </div>

            {/* Step 3 */}
            <div className="text-center">
              <div className="h-16 w-16 rounded-full bg-purple-100 flex items-center justify-center mb-5 mx-auto">
                <span className="text-2xl font-bold text-purple-600">3</span>
              </div>
              <h3 className="text-xl font-semibold text-gray-900 mb-3">
                Interact with Results
              </h3>
              <p className="text-gray-600">
                Get a comprehensive summary or chat with the AI about specific
                content
              </p>
            </div>
          </div>
        </div>
      </div>

      {/* CTA Section */}
      <div className="bg-purple-700 py-16">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
          <h2 className="text-3xl font-bold text-white mb-4">
            Ready to Unlock Podcast Insights?
          </h2>
          <p className="text-lg text-purple-200 mb-8">
            Start summarizing and chatting with your favorite podcasts today.
          </p>
          <button
            onClick={handleSignIn}
            disabled={loading}
            className="px-8 py-3 bg-white text-purple-700 rounded-lg hover:bg-gray-100 text-lg font-medium disabled:opacity-50"
          >
            {loading ? "Redirecting..." : "Get Started Now"}
          </button>
        </div>
      </div>

      {/* Footer */}
      <footer className="bg-[#f7f7ff] py-8">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center text-gray-500">
          &copy; {new Date().getFullYear()} YouTube AI Podcast Assistant. All
          rights reserved.
        </div>
      </footer>
    </div>
  );
}
</file>

<file path="components/Chat.tsx">
import React, { useState, useRef, useEffect } from "react";
import ReactMarkdown from "react-markdown";
import remarkGfm from "remark-gfm";
import { toast } from "react-hot-toast";
import { PaperAirplaneIcon } from "@heroicons/react/24/outline";

interface Message {
  role: "user" | "assistant";
  content: string;
  timestamp?: Date;
}

interface ChatProps {
  videoUrl: string;
}

const Chat: React.FC<ChatProps> = ({ videoUrl }) => {
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState<Message[]>([]);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const inputRef = useRef<HTMLInputElement>(null);

  // Scroll to bottom of chat when messages change
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [messages]);

  // Focus input field when component mounts
  useEffect(() => {
    if (!loading && inputRef.current) {
      inputRef.current.focus();
    }
  }, [loading]);

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!input.trim() || loading) return;

    const userMessage: Message = {
      role: "user",
      content: input,
      timestamp: new Date(),
    };
    setMessages((prev) => [...prev, userMessage]);
    setInput("");
    setError(null);
    setLoading(true);

    try {
      const response = await fetch("/api/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          url: videoUrl,
          question: input,
          chatHistory: messages.map(({ role, content }) => ({ role, content })),
        }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || "Failed to get response");
      }

      const assistantMessage: Message = {
        role: "assistant",
        content: data.answer,
        timestamp: new Date(),
      };
      setMessages((prev) => [...prev, assistantMessage]);
    } catch (err) {
      console.error("Error:", err);
      const errorMessage =
        err instanceof Error ? err.message : "Failed to get response";
      setError(errorMessage);
      toast.error(`Error: ${errorMessage}`);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="bg-white rounded-lg flex flex-col h-full">
      <div className="flex-1 flex flex-col">
        <div
          className="flex-1 overflow-y-auto p-4 space-y-4 bg-gray-50"
          style={{ maxHeight: "calc(70vh - 130px)" }}
          aria-live="polite"
        >
          {messages.length === 0 ? (
            <div className="text-center text-gray-500 my-8 bg-white p-6 rounded-lg">
              <p className="font-medium text-gray-700 mb-3">
                Ask any question about this podcast
              </p>
              <p className="text-sm mb-3">For example:</p>
              <div className="grid grid-cols-1 md:grid-cols-2 gap-2">
                {[
                  "What were the main topics discussed?",
                  "Summarize the key points about [specific topic]",
                  "What did the speaker say about [specific concept]?",
                  "What were the most interesting insights shared?",
                ].map((suggestion, index) => (
                  <button
                    key={index}
                    onClick={() => {
                      setInput(suggestion);
                      if (inputRef.current) inputRef.current.focus();
                    }}
                    className="text-sm text-left p-2 bg-purple-50 text-purple-700 rounded-md hover:bg-purple-100 transition-colors"
                  >
                    {suggestion}
                  </button>
                ))}
              </div>
            </div>
          ) : (
            messages.map((message, index) => (
              <div
                key={index}
                className={`flex ${
                  message.role === "user" ? "justify-end" : "justify-start"
                }`}
              >
                <div
                  className={`max-w-[80%] rounded-lg p-4 ${
                    message.role === "user"
                      ? "bg-purple-100 text-purple-900"
                      : "bg-white text-gray-800 border border-gray-100"
                  }`}
                >
                  <div className="prose prose-sm max-w-none">
                    <ReactMarkdown remarkPlugins={[remarkGfm]}>
                      {message.content}
                    </ReactMarkdown>
                  </div>
                  {message.timestamp && (
                    <div className="text-xs text-gray-500 mt-2 text-right">
                      {message.timestamp.toLocaleTimeString([], {
                        hour: "2-digit",
                        minute: "2-digit",
                      })}
                    </div>
                  )}
                </div>
              </div>
            ))
          )}
          {loading && (
            <div className="flex justify-start">
              <div className="max-w-[80%] rounded-lg p-4 bg-white border border-gray-100">
                <div className="flex items-center space-x-2">
                  <div
                    className="w-2 h-2 rounded-full bg-purple-400 animate-bounce"
                    style={{ animationDelay: "0ms" }}
                  ></div>
                  <div
                    className="w-2 h-2 rounded-full bg-purple-500 animate-bounce"
                    style={{ animationDelay: "150ms" }}
                  ></div>
                  <div
                    className="w-2 h-2 rounded-full bg-purple-600 animate-bounce"
                    style={{ animationDelay: "300ms" }}
                  ></div>
                </div>
              </div>
            </div>
          )}
          {error && (
            <div className="text-red-600 text-sm p-3 bg-red-50 rounded border border-red-200">
              Error: {error}
            </div>
          )}
          <div ref={messagesEndRef} />
        </div>
        <form onSubmit={handleSubmit} className="p-4 bg-white">
          <div className="flex items-center">
            <input
              type="text"
              ref={inputRef}
              value={input}
              onChange={(e) => setInput(e.target.value)}
              placeholder="Ask a question about this podcast..."
              className="flex-1 p-3 border border-gray-300 rounded-l-lg focus:outline-none focus:ring-2 focus:ring-purple-500"
              disabled={loading}
              aria-label="Your question"
            />
            <button
              type="submit"
              disabled={loading || !input.trim()}
              className="px-4 py-3 bg-purple-600 text-white rounded-r-lg hover:bg-purple-700 disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center"
              aria-label={loading ? "Sending..." : "Send message"}
            >
              {loading ? (
                <div className="flex items-center space-x-1">
                  <div className="w-1.5 h-1.5 bg-white rounded-full animate-pulse"></div>
                  <div
                    className="w-1.5 h-1.5 bg-white rounded-full animate-pulse"
                    style={{ animationDelay: "150ms" }}
                  ></div>
                  <div
                    className="w-1.5 h-1.5 bg-white rounded-full animate-pulse"
                    style={{ animationDelay: "300ms" }}
                  ></div>
                </div>
              ) : (
                <PaperAirplaneIcon className="h-5 w-5" />
              )}
            </button>
          </div>
        </form>
      </div>
    </div>
  );
};

export default Chat;
</file>

<file path="components/Navbar.tsx">
"use client";

import React, { useState, useEffect } from "react";
import Link from "next/link";
import { usePathname, useRouter } from "next/navigation";
import { createClient } from "@/lib/supabase/client";
import { Session } from "@supabase/supabase-js";

const Navbar = () => {
  const pathname = usePathname();
  const router = useRouter();
  const [session, setSession] = useState<Session | null>(null);
  const [loading, setLoading] = useState(true);
  const supabase = createClient();

  useEffect(() => {
    supabase.auth.getSession().then(({ data: { session } }) => {
      setSession(session);
      setLoading(false);
    });

    const {
      data: { subscription },
    } = supabase.auth.onAuthStateChange((_event, session) => {
      setSession(session);
      setLoading(false);
    });

    return () => subscription?.unsubscribe();
  }, [supabase.auth]);

  const handleSignIn = async () => {
    setLoading(true);
    const { error } = await supabase.auth.signInWithOAuth({
      provider: "google",
      options: {
        redirectTo: `${window.location.origin}/auth/callback`,
      },
    });
    if (error) {
      console.error("Error signing in:", error.message);
      setLoading(false);
    }
  };

  const handleSignOut = async () => {
    setLoading(true);
    const { error } = await supabase.auth.signOut();
    if (error) {
      console.error("Error signing out:", error.message);
    } else {
      setSession(null);
      router.push("/");
    }
    setLoading(false);
  };

  return (
    <nav className="flex items-center justify-between w-full max-w-7xl mx-auto px-4 py-6">
      <Link href="/" className="flex items-center">
        <div className="w-10 h-10 bg-purple-600 rounded-full flex items-center justify-center mr-2">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            viewBox="0 0 24 24"
            fill="white"
            className="w-6 h-6"
          >
            <path d="M8.25 4.5a3.75 3.75 0 117.5 0v8.25a3.75 3.75 0 11-7.5 0V4.5z" />
            <path d="M6 10.5a.75.75 0 01.75.75v1.5a5.25 5.25 0 1010.5 0v-1.5a.75.75 0 011.5 0v1.5a6.751 6.751 0 01-6 6.709v2.291h3a.75.75 0 010 1.5h-7.5a.75.75 0 010-1.5h3v-2.291a6.751 6.751 0 01-6-6.709v-1.5A.75.75 0 016 10.5z" />
          </svg>
        </div>
        <span className="text-2xl font-bold text-purple-600">PodAI</span>
      </Link>
      <div className="flex items-center space-x-4">
        {loading ? (
          <div className="text-gray-500">Loading...</div>
        ) : session ? (
          <>
            {pathname === "/dashboard" ? (
              <Link
                href="/"
                className="text-gray-600 hover:text-purple-600 transition-colors"
              >
                Home
              </Link>
            ) : (
              <Link
                href="/dashboard"
                className="text-gray-600 hover:text-purple-600 transition-colors"
              >
                Dashboard
              </Link>
            )}
            <Link
              href="/validate-transcript"
              className="text-gray-600 hover:text-purple-600 transition-colors"
            >
              Validate Transcript
            </Link>
            <span className="text-sm text-gray-600 hidden sm:inline">
              {session.user.email}
            </span>
            <button
              onClick={handleSignOut}
              className="px-4 py-2 border border-purple-600 text-purple-600 rounded-lg hover:bg-purple-50 transition-colors"
            >
              Sign Out
            </button>
          </>
        ) : (
          <>
            <Link
              href="/validate-transcript"
              className="text-gray-600 hover:text-purple-600 transition-colors mr-4"
            >
              Validate Transcript
            </Link>
            <button
              onClick={handleSignIn}
              className="px-4 py-2 bg-purple-600 text-white rounded-lg hover:bg-purple-700 transition-colors flex items-center space-x-2"
            >
              <span>Sign In with Google</span>
            </button>
          </>
        )}
      </div>
    </nav>
  );
};

export default Navbar;
</file>

<file path="components/Summary.tsx">
import React, { useEffect, useState } from "react";
import ReactMarkdown from "react-markdown";
import remarkGfm from "remark-gfm";
import { toast } from "react-hot-toast";
import { DocumentDuplicateIcon } from "@heroicons/react/24/outline";
import { PodcastMetadata } from "./PodcastMetadata";

interface SummaryProps {
  summary: string;
  videoUrl: string;
}

const Summary: React.FC<SummaryProps> = ({ summary, videoUrl }) => {
  const [metadata, setMetadata] = useState<PodcastMetadata | null>(null);
  const [loading, setLoading] = useState(true);

  // Fetch metadata if it's not already in the summary
  useEffect(() => {
    const fetchMetadata = async () => {
      if (!videoUrl) return;

      try {
        setLoading(true);
        const response = await fetch("/api/podcast-metadata", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify({ url: videoUrl }),
        });

        if (response.ok) {
          const data = await response.json();
          setMetadata(data.metadata);
        }
      } catch (error) {
        console.error("Error fetching metadata:", error);
      } finally {
        setLoading(false);
      }
    };

    fetchMetadata();
  }, [videoUrl]);

  const handleCopy = async () => {
    try {
      // Include metadata in the copied content if available
      let fullContent = "";

      if (metadata) {
        fullContent += `# ${metadata.title}\n`;
        fullContent += `Channel: ${metadata.channelName}\n`;
        fullContent += `Duration: ${metadata.duration}\n`;

        // Add published date if available
        if (metadata.publishedAt) {
          const date = new Date(metadata.publishedAt);
          fullContent += `Published: ${date.toLocaleDateString()}\n`;
        }

        // Add view count if available
        if (metadata.viewCount) {
          fullContent += `Views: ${parseInt(
            metadata.viewCount
          ).toLocaleString()}\n`;
        }

        // Add like count if available
        if (metadata.likeCount) {
          fullContent += `Likes: ${parseInt(
            metadata.likeCount
          ).toLocaleString()}\n`;
        }

        fullContent += "\n";
      }

      fullContent += summary;

      await navigator.clipboard.writeText(fullContent);
      toast.success("Summary copied to clipboard!");
    } catch (error) {
      console.error("Failed to copy:", error);
      toast.error("Failed to copy to clipboard. Please try again.");
    }
  };

  return (
    <div className="bg-white rounded-lg">
      <div className="flex justify-end p-2">
        <button
          onClick={handleCopy}
          className="flex items-center px-3 py-1.5 text-sm bg-gray-100 text-gray-700 rounded-lg hover:bg-gray-200 transition-colors"
          aria-label="Copy summary to clipboard"
        >
          <DocumentDuplicateIcon className="h-4 w-4 mr-1.5" />
          Copy
        </button>
      </div>

      <div className="px-4 pb-4 overflow-y-auto max-h-[70vh]">
        {/* Metadata section has been removed */}

        <div className="w-full max-w-full overflow-hidden markdown-content bg-gray-50 border border-gray-200 rounded-lg p-6 py-0">
          <div className="prose prose-lg !max-w-full !w-full prose-headings:text-purple-700 prose-h1:text-2xl prose-h2:text-xl prose-h2:mt-8 prose-h2:mb-4 prose-h2:pb-2 prose-h2:border-b prose-h2:border-gray-200 prose-h3:text-lg prose-h3:text-gray-700 prose-h3:mt-6 prose-p:text-gray-600 prose-p:my-4 prose-p:leading-relaxed prose-ul:my-4 prose-ol:my-4 prose-li:my-1.5 prose-li:text-gray-600 prose-strong:text-purple-800 prose-strong:font-medium prose-a:text-purple-600 prose-a:no-underline hover:prose-a:underline">
            <ReactMarkdown remarkPlugins={[remarkGfm]}>{summary}</ReactMarkdown>
          </div>
        </div>
      </div>
    </div>
  );
};

export default Summary;
</file>

<file path="DEPLOYMENT_VERCEL.md">
# Deploying YouTube AI Podcast Assistant to Vercel

This document outlines the steps to deploy the application to Vercel.

## 1. Prerequisites

- Ensure your code is pushed to a Git repository (GitHub, GitLab, or Bitbucket).
- Have a Vercel account ([https://vercel.com/signup](https://vercel.com/signup)).

## 2. Vercel Project Setup

1.  Log in to your Vercel dashboard.
2.  Click **Add New...** -> **Project**.
3.  **Import Git Repository**: Select the Git provider where your repository is hosted and import the `youtube-ai-podcast-assistant` repository.
4.  Vercel should automatically detect it as a Next.js project.

## 3. Configure Project Settings

1.  **Framework Preset**: Verify Vercel sets it to "Next.js".
2.  **Build & Development Settings**: The defaults usually work for Next.js (`npm run build`, output directory `.next`). No changes are typically needed.
3.  **Environment Variables**: This is crucial.
    - Navigate to your project's **Settings** -> **Environment Variables**.
    - Add the following variables:
      - `OPENAI_API_KEY`: Enter your OpenAI API key. Mark it as **Secret**.
      - `NEXT_PUBLIC_SUPABASE_URL`: Enter your Supabase project URL.
      - `NEXT_PUBLIC_SUPABASE_ANON_KEY`: Enter your Supabase Anon Key.
      - `YOUTUBE_API_KEY`: Enter your YouTube Data API key. Mark it as **Secret**.
    - _Note: The `NEXT_PUBLIC_` prefixes are required for the Supabase variables to be accessible in the browser.\_

## 4. Deploy

1.  Review the settings.
2.  Click the **Deploy** button.
3.  Vercel will clone the repository, install dependencies (`npm install`), build the project (`npm run build`), and deploy it. Wait for the process to complete.

## 5. Update Supabase & Google Cloud Redirect URIs

_This is a critical post-deployment step._

1.  **Get Deployment URL**: Once Vercel deployment is successful, note your production URL (e.g., `your-project-name.vercel.app`).
2.  **Update Supabase**:
    - Go to your Supabase project dashboard -> **Authentication** -> **URL Configuration**.
    - In the **Redirect URLs** section, add your Vercel production URL followed by `/auth/v1/callback`.
    - Example: `https://your-project-name.vercel.app/auth/v1/callback`
    - Save the changes.
3.  **Update Google Cloud Console**:
    - Go to your Google Cloud project -> **APIs & Services** -> **Credentials**.
    - Find the OAuth 2.0 Client ID you configured for Supabase authentication.
    - Edit the client ID.
    - Under **Authorized redirect URIs**, add the _exact same_ Vercel callback URL as added in Supabase.
    - Example: `https://your-project-name.vercel.app/auth/v1/callback`
    - Save the changes.

_Failure to update these redirect URIs will prevent the Google Sign-In from working on your deployed Vercel application._

## 6. Testing

1.  Access your Vercel deployment URL in a browser.
2.  Verify the landing page loads correctly.
3.  Test the **Sign In** button and complete the Google OAuth flow. You should be redirected to the `/dashboard`.
4.  On the dashboard, enter a valid YouTube podcast URL.
5.  Click **Process Podcast**.
6.  Verify that the metadata, summary, and chat features work as expected.
7.  Test the **Sign Out** functionality.
</file>

<file path="README.md">
# YouTube AI Podcast Assistant

A modern web application that helps users extract insights from YouTube podcasts through AI-powered summarization and interactive chat.

## Features

- **User Authentication**: Secure sign-in via Google OAuth powered by Supabase
- **Podcast Transcription**: Automatically extracts transcripts from YouTube videos
- **YouTube Metadata Extraction**: Fetches video title, channel name, and duration for better context
- **AI-Powered Summarization**: Generates structured summaries of podcast content with:
  - Executive Summary
  - Key Insights with timestamps
  - Detailed Timeline
  - Notable Quotes
  - Related Resources
  - Thought-provoking Questions
- **Interactive Chat**: Ask specific questions about the podcast content and get AI-generated answers
- **Seamless Experience**: Enter a YouTube URL once and switch between summary and chat features
- **Markdown Support**: All AI-generated content is formatted in Markdown for better readability
- **Responsive Design**: Works well on both desktop and mobile devices
- **Error Handling**: Robust error handling for transcript extraction and API responses
- **Landing Page**: Modern landing page for unauthenticated users with sign-in options
- **Protected Dashboard**: Authenticated user workspace with podcast processing tools accessible only after login

## Tech Stack

- **Frontend**: Next.js 14, React, TypeScript, Tailwind CSS
- **Authentication**: Supabase (Auth, Google Provider)
- **UI Components**: React Markdown, Headless UI, Hero Icons, `@supabase/ssr`, `@supabase/supabase-js`
- **Notifications**: React Hot Toast
- **AI Integration**: OpenAI API (using GPT-4o-mini)
- **YouTube Integration**: YouTube Transcript API, YouTube oEmbed API
- **State Management**: React useState/useEffect hooks
- **Build Tools**: TypeScript, PostCSS, Autoprefixer
- **Routing**: App Router with middleware for authenticated/unauthenticated routes

## Getting Started

### Prerequisites

- Node.js 18+ and npm
- OpenAI API key
- Supabase Project: Set up a project on [Supabase](https://supabase.com/)
- Supabase Project URL and Anon Key
- Google Cloud Project: Configured with OAuth 2.0 credentials (Client ID & Secret)
- Authorized Redirect URI in Google Cloud matching your Supabase callback URL (e.g., `YOUR_SUPABASE_URL/auth/v1/callback`)
- Supabase Authentication configured with your Google Client ID and Secret

### Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/youtube-ai-podcast-assistant.git
   cd youtube-ai-podcast-assistant
   ```

2. Install dependencies:

   ```bash
   npm install
   npm install @supabase/ssr @supabase/supabase-js
   ```

3. Create a `.env.local` file in the root directory with your keys:

   ```dotenv
   OPENAI_API_KEY=your_openai_api_key_here
   NEXT_PUBLIC_SUPABASE_URL=your_supabase_project_url
   NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
   ```

4. Start the development server:

   ```bash
   npm run dev
   ```

5. Open [http://localhost:3000](http://localhost:3000) in your browser to see the application.

## User Experience

### Non-authenticated Users

Non-authenticated users will see the landing page at the root URL (`/`) with:

- Navigation bar with "Sign In" button
- Engaging hero section with a visual representation of the chat interface and a "Get Started" button
- Feature highlights showcasing key capabilities
- Step-by-step guide on how the platform works
- Call-to-action buttons ("Get Started", "Sign In") triggering the Google OAuth flow via Supabase

### Authenticated Users

After signing in (via Google OAuth redirect handled by Supabase), users are directed to the protected dashboard (`/dashboard`) where they can:

1. See their email in the Navbar and a "Sign Out" button
2. Enter a YouTube podcast URL in the input field
3. Click "Process Podcast" to extract the content
4. View the AI-generated summary in the Summary tab, including video metadata
5. Switch to the Chat tab to ask specific questions about the podcast content

## How It Works

1. **Authentication (Optional for Landing Page, Required for Dashboard)**:
   - User clicks "Sign In" or "Get Started" on the landing page.
   - App initiates Supabase Google OAuth flow.
   - Supabase handles the redirect to Google and the callback (`/auth/callback`).
   - Session is established via cookies managed by `@supabase/ssr`.
   - Middleware (`middleware.ts`) protects `/dashboard` and redirects users based on auth state.
2. **User Input**: Authenticated user enters a YouTube podcast URL in the dashboard input field
3. **Metadata & Transcript Extraction**:
   - App extracts metadata (title, channel, duration) using YouTube oEmbed API
   - App extracts the transcript from the video using the YouTube Transcript API
4. **AI Processing**:
   - For summaries: Transcript and metadata are sent to OpenAI API with specific prompts
   - For chat: User questions are sent with the transcript context to get relevant answers
5. **Display**: Results are displayed in a clean, user-friendly interface with proper Markdown formatting

## Technical Highlights

- Supabase integration for secure Google OAuth authentication
- Server-side session management using Next.js Middleware and `@supabase/ssr` helpers
- Client-side authentication state handling in components (`Navbar`, `app/page.tsx`)
- Handles large transcripts by truncating them to fit within OpenAI token limits
- Custom prompts to generate well-structured summaries with specific sections
- Metadata enrichment for improved context in AI processing
- Chat history management for contextual conversation
- Accessibility features including proper ARIA labels
- Responsive UI with loading indicators for better user experience
- Error handling for various failure scenarios (invalid URLs, missing transcripts, API failures)

## Project Structure

```
youtube-ai-podcast-assistant/
 app/                  # Next.js app directory
    api/              # API routes
       chat/         # Chat API endpoint
       podcast-metadata/ # Metadata API endpoint
       summarize/    # Summarization API endpoint
    auth/             # Authentication related routes
       callback/     # Supabase OAuth callback handler
          route.ts
       auth-code-error/ # Error page for auth failures
           page.tsx
    dashboard/        # Protected dashboard page (authenticated users)
       page.tsx      # Dashboard component
    globals.css       # Global styles
    layout.tsx        # Root layout
    page.tsx          # Landing page component (unauthenticated users)
 components/           # React components
    Chat.tsx          # Chat interface component
    Summary.tsx       # Summary display component
    PodcastHeader.tsx # Podcast header component
    PodcastMetadata.tsx # Metadata provider component
    Navbar.tsx        # Navigation bar (used in Dashboard, shows auth state)
    ErrorBoundary.tsx # Error handling component
 lib/
    supabase/         # Supabase client utilities
        client.ts     # Browser client
        server.ts     # Server/Middleware client
 public/               # Static assets
    app-screenshot-new.png    # Updated screenshot name
    create-screenshot.html # Tool for generating app screenshots
 .env.local            # Environment variables (API keys, Supabase URL/Key)
 middleware.ts         # Next.js middleware for auth redirects & session refresh
 next.config.js        # Next.js configuration
 package.json          # Project dependencies
 postcss.config.js     # PostCSS configuration
 supabaseplan.md       # Supabase implementation plan (optional)
 tailwind.config.js    # Tailwind CSS configuration
 tsconfig.json         # TypeScript configuration
```

## Landing Page Features

The landing page includes:

- Navigation bar with "Sign In" button
- Hero section with two-column layout (text and app screenshot)
- Feature cards highlighting key capabilities
- "How It Works" section explaining the user flow
- Call-to-action section for conversion
- Responsive design that works well on all devices

## Dashboard Features

The dashboard includes:

- YouTube URL input form
- Tabbed interface for Summary and Chat views
- Podcast metadata display with video thumbnail
- Interactive chat interface
- AI-generated summary with structured sections
- Responsive layout for different screen sizes

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- OpenAI for providing the AI capabilities
- Next.js team for the amazing framework
- All open-source libraries used in this project
</file>

<file path="tailwind.config.js">
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    "./pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      backgroundImage: {
        "gradient-radial": "radial-gradient(var(--tw-gradient-stops))",
        "gradient-conic":
          "conic-gradient(from 180deg at 50% 50%, var(--tw-gradient-stops))",
      },
    },
  },
  plugins: [require("@tailwindcss/aspect-ratio")],
};
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": false,
    "noEmit": true,
    "incremental": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "plugins": [
      {
        "name": "next"
      }
    ],
    "baseUrl": ".",
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": ["next-env.d.ts", ".next/types/**/*.ts", "**/*.ts", "**/*.tsx"],
  "exclude": ["node_modules"]
}
</file>

<file path="package.json">
{
  "name": "youtube-ai-transcriber",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@headlessui/react": "^1.7.18",
    "@heroicons/react": "^2.1.1",
    "@supabase/ssr": "^0.6.1",
    "@supabase/supabase-js": "^2.49.4",
    "@tailwindcss/aspect-ratio": "^0.4.2",
    "axios": "^1.6.7",
    "next": "14.2.26",
    "next-auth": "^4.24.5",
    "node-fetch": "^2.7.0",
    "openai": "^4.28.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-hot-toast": "^2.4.1",
    "react-markdown": "^10.0.0",
    "remark-gfm": "^4.0.1",
    "youtube-transcript": "^1.0.6"
  },
  "devDependencies": {
    "@types/node": "^20.11.19",
    "@types/node-fetch": "^2.6.12",
    "@types/react": "^18.2.57",
    "@types/react-dom": "^18.2.19",
    "autoprefixer": "^10.4.17",
    "eslint": "^8.56.0",
    "eslint-config-next": "14.1.0",
    "postcss": "^8.4.35",
    "tailwindcss": "^3.4.1",
    "typescript": "^5.3.3"
  }
}
</file>

<file path="app/api/chat/route.ts">
import { NextResponse } from "next/server";
import { YoutubeTranscript } from "youtube-transcript";
import OpenAI from "openai";

// Check if OpenAI API key is set
if (!process.env.OPENAI_API_KEY) {
  console.error("OPENAI_API_KEY is not set in environment variables");
  throw new Error("OPENAI_API_KEY is not set in environment variables");
}

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Custom function to fetch YouTube transcript directly
// This serves as a fallback if the youtube-transcript library fails
async function fetchYouTubeTranscriptDirectly(videoId: string) {
  try {
    console.log(`[${videoId}] Attempting direct transcript fetch for chat...`);

    // First, fetch the video page to extract necessary tokens
    const videoPageResponse = await fetch(
      `https://www.youtube.com/watch?v=${videoId}`,
      {
        headers: {
          "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
          "Accept-Language": "en-US,en;q=0.9",
          Accept:
            "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        },
      }
    );

    if (!videoPageResponse.ok) {
      throw new Error(
        `Failed to fetch video page: ${videoPageResponse.status}`
      );
    }

    const videoPageContent = await videoPageResponse.text();

    // Extract captions data from the page
    const captionsMatch = videoPageContent.match(
      /"captions":(.*?),"videoDetails"/
    );
    if (!captionsMatch || !captionsMatch[1]) {
      throw new Error("No captions data found in video page");
    }

    // Parse captions data
    let captionsData;
    try {
      // Clean up the JSON string before parsing
      const cleanedJson = captionsMatch[1]
        .replace(/\\"/g, '"')
        .replace(/\\\\/g, "\\");
      captionsData = JSON.parse(cleanedJson);
    } catch (e: any) {
      throw new Error(`Failed to parse captions data: ${e.message}`);
    }

    // Check if captions are available
    if (!captionsData.playerCaptionsTracklistRenderer) {
      throw new Error("Transcript is disabled on this video");
    }

    if (!captionsData.playerCaptionsTracklistRenderer.captionTracks) {
      throw new Error("No transcript tracks available");
    }

    // Get the first available transcript URL (usually English if available)
    const transcriptUrl =
      captionsData.playerCaptionsTracklistRenderer.captionTracks[0].baseUrl;

    // Fetch the transcript XML
    const transcriptResponse = await fetch(transcriptUrl, {
      headers: {
        "User-Agent":
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
      },
    });

    if (!transcriptResponse.ok) {
      throw new Error(
        `Failed to fetch transcript data: ${transcriptResponse.status}`
      );
    }

    const transcriptXml = await transcriptResponse.text();

    // Parse the XML to extract transcript
    const regex = /<text start="([^"]*)" dur="([^"]*)">([^<]*)<\/text>/g;
    let matches = [];
    let match;
    while ((match = regex.exec(transcriptXml)) !== null) {
      matches.push(match);
    }

    if (matches.length === 0) {
      throw new Error("Failed to parse transcript XML");
    }

    // Convert to transcript format
    const transcript = matches.map((match) => ({
      text: match[3]
        .replace(/&amp;/g, "&")
        .replace(/&lt;/g, "<")
        .replace(/&gt;/g, ">")
        .replace(/&#39;/g, "'")
        .replace(/&quot;/g, '"'),
      duration: parseFloat(match[2]),
      offset: parseFloat(match[1]),
    }));

    console.log(
      `[${videoId}] Successfully fetched transcript directly for chat: ${transcript.length} segments`
    );
    return transcript;
  } catch (error) {
    console.error(
      `[${videoId}] Direct transcript fetch for chat failed:`,
      error
    );
    throw error;
  }
}

export async function POST(request: Request) {
  try {
    // Parse request body
    const body = await request.json();
    const { url, question, chatHistory = [] } = body;

    console.log("Processing chat for URL:", url);
    console.log("Question:", question);

    if (!url) {
      console.error("No URL provided");
      return NextResponse.json({ error: "No URL provided" }, { status: 400 });
    }

    if (!question) {
      console.error("No question provided");
      return NextResponse.json(
        { error: "No question provided" },
        { status: 400 }
      );
    }

    // Extract video ID from URL
    const videoId = url.match(
      /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
    )?.[1];

    if (!videoId) {
      console.error("Invalid YouTube URL:", url);
      return NextResponse.json(
        { error: "Invalid YouTube URL" },
        { status: 400 }
      );
    }

    console.log("Extracted video ID:", videoId);

    // Get transcript
    try {
      console.log(`[${videoId}] Attempting to fetch transcript for chat...`);

      let transcript;

      // First try with the standard library
      try {
        transcript = await YoutubeTranscript.fetchTranscript(videoId);
        console.log(
          `[${videoId}] Standard library successfully fetched transcript for chat: ${transcript?.length} segments`
        );
      } catch (standardError) {
        console.error(
          `[${videoId}] Standard library transcript fetch failed for chat:`,
          standardError
        );
        console.log(
          `[${videoId}] Attempting fallback transcript fetch method for chat...`
        );

        // Try with our custom direct fetch method
        transcript = await fetchYouTubeTranscriptDirectly(videoId);
      }

      console.log(
        `[${videoId}] Raw transcript response received for chat. Length: ${transcript?.length}`
      );

      if (!transcript || transcript.length === 0) {
        console.error(
          `[${videoId}] No transcript data found in the response for chat.`
        );
        return NextResponse.json(
          { error: "No transcript found for this video" },
          { status: 404 }
        );
      }

      const transcriptText = transcript.map((item) => item.text).join(" ");
      console.log(
        `[${videoId}] Transcript processed for chat. Text length: ${transcriptText.length}`
      );

      if (!transcriptText || transcriptText.trim() === "") {
        console.error(
          `[${videoId}] Processed transcript text is empty for chat.`
        );
        return NextResponse.json(
          { error: "Empty transcript found for this video" },
          { status: 404 }
        );
      }

      // Truncate transcript if it's too long (OpenAI has token limits)
      const maxChars = 42000; // Approximately 12000 tokens
      const truncatedText =
        transcriptText.length > maxChars
          ? transcriptText.slice(0, maxChars) + "..."
          : transcriptText;

      console.log(
        `[${videoId}] Truncated transcript text length for chat: ${truncatedText.length}`
      );

      // Prepare chat history for the API
      const messages = [
        {
          role: "system",
          content: `You're a friendly podcast assistant who helps users understand podcast content better. You have access to the transcript of a YouTube podcast. Answer questions about the podcast in a conversational, helpful way. 

When answering:
- Be specific and reference the content directly
- If you can identify timestamps for relevant parts, include them
- If the question asks about something not covered in the podcast, politely explain that it wasn't discussed
- Keep your tone casual and friendly, like you're chatting with a friend
- If appropriate, mention related topics that were discussed in the podcast that might interest the user

Here's the podcast transcript: ${truncatedText}`,
        },
        ...chatHistory,
        {
          role: "user",
          content: question,
        },
      ];

      // Generate answer using OpenAI
      try {
        console.log("Calling OpenAI API for chat...");
        const completion = await openai.chat.completions.create({
          messages,
          model: "gpt-4o-mini",
          max_tokens: 1024,
          temperature: 0.7,
        });

        console.log("OpenAI API response received");
        const answer = completion.choices[0].message.content;

        if (!answer) {
          console.error("No answer generated by OpenAI");
          throw new Error("No answer generated by OpenAI");
        }

        console.log("Answer length:", answer.length, "characters");
        return NextResponse.json({
          answer,
          chatHistory: [
            ...chatHistory,
            { role: "user", content: question },
            { role: "assistant", content: answer },
          ],
        });
      } catch (openaiError: any) {
        console.error("OpenAI API Error:", {
          message: openaiError.message,
          type: openaiError.type,
          stack: openaiError.stack,
          response: openaiError.response?.data,
        });
        return NextResponse.json(
          {
            error: `Failed to generate answer using AI: ${openaiError.message}`,
          },
          { status: 500 }
        );
      }
    } catch (transcriptError: any) {
      console.error(`[${videoId}] Transcript Fetching Error (Chat):`, {
        message: transcriptError.message,
        name: transcriptError.name,
        // Consider logging more properties if available, e.g., error code
        stack: transcriptError.stack,
      });
      return NextResponse.json(
        {
          error: `Failed to fetch video transcript: ${transcriptError.message}`,
        },
        { status: 404 }
      );
    }
  } catch (error: any) {
    console.error("General Error:", {
      message: error.message,
      stack: error.stack,
    });
    return NextResponse.json(
      { error: `Failed to process request: ${error.message}` },
      { status: 500 }
    );
  }
}
</file>

<file path="lib/youtube-transcript.ts">
interface TranscriptLine {
  text: string;
  duration: number;
  offset: number;
}

// Helper to parse HH:MM:SS.ms format to seconds
function parseTimestamp(timestamp: string): number {
  const parts = timestamp.split(":");
  let seconds = 0;
  if (parts.length === 3) {
    seconds += parseFloat(parts[0]) * 3600;
    seconds += parseFloat(parts[1]) * 60;
    seconds += parseFloat(parts[2]);
  } else if (parts.length === 2) {
    seconds += parseFloat(parts[0]) * 60;
    seconds += parseFloat(parts[1]);
  } else if (parts.length === 1) {
    seconds += parseFloat(parts[0]);
  }
  return isNaN(seconds) ? 0 : seconds; // Return 0 if parsing fails
}

// Enhanced headers to better mimic a real browser
const getBrowserLikeHeaders = () => {
  return {
    "User-Agent":
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
    Accept:
      "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Sec-Fetch-Site": "same-origin",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-User": "?1",
    "Sec-Fetch-Dest": "document",
    "sec-ch-ua":
      '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"macOS"',
    "Upgrade-Insecure-Requests": "1",
    Referer: "https://www.youtube.com/",
    "Cache-Control": "max-age=0",
  };
};

// Storage for cookies and session data between requests
let cookieJar = "";
let consentToken = "";

// Simple cache for HTML content by videoId
const htmlCache: Record<string, { html: string; timestamp: number }> = {};
const CACHE_TTL = 60 * 1000; // 1 minute cache

// Helper function to establish YouTube session and get initial cookies
async function establishYouTubeSession(): Promise<boolean> {
  try {
    // Reset cookie jar for a fresh session
    cookieJar = "";

    // Initial touch to get YouTube cookies and possibly consent page
    const initialResponse = await fetch("https://www.youtube.com/", {
      headers: getBrowserLikeHeaders(),
      signal: AbortSignal.timeout(10000),
      redirect: "follow",
    });

    // Save cookies from initial request
    const setCookieHeader = initialResponse.headers.get("set-cookie");
    if (setCookieHeader) {
      cookieJar = setCookieHeader;

      // Look for and process consent tokens if available
      const html = await initialResponse.text();
      const consentMatch = html.match(/consent.youtube.com\/[^"]+/);
      if (consentMatch) {
        const consentUrl = `https://${consentMatch[0]}`;
        await processConsentPage(consentUrl);
      }

      return true;
    }
    return false;
  } catch (error) {
    console.warn(`Failed to establish YouTube session: ${error}`);
    return false;
  }
}

// Helper function to handle YouTube consent pages
async function processConsentPage(consentUrl: string): Promise<void> {
  try {
    // First fetch the consent page
    const consentPageResponse = await fetch(consentUrl, {
      headers: {
        ...getBrowserLikeHeaders(),
        ...(cookieJar ? { Cookie: cookieJar } : {}),
      },
      redirect: "follow",
    });

    // Update cookies
    const consentCookies = consentPageResponse.headers.get("set-cookie");
    if (consentCookies) {
      cookieJar = consentCookies;
    }

    // Parse the consent page
    const consentHtml = await consentPageResponse.text();

    // Find the form data needed for consent
    const formMatch = consentHtml.match(/<form[^>]*>[\s\S]*?<\/form>/i);
    if (formMatch) {
      // Extract form action URL
      const actionMatch = formMatch[0].match(/action="([^"]+)"/);
      const formAction = actionMatch ? actionMatch[1] : "";

      // Extract hidden fields - fix for TypeScript compatibility
      const hiddenFieldRegex =
        /<input[^>]*type="hidden"[^>]*name="([^"]+)"[^>]*value="([^"]*)"[^>]*>/g;
      const formData = new URLSearchParams();

      // Use a regular RegExp.exec approach instead of matchAll
      let hiddenMatch;
      while ((hiddenMatch = hiddenFieldRegex.exec(formMatch[0])) !== null) {
        formData.append(hiddenMatch[1], hiddenMatch[2]);
      }

      // Add consent selection (agree to all)
      formData.append("consent_submitted", "true");
      formData.append("continue", "https://www.youtube.com/");
      formData.append("bl", "boq_identityfrontenduiserver_20231128.03_p0");
      formData.append("hl", "en");
      formData.append("consent_ack", "yes");
      formData.append("consent_hl", "en");
      formData.append("consent_gac", "1");

      // Submit the consent form
      const submitResponse = await fetch(formAction || consentUrl, {
        method: "POST",
        headers: {
          ...getBrowserLikeHeaders(),
          "Content-Type": "application/x-www-form-urlencoded",
          ...(cookieJar ? { Cookie: cookieJar } : {}),
        },
        body: formData.toString(),
        redirect: "follow",
      });

      // Update cookies once more
      const submitCookies = submitResponse.headers.get("set-cookie");
      if (submitCookies) {
        cookieJar = submitCookies;
        console.log("Processed YouTube consent page successfully");
      }
    }
  } catch (error) {
    console.warn(`Failed to process consent page: ${error}`);
  }
}

// Extracts caption data from HTML, finds URL, fetches, and parses transcript
async function extractAndParseTranscriptFromHtml(
  html: string,
  videoId: string
): Promise<TranscriptLine[] | null> {
  console.log(`[${videoId}] Extracting transcript data from HTML...`);
  let rawCaptionsData = null;
  const patterns = [
    /"captions":\s*(\{.*?"captionTracks":.*?\}),\s*"videoDetails"/,
    /"captionTracks":\s*(\[.*?\])/,
    /"baseUrl":"(https:\/\/www\.youtube\.com\/api\/timedtext.*?)"/,
  ];

  for (const pattern of patterns) {
    const match = html.match(pattern);
    if (match && match[1]) {
      const matchedData = match[1].replace(/\\"/g, '"').replace(/\\\\/g, "\\");
      console.log(
        `[${videoId}] Found potential captions data using pattern: ${pattern.source.substring(
          0,
          30
        )}...`
      );
      if (pattern.source.includes("baseUrl")) {
        const decodedUrl = decodeURIComponent(JSON.parse(`"${matchedData}"`));
        rawCaptionsData = JSON.stringify({
          playerCaptionsTracklistRenderer: {
            captionTracks: [{ baseUrl: decodedUrl }],
          },
        });
        console.log(`[${videoId}] Extracted direct transcript URL.`);
      } else {
        try {
          const parsed = JSON.parse(matchedData);
          if (
            parsed &&
            (parsed.captionTracks ||
              (parsed.playerCaptionsTracklistRenderer &&
                parsed.playerCaptionsTracklistRenderer.captionTracks))
          ) {
            if (
              parsed.captionTracks &&
              !parsed.playerCaptionsTracklistRenderer
            ) {
              rawCaptionsData = JSON.stringify({
                playerCaptionsTracklistRenderer: {
                  captionTracks: parsed.captionTracks,
                },
              });
            } else {
              rawCaptionsData = JSON.stringify(parsed);
            }
            console.log(`[${videoId}] Successfully parsed extracted JSON.`);
          } else {
            console.warn(`[${videoId}] Parsed JSON has unexpected structure.`);
          }
        } catch (parseError) {
          console.warn(
            `[${videoId}] Failed to parse extracted data for pattern ${pattern.source.substring(
              0,
              30
            )}...: ${parseError}`
          );
        }
      }
      if (rawCaptionsData) break;
    }
  }

  if (!rawCaptionsData) {
    console.error(
      `[${videoId}] No captions data found in video page using any pattern.`
    );
    return null;
  }

  let captionsData;
  try {
    captionsData = JSON.parse(rawCaptionsData);
  } catch (e: any) {
    console.error(
      `[${videoId}] Failed to parse final captions JSON: ${
        e.message
      }. Raw Data: ${rawCaptionsData.substring(0, 200)}...`
    );
    throw new Error(`Failed to parse final captions data: ${e.message}`);
  }

  if (
    !captionsData?.playerCaptionsTracklistRenderer?.captionTracks ||
    captionsData.playerCaptionsTracklistRenderer.captionTracks.length === 0
  ) {
    console.warn(
      `[${videoId}] Parsed captions data lacks track information or is empty.`
    );
    return null;
  }

  let transcriptUrl = "";
  const tracks = captionsData.playerCaptionsTracklistRenderer.captionTracks;
  const englishTrack = tracks.find((track: any) => track.languageCode === "en");
  transcriptUrl = englishTrack?.baseUrl || tracks[0]?.baseUrl;

  if (!transcriptUrl) {
    console.error(
      `[${videoId}] Could not find a valid baseUrl in caption tracks.`
    );
    return null;
  }
  console.log(
    `[${videoId}] Using transcript URL: ${transcriptUrl.substring(0, 60)}...`
  );

  // Fetch the transcript XML/TTML
  const transcriptResponse = await fetch(transcriptUrl, {
    headers: {
      ...getBrowserLikeHeaders(),
      ...(cookieJar ? { Cookie: cookieJar } : {}),
    },
    signal: AbortSignal.timeout(10000),
  });

  // Save cookies for future requests
  const setCookieHeader = transcriptResponse.headers.get("set-cookie");
  if (setCookieHeader) {
    cookieJar = setCookieHeader;
  }

  if (!transcriptResponse.ok) {
    const errorText = await transcriptResponse.text();
    console.error(
      `[${videoId}] Failed to fetch transcript content: ${
        transcriptResponse.status
      } ${transcriptResponse.statusText}. Response: ${errorText.substring(
        0,
        200
      )}`
    );
    throw new Error(
      `Failed to fetch transcript data: ${transcriptResponse.status} ${transcriptResponse.statusText}`
    );
  }

  const transcriptContent = await transcriptResponse.text();
  if (!transcriptContent || transcriptContent.length < 50) {
    console.error(`[${videoId}] Invalid or empty transcript content received.`);
    return null;
  }
  if (
    !transcriptContent.includes("<text") &&
    !transcriptContent.includes("<p")
  ) {
    console.warn(
      `[${videoId}] Transcript content might be invalid (missing common tags): ${transcriptContent.substring(
        0,
        200
      )}...`
    );
  }

  // Parse the XML/TTML to extract transcript segments
  const transcript: TranscriptLine[] = [];
  const xmlRegex = /<text start="([^"]*)" dur="([^"]*)">([^<]*)<\/text>/g;
  const ttmlRegex = /<p begin="([^"]*)" end="([^"]*)"[^>]*>([^<]*)<\/p>/g;
  let match;

  while ((match = xmlRegex.exec(transcriptContent)) !== null) {
    const offset = parseFloat(match[1]);
    const duration = parseFloat(match[2]);
    if (!isNaN(offset) && !isNaN(duration)) {
      transcript.push({
        text: match[3]
          .replace(/&amp;/g, "&")
          .replace(/&lt;/g, "<")
          .replace(/&gt;/g, ">")
          .replace(/&#39;/g, "'")
          .replace(/&quot;/g, '"')
          .replace(/\\n/g, " ")
          .trim(),
        duration: duration,
        offset: offset,
      });
    } else {
      console.warn(
        `[${videoId}] Skipping segment due to invalid number format (XML): start='${match[1]}', dur='${match[2]}'`
      );
    }
  }

  if (transcript.length === 0) {
    console.log(
      `[${videoId}] XML pattern found no segments, trying TTML pattern...`
    );
    while ((match = ttmlRegex.exec(transcriptContent)) !== null) {
      const offset = parseTimestamp(match[1]);
      const end = parseTimestamp(match[2]);
      const duration = end - offset;
      if (!isNaN(offset) && !isNaN(duration) && duration >= 0) {
        transcript.push({
          text: match[3]
            .replace(/&amp;/g, "&")
            .replace(/&lt;/g, "<")
            .replace(/&gt;/g, ">")
            .replace(/&#39;/g, "'")
            .replace(/&quot;/g, '"')
            .replace(/\\n/g, " ")
            .trim(),
          duration: duration,
          offset: offset,
        });
      } else {
        console.warn(
          `[${videoId}] Skipping segment due to invalid number format (TTML): begin='${match[1]}', end='${match[2]}'`
        );
      }
    }
  }

  if (transcript.length === 0) {
    console.error(
      `[${videoId}] Failed to parse transcript content using both XML and TTML patterns. Content snippet: ${transcriptContent.substring(
        0,
        500
      )}...`
    );
    return null;
  }

  console.log(
    `[${videoId}] Successfully extracted and parsed transcript from HTML: ${transcript.length} segments`
  );
  return transcript;
}

// Extracts caption data from HTML, finds URL, fetches, and parses transcript
async function fetchTranscriptDirect(
  videoId: string
): Promise<TranscriptLine[] | null> {
  console.log(`[${videoId}] Attempting Direct Fetch Strategy...`);

  try {
    // Check if we already have a session, if not establish one
    if (!cookieJar) {
      console.log(
        `[${videoId}] No active session, establishing YouTube session first...`
      );
      await establishYouTubeSession();
    }

    // Check cache first
    if (
      htmlCache[videoId] &&
      Date.now() - htmlCache[videoId].timestamp < CACHE_TTL
    ) {
      console.log(`[${videoId}] Using cached HTML content`);
      const transcript = await extractAndParseTranscriptFromHtml(
        htmlCache[videoId].html,
        videoId
      );
      if (transcript) {
        return transcript;
      }
      // If parsing failed with cached content, clear cache and try fresh fetch
      delete htmlCache[videoId];
    }

    // Random delay to mimic human behavior (100-300ms)
    await new Promise((resolve) =>
      setTimeout(resolve, 100 + Math.random() * 200)
    );

    // Log request headers for debugging
    const requestHeaders = {
      ...getBrowserLikeHeaders(),
      ...(cookieJar ? { Cookie: cookieJar } : {}),
    };
    console.log(
      `[${videoId}] Request headers (subset): User-Agent=${requestHeaders[
        "User-Agent"
      ]?.substring(0, 30)}..., Accept-Language=${
        requestHeaders["Accept-Language"]
      }, Cookie=${cookieJar ? "Set" : "Not set"}`
    );

    // Main video page request with cookies
    const response = await fetch(`https://www.youtube.com/watch?v=${videoId}`, {
      headers: requestHeaders,
      signal: AbortSignal.timeout(15000),
    });

    // Log response details for debugging
    console.log(
      `[${videoId}] YouTube response status: ${response.status}, URL: ${
        response.url
      }, Content-Type: ${response.headers.get("content-type")}`
    );

    // Check if we were redirected to consent page
    const finalUrl = response.url;
    if (finalUrl.includes("consent.youtube.com")) {
      console.log(`[${videoId}] Redirected to consent page, processing...`);
      await processConsentPage(finalUrl);

      // Retry the main request after consent
      console.log(
        `[${videoId}] Retrying main request after processing consent...`
      );
      const retryResponse = await fetch(
        `https://www.youtube.com/watch?v=${videoId}`,
        {
          headers: {
            ...getBrowserLikeHeaders(),
            ...(cookieJar ? { Cookie: cookieJar } : {}),
          },
          signal: AbortSignal.timeout(15000),
        }
      );

      // Log retry response details
      console.log(
        `[${videoId}] Retry response status: ${retryResponse.status}, URL: ${retryResponse.url}`
      );

      // Update cookies from the retry response
      const retryCookies = retryResponse.headers.get("set-cookie");
      if (retryCookies) {
        cookieJar = retryCookies;
      }

      // Continue with this response
      if (!retryResponse.ok) {
        throw new Error(
          `Failed to fetch video page after consent: ${retryResponse.status}`
        );
      }

      const html = await retryResponse.text();
      // Log HTML size and check for key markers
      console.log(
        `[${videoId}] Received HTML size: ${
          html.length
        }, Contains captions data: ${html.includes(
          "captionTracks"
        )}, Contains player data: ${html.includes("ytInitialPlayerResponse")}`
      );

      // Cache the HTML content
      htmlCache[videoId] = { html, timestamp: Date.now() };

      // Continue with parsing
      const transcript = await extractAndParseTranscriptFromHtml(html, videoId);
      if (!transcript) {
        throw new Error("Transcript extraction/parsing failed after consent");
      }
      return transcript;
    }

    // Update cookies from the video page response
    const videoPageCookies = response.headers.get("set-cookie");
    if (videoPageCookies) {
      cookieJar = videoPageCookies;
    }

    if (!response.ok) {
      throw new Error(
        `Failed to fetch video page: ${response.status} ${response.statusText}`
      );
    }

    const contentType = response.headers.get("content-type");
    if (!contentType || !contentType.includes("text/html")) {
      console.warn(
        `[${videoId}] Direct fetch received unexpected content-type: ${contentType}. Content might be blocked.`
      );
      // Attempt to read text anyway, might contain error info
      const maybeErrorText = await response.text();
      console.warn(
        `[${videoId}] Received content snippet: ${maybeErrorText.substring(
          0,
          500
        )}`
      );
      throw new Error(`Invalid content type returned: ${contentType}`);
    }

    const html = await response.text();
    if (!html || html.length < 500) {
      throw new Error(
        `Empty or too short response from YouTube (Length: ${html?.length})`
      );
    }

    // Log HTML content diagnostics
    console.log(
      `[${videoId}] HTML content length: ${
        html.length
      }, Contains captions data: ${html.includes(
        "captionTracks"
      )}, Contains player data: ${html.includes("ytInitialPlayerResponse")}`
    );

    // Update cache with the fresh HTML
    htmlCache[videoId] = { html, timestamp: Date.now() };

    // Call the combined extraction and parsing function
    const transcript = await extractAndParseTranscriptFromHtml(html, videoId);

    if (!transcript) {
      // Error logging happens inside extractAndParseTranscriptFromHtml
      throw new Error("Transcript extraction/parsing failed");
    }

    return transcript;
  } catch (error: any) {
    console.error(`[${videoId}] Direct Fetch Strategy failed:`, error.message);
    // Log specific failure details if available
    if (error.message.includes("No captions data found")) {
      console.error(
        `[${videoId}] Specific failure: Could not find captions JSON/patterns in HTML.`
      );
    }
    // Re-throw or return null/empty based on desired handling for the sequence
    // Throwing allows the sequence runner to catch and log appropriately
    throw error; // Let the calling sequence handle the failure logging for this method
  }
}

// fetchTranscriptInnertube remains largely the same, as it relies on a different initial fetch mechanism
// Potentially add more specific parsing/error handling within it if needed.
async function fetchTranscriptInnertube(
  videoId: string
): Promise<TranscriptLine[] | null> {
  console.log(`[${videoId}] Attempting Innertube API Strategy...`);

  try {
    // 1. Fetch Initial Page Content with enhanced headers
    const initialResponse = await fetch(
      `https://www.youtube.com/watch?v=${videoId}`,
      {
        headers: {
          ...getBrowserLikeHeaders(),
          ...(cookieJar ? { Cookie: cookieJar } : {}),
        },
        signal: AbortSignal.timeout(15000),
      }
    );

    // Update cookies from this response
    const setCookieHeader = initialResponse.headers.get("set-cookie");
    if (setCookieHeader) {
      cookieJar = setCookieHeader;
    }

    if (!initialResponse.ok) {
      throw new Error(
        `Innertube initial page fetch failed: ${initialResponse.status} ${initialResponse.statusText}`
      );
    }
    const html = await initialResponse.text();

    // 2. Extract Innertube API Key and Context from the HTML
    // Simplified regex - robust extraction is complex and brittle
    const apiKeyMatch = html.match(/"INNERTUBE_API_KEY":"(.*?)"/);
    const clientVersionMatch = html.match(
      /"INNERTUBE_CONTEXT_CLIENT_VERSION":"(.*?)"/
    );

    if (!apiKeyMatch || !clientVersionMatch) {
      console.error(
        `[${videoId}] Failed to extract Innertube API key or client version from page.`
      );
      // Maybe fallback to trying transcript extraction from this HTML directly?
      console.log(
        `[${videoId}] Attempting direct extraction from Innertube strategy's initial fetch...`
      );
      return await extractAndParseTranscriptFromHtml(html, videoId); // Fallback within fallback
      // throw new Error("Failed to extract Innertube API key/version");
    }
    const INNERTUBE_API_KEY = apiKeyMatch[1];
    const INNERTUBE_CONTEXT = {
      client: {
        clientName: "WEB",
        clientVersion: clientVersionMatch[1],
        // Other context fields might be necessary depending on YT changes
      },
    };

    console.log(
      `[${videoId}] Extracted Innertube Key: ${INNERTUBE_API_KEY.substring(
        0,
        10
      )}..., Version: ${INNERTUBE_CONTEXT.client.clientVersion}`
    );

    // 3. Make the Innertube API request with enhanced headers
    const playerApiUrl = `https://www.youtube.com/youtubei/v1/player?key=${INNERTUBE_API_KEY}`;
    const playerApiResponse = await fetch(playerApiUrl, {
      method: "POST",
      headers: {
        ...getBrowserLikeHeaders(),
        "Content-Type": "application/json",
        "X-YouTube-Client-Name": "1",
        "X-YouTube-Client-Version": INNERTUBE_CONTEXT.client.clientVersion,
        ...(cookieJar ? { Cookie: cookieJar } : {}),
      },
      body: JSON.stringify({
        context: INNERTUBE_CONTEXT,
        videoId: videoId,
        contentCheckOk: true,
        racyCheckOk: true,
      }),
      signal: AbortSignal.timeout(10000),
    });

    // Update cookies from this API response
    const apiCookies = playerApiResponse.headers.get("set-cookie");
    if (apiCookies) {
      cookieJar = apiCookies;
    }

    if (!playerApiResponse.ok) {
      const errorText = await playerApiResponse.text();
      console.error(
        `[${videoId}] Innertube Player API request failed: ${
          playerApiResponse.status
        }. Response: ${errorText.substring(0, 300)}`
      );
      throw new Error(
        `Innertube Player API request failed: ${playerApiResponse.status}`
      );
    }

    const playerResponse = await playerApiResponse.json();

    // 4. Find Caption Tracks within Player Response
    if (
      !playerResponse?.captions?.playerCaptionsTracklistRenderer
        ?.captionTracks ||
      playerResponse.captions.playerCaptionsTracklistRenderer.captionTracks
        .length === 0
    ) {
      console.error(
        `[${videoId}] No caption tracks found in Innertube player response`
      );
      console.error(
        "[Innertube Debug] No caption tracks found. Player Response Snippet:",
        JSON.stringify(playerResponse?.captions || {}).substring(0, 1000)
      );
      console.error(
        "[Innertube Info] No caption tracks listed in player response."
      );
      // Don't throw yet, maybe try extracting from initial page fetch as last resort
      // throw new Error('No caption tracks found in Innertube player response');
      console.log(
        `[${videoId}] Innertube API had no tracks, attempting direct extraction from initial page fetch...`
      );
      return await extractAndParseTranscriptFromHtml(html, videoId); // Final fallback attempt
    }

    const captionTracks =
      playerResponse.captions.playerCaptionsTracklistRenderer.captionTracks;

    // 5. Select and Fetch Transcript URL (similar to direct method)
    let transcriptUrl = "";
    const englishTrack = captionTracks.find(
      (track: any) => track.languageCode === "en"
    );
    transcriptUrl = englishTrack?.baseUrl || captionTracks[0]?.baseUrl;

    if (!transcriptUrl) {
      console.error(
        `[${videoId}] Could not find a valid baseUrl in Innertube caption tracks.`
      );
      return null; // Or throw
    }
    console.log(
      `[${videoId}] Using Innertube transcript URL: ${transcriptUrl.substring(
        0,
        60
      )}...`
    );

    // 6. Fetch the transcript with enhanced headers
    const transcriptResponse = await fetch(transcriptUrl, {
      headers: {
        ...getBrowserLikeHeaders(),
        ...(cookieJar ? { Cookie: cookieJar } : {}),
      },
      signal: AbortSignal.timeout(10000),
    });

    // Update cookies again
    const transcriptCookies = transcriptResponse.headers.get("set-cookie");
    if (transcriptCookies) {
      cookieJar = transcriptCookies;
    }

    if (!transcriptResponse.ok) {
      const errorText = await transcriptResponse.text();
      console.error(
        `[${videoId}] Failed to fetch Innertube transcript content: ${
          transcriptResponse.status
        } ${transcriptResponse.statusText}. Response: ${errorText.substring(
          0,
          200
        )}`
      );
      throw new Error(
        `Failed to fetch Innertube transcript data: ${transcriptResponse.status} ${transcriptResponse.statusText}`
      );
    }

    const transcriptContent = await transcriptResponse.text();
    if (!transcriptContent || transcriptContent.length < 50) {
      console.error(
        `[${videoId}] Invalid or empty Innertube transcript content received.`
      );
      return null;
    }
    if (
      !transcriptContent.includes("<text") &&
      !transcriptContent.includes("<p")
    ) {
      console.warn(
        `[${videoId}] Innertube transcript content might be invalid (missing common tags): ${transcriptContent.substring(
          0,
          200
        )}...`
      );
    }

    // Parse the XML/TTML to extract transcript segments
    const transcript: TranscriptLine[] = [];
    const xmlRegex = /<text start="([^"]*)" dur="([^"]*)">([^<]*)<\/text>/g;
    const ttmlRegex = /<p begin="([^"]*)" end="([^"]*)"[^>]*>([^<]*)<\/p>/g;
    let match;

    while ((match = xmlRegex.exec(transcriptContent)) !== null) {
      const offset = parseFloat(match[1]);
      const duration = parseFloat(match[2]);
      if (!isNaN(offset) && !isNaN(duration)) {
        transcript.push({
          text: match[3]
            .replace(/&amp;/g, "&")
            .replace(/&lt;/g, "<")
            .replace(/&gt;/g, ">")
            .replace(/&#39;/g, "'")
            .replace(/&quot;/g, '"')
            .replace(/\\n/g, " ")
            .trim(),
          duration: duration,
          offset: offset,
        });
      }
    }

    if (transcript.length === 0) {
      console.log(
        `[${videoId}] Innertube XML pattern found no segments, trying TTML pattern...`
      );
      while ((match = ttmlRegex.exec(transcriptContent)) !== null) {
        const offset = parseTimestamp(match[1]);
        const end = parseTimestamp(match[2]);
        const duration = end - offset;
        if (!isNaN(offset) && !isNaN(duration) && duration >= 0) {
          transcript.push({
            text: match[3]
              .replace(/&amp;/g, "&")
              .replace(/&lt;/g, "<")
              .replace(/&gt;/g, ">")
              .replace(/&#39;/g, "'")
              .replace(/&quot;/g, '"')
              .replace(/\\n/g, " ")
              .trim(),
            duration: duration,
            offset: offset,
          });
        }
      }
    }

    if (transcript.length === 0) {
      console.error(
        `[${videoId}] Failed to parse Innertube transcript content using both XML and TTML patterns.`
      );
      return null;
    }

    console.log(
      `[${videoId}] Successfully fetched and parsed transcript via Innertube API: ${transcript.length} segments`
    );
    return transcript;
  } catch (error: any) {
    console.error(`[${videoId}] Innertube API Strategy failed:`, error.message);
    // Check if it failed before even getting tracks, if so, try direct extraction from initial page fetch
    if (
      error.message.includes("Failed to extract Innertube API key") ||
      error.message.includes("Innertube Player API request failed")
    ) {
      console.log(
        `[${videoId}] Innertube core API failed, attempting direct extraction from initial page fetch...`
      );
      // Need to ensure html was fetched if error occurred early
      try {
        const initialResponse = await fetch(
          `https://www.youtube.com/watch?v=${videoId}`,
          {
            headers: getBrowserLikeHeaders(),
            signal: AbortSignal.timeout(15000),
          }
        );
        if (initialResponse.ok) {
          const html = await initialResponse.text();
          return await extractAndParseTranscriptFromHtml(html, videoId);
        }
      } catch (fallbackError: any) {
        console.error(
          `[${videoId}] Final fallback extraction also failed:`,
          fallbackError.message
        );
      }
    }
    throw error; // Let the calling sequence handle the failure logging for this method
  }
}
</file>

<file path="TRANSCRIPT_ISSUE_PLAN.md">
# YouTube Transcript Issue Troubleshooting Plan

This document outlines our systematic approach to resolving the YouTube transcript fetching issue that occurs on the Vercel deployment but not in local development.

## Current Issue

-  Application works correctly in local development environment
-  Application fails on Vercel deployment with multiple errors:
  -  Metadata fetching fixed (API Key Referrer Restriction)
  -  Transcript fetching still failing on Vercel (Direct & Innertube methods) - `No captions data found...`, `No caption tracks found...`

## Hypotheses

1. **CORS Restrictions**: Vercel's environment may have stricter CORS policies than local development
2. **YouTube IP Blocking / Page Structure Differences**: YouTube may be blocking requests or serving different HTML/data structures to Vercel's server IP ranges compared to local development (HIGHLY LIKELY for transcript failures)
3. **Library Compatibility**: The transcript fetching library might not be compatible with Vercel's serverless environment (Mitigated by custom implementation)
4. **Authentication/Headers**: The requests from Vercel might be missing necessary headers or user-agent information (Partially addressed by adding Accept-Language)
5. **Server vs. Client Execution**: The code might be executing in a different context (client vs server) on Vercel
6. ~~YouTube Page Structure~~: (Covered by Hypothesis 2)
7. ~~API Key Issues~~: (Resolved for metadata, but potentially still relevant if API method for transcripts is used/enabled)
8. ~~Page Format Changes~~: (Covered by Hypothesis 2)

## Current Diagnosis

Initial diagnosis indicated issues with API keys and URL construction. Further investigation and fixes revealed:

1. ~~URL construction for internal API calls was missing the proper protocol prefix~~ (Fixed by refactoring).
2. **YouTube returns different page structures/data when accessed from cloud providers (Vercel) vs. residential IPs**. This is the primary suspected cause for ongoing transcript failures for Direct/Innertube methods.
3. The specific regex patterns and JSON parsing logic for extracting captions/transcript URLs were failing on the data received in the Vercel environment.
4.  **The YouTube Data API metadata fetch failed due to API Key HTTP Referrer restrictions**. This was resolved by removing the restriction in Google Cloud Console, allowing server-side calls.

## Implemented Solution

We've addressed these issues with a multi-faceted approach:

1. Added more detailed logging to track down the exact point of failure (including HTML/JSON snippets on failure).
2. Fixed the URL construction for internal API calls by refactoring to direct function calls.
3. Implemented custom fallback methods (`Direct`, `Innertube`) for transcript fetching with browser-like headers (`User-Agent`, `Accept-Language`).
4. Added retry logic to try multiple methods of fetching transcripts (Library -> Direct -> Innertube).
5. **Refactored transcript fetching logic into a dedicated library (`lib/youtube-transcript.ts`)**.
6. Implemented detailed parsing logic within `lib/youtube-transcript.ts` including:
   - Multiple regex patterns to extract captions data from HTML.
   - Logic to find and select the transcript `baseUrl`.
   - Fetching the transcript XML/TTML content.
   - Parsing both XML (`<text>`) and TTML (`<p>`) formats.
7. Added timeout handling for HTTP requests.
8. Improved error handling and user-facing error messages.
9. Added direct oEmbed fallback for metadata.
10. Enhanced the Innertube method with key/context extraction and a fallback to direct HTML parsing if its API calls fail.
11. Provided more robust error handling to present useful information to the user even when transcripts can't be fetched.

### Phase 1: Initial Diagnosis & Fixes

 Identified missing API keys and URL construction issues

### Phase 2: First Iteration of Solutions

 Improved the server-side transcript fetching code with a robust fallback solution
 Added proper error handling with specific error types
 Added browser-like headers to all requests

### Phase 3: Enhanced Approach (Latest)

 **3.1 Multiple Approach Strategy**

- Implemented multiple methods: standard library, direct fetch, Innertube API (API method currently disabled/commented out).
- Added cascading fallbacks to try all methods before failing.
- Improved regex patterns for HTML data extraction.
- Added parsing logic for both XML and TTML transcript formats.
- Enhanced Innertube with fallback to direct HTML parsing.

 **3.2 API Key & Authentication**

- **Fixed YouTube Data API key HTTP referrer restriction**, enabling metadata fetch from Vercel server-side.
- Implemented direct oEmbed approach for metadata fallback.

 **3.3 Enhanced Error Handling**

- Improved error classification and user-friendly messages.
- Return metadata even when transcript fetching fails.
- Added specific handling for different error types (timeouts, parsing, etc.).
- Added detailed logging within `catch` blocks and parsing functions for better Vercel diagnostics.

 **3.4 YouTube Innertube API**

- Implemented YouTube's internal API approach for transcript fetching.
- Added dynamic extraction for API key and client context from page HTML.
- Included fallbacks at every level of the process (including parsing initial HTML if API fails).

 **3.5 API Refactoring (Commit dd95513)**

- **Centralized Metadata Fetching**: Created `lib/youtube.ts`.
- **Removed Internal API Call**: Refactored routes to call `fetchMetadataFromYouTubeAPI` directly.
- **Standardized Fallbacks**: Consistent oEmbed fallback.
- **Improved Sequential Transcript Logic**: Refined sequence in `/api/summarize/route.ts`.

 **3.6 Transcript Library Refactoring (Commits d4f2b4a, dcd558b, 1c7f63d)**

- **Moved Transcript Logic**: Created `lib/youtube-transcript.ts` to house fetching and parsing functions (`fetchTranscriptDirect`, `fetchTranscriptInnertube`, `extractAndParseTranscriptFromHtml`, `parseTimestamp`).
- **Added Type Definitions**: Defined `TranscriptLine` interface.
- **Implemented Parsing**: Moved and refined HTML data extraction and XML/TTML parsing logic into the library file.
- **Enhanced Logging**: Added more detailed logs within the library functions.
- **Fixed Build Issues**: Resolved type errors related to the refactoring.

## Monitoring and Validation

For the implemented solution:

1. Deploy latest commit (1c7f63d) to Vercel.
2. Test with the specific video ID that previously failed: `b9gPwO-IsB4`.
3. **Monitor Vercel logs closely** for output from `lib/youtube-transcript.ts`:
   - Regex matching success/failure (`Found potential captions data...`, `No captions data found...`).
   - Transcript URL used (`Using transcript URL...`).
   - Transcript content fetching/parsing errors (`Failed to fetch transcript content...`, `Failed to parse transcript content...`, `Invalid or empty transcript content...`).
   - Innertube specific logs (`Extracting transcript data from HTML...`, `Attempting Innertube API Strategy...`, `Extracted Innertube Key...`, `Innertube Player API request failed...`, `No caption tracks found in Innertube player response`).
4. Check if transcripts are successfully retrieved and displayed.
5. Test with other videos (with captions, auto-generated, non-English) to ensure no regressions.

## Success Criteria

- Application successfully retrieves transcripts for videos that have them available **on Vercel**.
- Application properly handles and communicates when transcripts are unavailable.
- Solution works consistently across multiple videos and over time.
- User receives helpful error messages when transcripts cannot be fetched.
- Basic functionality continues to work even when some components fail (graceful degradation).

## Future Enhancements (If Needed)

- [ ] **Refine Regex/Parsing**: If logs show specific failures, adjust patterns in `extractAndParseTranscriptFromHtml`.
- [ ] **Proxy Solution**: If direct Vercel IPs remain blocked/served different content, consider a proxy.
- [ ] **YouTube Data API for Captions**: Explore OAuth for official caption fetching if custom methods prove too unstable.
- [ ] **Alternative Libraries**: Re-evaluate third-party libraries if custom solution becomes unmaintainable.

## Implementation Notes

Metadata fetching is now stable after fixing the API key restrictions. The primary focus is on the transcript fetching instability within the Vercel environment. The refactoring into `lib/youtube-transcript.ts` centralizes the logic, and the detailed logging added should pinpoint the exact failure point (HTML structure mismatch, parsing error, etc.) when run on Vercel. The Innertube method now includes fallbacks to direct HTML parsing, increasing its resilience.

## Rollback Plan

If any implementation significantly degrades the application:

- Revert to the previous working deployment (e.g., commit `dcd558b` before parsing implementation if necessary).
- Document what caused the issue for future reference.
</file>

<file path="app/api/summarize/route.ts">
import { NextResponse } from "next/server";
import { YoutubeTranscript, TranscriptConfig } from "youtube-transcript";
import OpenAI from "openai";
import { fetchMetadataFromYouTubeAPI } from "@/lib/youtube";
import { PodcastMetadata } from "@/components/PodcastMetadata";

// Check if OpenAI API key is set
if (!process.env.OPENAI_API_KEY) {
  console.error("OPENAI_API_KEY is not set in environment variables");
  throw new Error("OPENAI_API_KEY is not set in environment variables");
}

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Define a simple type for oEmbed response (can be shared or redefined)
interface OEmbedResponse {
  title?: string;
  author_name?: string;
  thumbnail_url?: string;
}

// Simple oEmbed fetch function (can be shared or redefined) - adjusted for this context
async function fetchOEmbedMetadataForSummarize(
  videoId: string
): Promise<Partial<PodcastMetadata> | null> {
  const url = `https://www.youtube.com/oembed?url=https://www.youtube.com/watch?v=${videoId}&format=json`;
  try {
    console.log(
      `[${videoId}] (Summarize) Attempting oEmbed fallback for metadata...`
    );
    const response = await fetch(url, {
      next: { revalidate: 3600 },
      signal: AbortSignal.timeout(8000),
    }); // Cache & 8s timeout
    if (!response.ok) {
      console.error(
        `[${videoId}] (Summarize) oEmbed request failed with status ${response.status} ${response.statusText}`
      );
      return null;
    }
    const data: OEmbedResponse = await response.json();
    const metadata: Partial<PodcastMetadata> = {
      videoId: videoId,
      title: data.title || "YouTube Video (oEmbed)",
      channelName: data.author_name || "Unknown Channel (oEmbed)",
      thumbnails: data.thumbnail_url
        ? { default: { url: data.thumbnail_url, width: 0, height: 0 } }
        : null,
      duration: "0:00", // oEmbed doesn't provide duration
    };
    console.log(
      `[${videoId}] (Summarize) Successfully fetched partial metadata via oEmbed fallback.`
    );
    return metadata;
  } catch (error: any) {
    if (error.name === "AbortError") {
      console.error(`[${videoId}] (Summarize) oEmbed request timed out.`);
    } else {
      console.error(
        `[${videoId}] (Summarize) Error fetching oEmbed metadata:`,
        error.message || error
      );
    }
    return null;
  }
}

// Custom function to fetch YouTube transcript directly
async function fetchYouTubeTranscriptDirectly(videoId: string) {
  console.log(`[${videoId}] Attempting direct transcript fetch...`);
  try {
    // First, fetch the video page to extract necessary tokens
    const videoPageResponse = await fetch(
      `https://www.youtube.com/watch?v=${videoId}`,
      {
        headers: {
          "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
          "Accept-Language": "en-US,en;q=0.9",
          Accept:
            "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        },
        signal: AbortSignal.timeout(15000), // 15 second timeout
      }
    );

    if (!videoPageResponse.ok) {
      throw new Error(
        `Failed to fetch video page: ${videoPageResponse.status} ${videoPageResponse.statusText}`
      );
    }

    const contentType = videoPageResponse.headers.get("content-type");
    if (!contentType || !contentType.includes("text/html")) {
      // Sometimes YouTube might return JSON even on direct page request, handle this
      if (contentType.includes("application/json")) {
        console.warn(
          `[${videoId}] Direct fetch received JSON instead of HTML. Content might be blocked or page structure changed.`
        );
        // Optionally try to parse JSON for errors if relevant
      } else {
        throw new Error(`Invalid content type returned: ${contentType}`);
      }
    }

    const videoPageContent = await videoPageResponse.text();

    if (!videoPageContent || videoPageContent.length < 500) {
      // Increased threshold slightly
      throw new Error(
        `Empty or too short response from YouTube (Length: ${videoPageContent?.length})`
      );
    }

    // --- Start Regex Extraction ---
    let rawCaptionsData = null;
    const patterns = [
      /"captions":\s*(\{.*?"captionTracks":.*?\}),\s*"videoDetails"/, // Main pattern
      /"captionTracks":\s*(\[.*?\])/, // Simpler track array
      /"baseUrl":"(https:\/\/www\.youtube\.com\/api\/timedtext.*?)"/, // Corrected: Reduced escaping for slashes
    ];

    for (const pattern of patterns) {
      const match = videoPageContent.match(pattern);
      if (match && match[1]) {
        const matchedData = match[1]
          .replace(/\\"/g, '"')
          .replace(/\\\\/g, "\\"); // Basic cleaning
        console.log(
          `[${videoId}] Found potential captions data using pattern: ${pattern.source.substring(
            0,
            30
          )}...`
        );

        if (pattern.source.includes("baseUrl")) {
          // Handle direct URL pattern
          // Decode URL-encoded characters
          const decodedUrl = decodeURIComponent(JSON.parse(`"${matchedData}"`)); // Safer decoding
          rawCaptionsData = JSON.stringify({
            playerCaptionsTracklistRenderer: {
              captionTracks: [{ baseUrl: decodedUrl }],
            },
          });
          console.log(`[${videoId}] Extracted direct transcript URL.`);
        } else {
          // Attempt to parse as JSON for other patterns
          try {
            // More robust cleaning might be needed depending on YouTube's output
            const parsed = JSON.parse(matchedData);
            // Ensure structure is somewhat valid before accepting
            if (
              parsed &&
              (parsed.captionTracks ||
                (parsed.playerCaptionsTracklistRenderer &&
                  parsed.playerCaptionsTracklistRenderer.captionTracks))
            ) {
              // Re-stringify into a consistent format if needed, or use directly if structure matches
              if (
                parsed.captionTracks &&
                !parsed.playerCaptionsTracklistRenderer
              ) {
                rawCaptionsData = JSON.stringify({
                  playerCaptionsTracklistRenderer: {
                    captionTracks: parsed.captionTracks,
                  },
                });
              } else {
                rawCaptionsData = JSON.stringify(parsed); // Assume structure is okay
              }
              console.log(`[${videoId}] Successfully parsed extracted JSON.`);
            } else {
              console.warn(
                `[${videoId}] Parsed JSON has unexpected structure.`
              );
            }
          } catch (parseError) {
            console.warn(
              `[${videoId}] Failed to parse extracted data for pattern ${pattern.source.substring(
                0,
                30
              )}...: ${parseError}`
            );
            // Continue to next pattern
          }
        }
        if (rawCaptionsData) break; // Stop if valid data found
      }
    }
    // --- End Regex Extraction ---

    if (!rawCaptionsData) {
      console.error(
        `[${videoId}] No captions data found in video page using any pattern.`
      );
      // Optional: Log a snippet of the page for debugging (be careful with size/PII)
      // console.log(`[${videoId}] Page Snippet: ${videoPageContent.substring(0, 500)}`);
      throw new Error("No captions data found in video page");
    }

    let captionsData;
    try {
      captionsData = JSON.parse(rawCaptionsData); // Already cleaned during extraction
    } catch (e: any) {
      console.error(
        `[${videoId}] Failed to parse final captions JSON: ${
          e.message
        }. Raw Data: ${rawCaptionsData.substring(0, 200)}...`
      );
      throw new Error(`Failed to parse final captions data: ${e.message}`);
    }

    if (
      !captionsData?.playerCaptionsTracklistRenderer?.captionTracks ||
      captionsData.playerCaptionsTracklistRenderer.captionTracks.length === 0
    ) {
      console.warn(
        `[${videoId}] Parsed captions data lacks track information or is empty.`
      );
      throw new Error(
        "Transcript tracks unavailable or disabled in parsed data"
      );
    }

    // Find the first usable transcript URL (prefer 'en' if available, otherwise take first)
    let transcriptUrl = "";
    const tracks = captionsData.playerCaptionsTracklistRenderer.captionTracks;
    const englishTrack = tracks.find(
      (track: any) => track.languageCode === "en"
    );
    transcriptUrl = englishTrack?.baseUrl || tracks[0]?.baseUrl;

    if (!transcriptUrl) {
      console.error(
        `[${videoId}] Could not find a valid baseUrl in caption tracks.`
      );
      throw new Error("No valid transcript URL found in caption tracks");
    }
    console.log(
      `[${videoId}] Using transcript URL: ${transcriptUrl.substring(0, 60)}...`
    );

    // Fetch the transcript XML/TTML
    const transcriptResponse = await fetch(transcriptUrl, {
      headers: {
        "User-Agent":
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9", // Important for getting english transcript if auto-selected
        Accept: "*/*", // Accept any content type
      },
      signal: AbortSignal.timeout(10000), // 10 second timeout
    });

    if (!transcriptResponse.ok) {
      const errorText = await transcriptResponse.text();
      console.error(
        `[${videoId}] Failed to fetch transcript content: ${
          transcriptResponse.status
        } ${transcriptResponse.statusText}. Response: ${errorText.substring(
          0,
          200
        )}`
      );
      throw new Error(
        `Failed to fetch transcript data: ${transcriptResponse.status} ${transcriptResponse.statusText}`
      );
    }

    const transcriptContent = await transcriptResponse.text();

    // Basic validation of transcript content
    if (!transcriptContent || transcriptContent.length < 50) {
      console.error(
        `[${videoId}] Invalid or empty transcript content received.`
      );
      throw new Error("Invalid or empty transcript data received");
    }
    // More robust check: look for common transcript tags
    if (
      !transcriptContent.includes("<text") &&
      !transcriptContent.includes("<p")
    ) {
      console.warn(
        `[${videoId}] Transcript content might be invalid (missing common tags): ${transcriptContent.substring(
          0,
          200
        )}...`
      );
      // Decide whether to throw error or proceed cautiously
      // throw new Error("Transcript content appears invalid (missing tags)");
    }

    // Parse the XML/TTML to extract transcript segments
    const transcript = [];
    // Handle standard XML format <text start="..." dur="...">...</text>
    const xmlRegex = /<text start="([^"]*)" dur="([^"]*)">([^<]*)<\/text>/g;
    // Handle alternative format (often in TTML) <p begin="..." end="..." ...>...</p>
    const ttmlRegex = /<p begin="([^"]*)" end="([^"]*)"[^>]*>([^<]*)<\/p>/g;

    let match;
    // Try XML first
    while ((match = xmlRegex.exec(transcriptContent)) !== null) {
      const offset = parseFloat(match[1]);
      const duration = parseFloat(match[2]);
      // Basic sanity check for parsed numbers
      if (!isNaN(offset) && !isNaN(duration)) {
        transcript.push({
          text: match[3]
            .replace(/&amp;/g, "&")
            .replace(/&lt;/g, "<")
            .replace(/&gt;/g, ">")
            .replace(/&#39;/g, "'")
            .replace(/&quot;/g, '"')
            .replace(/\\n/g, " ")
            .trim(),
          duration: duration,
          offset: offset,
        });
      } else {
        console.warn(
          `[${videoId}] Skipping segment due to invalid number format (XML): start='${match[1]}', dur='${match[2]}'`
        );
      }
    }

    // If XML parsing yielded nothing, try TTML format
    if (transcript.length === 0) {
      console.log(
        `[${videoId}] XML pattern found no segments, trying TTML pattern...`
      );
      while ((match = ttmlRegex.exec(transcriptContent)) !== null) {
        const offset = parseTimestamp(match[1]); // Need helper to parse HH:MM:SS.ms
        const end = parseTimestamp(match[2]);
        const duration = end - offset;
        // Basic sanity check for parsed numbers
        if (!isNaN(offset) && !isNaN(duration) && duration >= 0) {
          transcript.push({
            text: match[3]
              .replace(/&amp;/g, "&")
              .replace(/&lt;/g, "<")
              .replace(/&gt;/g, ">")
              .replace(/&#39;/g, "'")
              .replace(/&quot;/g, '"')
              .replace(/\\n/g, " ")
              .trim(),
            duration: duration,
            offset: offset,
          });
        } else {
          console.warn(
            `[${videoId}] Skipping segment due to invalid number format (TTML): begin='${match[1]}', end='${match[2]}'`
          );
        }
      }
    }

    if (transcript.length === 0) {
      console.error(
        `[${videoId}] Failed to parse transcript content using both XML and TTML patterns. Content snippet: ${transcriptContent.substring(
          0,
          500
        )}...`
      );
      throw new Error("Failed to parse transcript XML/TTML content");
    }

    console.log(
      `[${videoId}] Successfully fetched and parsed transcript directly: ${transcript.length} segments`
    );
    return transcript;
  } catch (error: any) {
    // Enhanced logging
    console.error(
      `[${videoId}] Direct transcript fetch failed: ${error.message || error}`
    );
    // Add more specific logging based on error message content
    if (error.message?.includes("fetch video page")) {
      console.error(
        `[${videoId}] Specific failure: Fetching main video page. Status: ${
          error.message.split(": ")[1]
        }`
      );
    } else if (error.message?.includes("No captions data found")) {
      console.error(
        `[${videoId}] Specific failure: Could not find captions JSON/patterns in HTML.`
      );
    } else if (error.message?.includes("parse final captions data")) {
      console.error(
        `[${videoId}] Specific failure: Parsing extracted captions JSON.`
      );
    } else if (error.message?.includes("fetch transcript data")) {
      console.error(
        `[${videoId}] Specific failure: Fetching the transcript content file. Status: ${
          error.message.split(": ")[1]
        }`
      );
    } else if (error.message?.includes("parse transcript XML/TTML")) {
      console.error(
        `[${videoId}] Specific failure: Parsing the final transcript content.`
      );
    } else if (error.name === "AbortError") {
      console.error(
        `[${videoId}] Direct transcript fetch sub-request timed out.`
      );
    } else if (error.message?.includes("Invalid content type")) {
      console.error(
        `[${videoId}] Specific failure: Received unexpected content type from YouTube.`
      );
    } else if (error.message?.includes("Empty or too short response")) {
      console.error(
        `[${videoId}] Specific failure: Received unusable short response from YouTube.`
      );
    }
    // Re-throw the original error to be caught by the main handler's loop
    throw error;
  }
}

// Helper to parse HH:MM:SS.ms timestamps from TTML
function parseTimestamp(timestamp: string): number {
  const parts = timestamp.split(":");
  let seconds = 0;
  if (parts.length === 3) {
    seconds += parseFloat(parts[0]) * 3600;
    seconds += parseFloat(parts[1]) * 60;
    seconds += parseFloat(parts[2]);
  } else if (parts.length === 2) {
    seconds += parseFloat(parts[0]) * 60;
    seconds += parseFloat(parts[1]);
  } else if (parts.length === 1) {
    seconds += parseFloat(parts[0]);
  }
  return isNaN(seconds) ? 0 : seconds; // Return 0 if parsing fails
}

// Helper function to fetch podcast metadata (Refactored for Summarize route)
const fetchPodcastMetadataForSummarize = async (
  videoId: string
): Promise<Partial<PodcastMetadata> | null> => {
  let metadata: Partial<PodcastMetadata> | null = null;

  // 1. Try fetching using the YouTube Data API (shared function)
  try {
    console.log(
      `[${videoId}] (Summarize) Attempting metadata fetch via YouTube API...`
    );
    metadata = await fetchMetadataFromYouTubeAPI(videoId);
    if (metadata) {
      console.log(
        `[${videoId}] (Summarize) Metadata successfully fetched via YouTube API.`
      );
      return metadata;
    } else {
      console.warn(
        `[${videoId}] (Summarize) YouTube API metadata fetch returned null, proceeding to fallback.`
      );
    }
  } catch (apiError: any) {
    console.error(
      `[${videoId}] (Summarize) Error during YouTube API metadata fetch: ${
        apiError.message || apiError
      }. Proceeding to fallback.`
    );
    if (apiError.name === "AbortError") {
      console.error(`[${videoId}] (Summarize) YouTube API request timed out.`);
    }
  }

  // 2. If API fails or returns null, try oEmbed fallback specific to this route
  if (!metadata) {
    try {
      console.log(
        `[${videoId}] (Summarize) Attempting metadata fetch via oEmbed fallback...`
      );
      metadata = await fetchOEmbedMetadataForSummarize(videoId); // Use the function defined in this file
      if (metadata) {
        console.log(
          `[${videoId}] (Summarize) Metadata successfully fetched via oEmbed.`
        );
        return metadata;
      } else {
        console.warn(
          `[${videoId}] (Summarize) oEmbed metadata fetch also returned null.`
        );
      }
    } catch (oembedError: any) {
      console.error(
        `[${videoId}] (Summarize) Error during oEmbed metadata fetch: ${
          oembedError.message || oembedError
        }`
      );
    }
  }

  // 3. If both methods fail, return null
  if (!metadata) {
    console.error(
      `[${videoId}] (Summarize) All methods failed to fetch metadata.`
    );
    return null;
  }

  return metadata;
};

// Function to fetch transcript using YouTube API (Enhanced Logging)
// Note: This often requires OAuth for non-public captions. API key might only work for public ones.
async function fetchYouTubeTranscriptViaAPI(videoId: string) {
  const apiKey = process.env.YOUTUBE_API_KEY;
  if (!apiKey) {
    console.warn(
      `[${videoId}] YouTube API key missing, cannot attempt transcript via API.`
    );
    return null;
  }
  console.log(`[${videoId}] Attempting transcript fetch via YouTube API...`);
  try {
    // Fetch caption list
    const listUrl = `https://www.googleapis.com/youtube/v3/captions?part=snippet&videoId=${videoId}&key=${apiKey}`;
    const listResponse = await fetch(listUrl, {
      signal: AbortSignal.timeout(10000),
    });

    if (!listResponse.ok) {
      const errorText = await listResponse.text();
      console.error(
        `[${videoId}] YouTube API caption list request failed: Status ${
          listResponse.status
        }. Response: ${errorText.substring(0, 200)}`
      );
      if (listResponse.status === 403) {
        // Forbidden often means captions disabled by owner or requires OAuth
        console.warn(
          `[${videoId}] API Error 403: Captions likely disabled or require owner permission (OAuth).`
        );
        throw new Error(`Captions disabled or private (API 403)`);
      } else if (listResponse.status === 404) {
        console.warn(`[${videoId}] API Error 404: Video/Captions not found.`);
        throw new Error(`Video or captions not found (API 404)`);
      } else {
        throw new Error(
          `Failed to list captions via API: ${listResponse.statusText}`
        );
      }
    }

    const listData = await listResponse.json();
    if (!listData.items || listData.items.length === 0) {
      console.warn(`[${videoId}] YouTube API returned no caption tracks.`);
      throw new Error("No caption tracks found via API");
    }

    // Prefer English, fallback to first available
    const englishTrack = listData.items.find(
      (item: any) => item.snippet?.language === "en"
    );
    const trackToDownload = englishTrack || listData.items[0];
    const captionId = trackToDownload.id;

    console.log(
      `[${videoId}] Found API caption track ID: ${captionId}, Language: ${trackToDownload.snippet?.language}`
    );

    // Download the caption track (try common formats, srt might work with API key)
    // Note: download often requires OAuth, but try standard formats. ttml is common.
    let downloadedTranscript = null;
    const formatsToTry = ["srt", "vtt", "ttml"]; // Common formats
    for (const format of formatsToTry) {
      const downloadUrl = `https://www.googleapis.com/youtube/v3/captions/${captionId}?key=${apiKey}&tfmt=${format}`;
      try {
        console.log(`[${videoId}] Attempting API download format: ${format}`);
        const downloadResponse = await fetch(downloadUrl, {
          signal: AbortSignal.timeout(10000),
        });
        if (downloadResponse.ok) {
          downloadedTranscript = await downloadResponse.text();
          console.log(
            `[${videoId}] Successfully downloaded API transcript format: ${format}`
          );
          break; // Success
        } else {
          const errorText = await downloadResponse.text();
          console.warn(
            `[${videoId}] API download failed for format ${format}: Status ${
              downloadResponse.status
            }. Response: ${errorText.substring(0, 200)}`
          );
          // Continue to next format
        }
      } catch (downloadError: any) {
        console.warn(
          `[${videoId}] Error during API download attempt for format ${format}: ${downloadError.message}`
        );
        // Continue to next format
      }
    }

    if (!downloadedTranscript) {
      console.error(
        `[${videoId}] Failed to download API transcript using formats: ${formatsToTry.join(
          ", "
        )}`
      );
      throw new Error(`Failed to download caption track using any format`);
    }

    // Simple parsing (assuming SRT or VTT-like format - needs improvement for robustness)
    // This is a placeholder - a proper SRT/VTT parser is recommended
    const lines = downloadedTranscript.split("\\n");
    const transcript = lines
      .map((line) => line.trim())
      .filter((line) => line && !line.match(/^\d+$/) && !line.includes("-->")) // Basic filter
      .map((text) => ({ text, duration: 0, offset: 0 })); // Dummy duration/offset

    if (transcript.length === 0) {
      console.error(
        `[${videoId}] Failed to parse downloaded API transcript content.`
      );
      throw new Error("Failed to parse downloaded API transcript");
    }

    console.log(
      `[${videoId}] Successfully fetched and parsed transcript via YouTube API: ${transcript.length} lines (basic parse).`
    );
    return transcript; // Return the parsed transcript
  } catch (error: any) {
    console.error(
      `[${videoId}] YouTube API transcript fetch failed: ${
        error.message || error
      }`
    );
    if (error.name === "AbortError") {
      console.error(
        `[${videoId}] YouTube API transcript fetch sub-request timed out.`
      );
    }
    // Specific logging based on error message
    if (
      error.message?.includes("Captions disabled") ||
      error.message?.includes("API 403")
    ) {
      console.warn(
        `[${videoId}] Transcript fetch via API blocked (likely disabled/private).`
      );
    } else if (error.message?.includes("No caption tracks")) {
      console.warn(`[${videoId}] No caption tracks listed by API.`);
    } else if (error.message?.includes("Failed to download")) {
      console.error(
        `[${videoId}] Critical failure during API transcript download phase.`
      );
    }
    return null; // Indicate failure to the caller to try next method
  }
}

// Function to fetch transcript using Innertube API (Enhanced Logging)
async function fetchYouTubeTranscriptViaInnertubeAPI(videoId: string) {
  console.log(`[${videoId}] Attempting transcript fetch via Innertube API...`);
  let pageContent = ""; // Store page content for debugging
  try {
    // 1. Fetch the video page
    const pageResponse = await fetch(
      `https://www.youtube.com/watch?v=${videoId}`,
      {
        headers: {
          "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
          "Accept-Language": "en-US,en;q=0.9",
        },
        signal: AbortSignal.timeout(15000),
      }
    );
    if (!pageResponse.ok) {
      throw new Error(
        `Failed to fetch video page: ${pageResponse.status} ${pageResponse.statusText}`
      );
    }
    pageContent = await pageResponse.text();
    if (!pageContent || pageContent.length < 1000) {
      throw new Error("Empty or too short response from YouTube page fetch");
    }

    // 2. Extract necessary data (API Key, Client Version, Context)
    const apiKeyMatch = pageContent.match(/"innertubeApiKey":"([^"]+)"/);
    const clientVersionMatch = pageContent.match(/"clientVersion":"([^"]+)"/);
    // Context extraction is complex, find ytInitialPlayerResponse or similar
    const playerResponseMatch = pageContent.match(
      /ytInitialPlayerResponse\s*=\s*(\{.*?\});/
    );

    if (!apiKeyMatch || !apiKeyMatch[1])
      throw new Error("Could not extract Innertube API key");
    if (!clientVersionMatch || !clientVersionMatch[1])
      throw new Error("Could not extract client version");
    if (!playerResponseMatch || !playerResponseMatch[1])
      throw new Error(
        "Could not extract player context (ytInitialPlayerResponse)"
      );

    const apiKey = apiKeyMatch[1];
    const clientVersion = clientVersionMatch[1];
    let playerResponse;
    try {
      playerResponse = JSON.parse(playerResponseMatch[1]);
    } catch (e: any) {
      console.error(
        `[${videoId}] Failed to parse ytInitialPlayerResponse JSON: ${e.message}`
      );
      throw new Error("Failed to parse player context JSON");
    }

    // Find captions URL within the player response
    const captionTracks =
      playerResponse?.captions?.playerCaptionsTracklistRenderer?.captionTracks;
    if (!captionTracks || captionTracks.length === 0) {
      throw new Error("No caption tracks found in Innertube player response");
    }

    // Prefer English track, fallback to the first one
    const englishTrack = captionTracks.find(
      (track: any) => track.languageCode === "en"
    );
    const targetTrack = englishTrack || captionTracks[0];
    const captionsUrl = targetTrack?.baseUrl;

    if (!captionsUrl) {
      throw new Error(
        "Could not find captions baseUrl in Innertube player response"
      );
    }

    console.log(
      `[${videoId}] Found Innertube captions URL for lang ${
        targetTrack.languageCode
      }: ${captionsUrl.substring(0, 60)}...`
    );

    // 3. Fetch the actual transcript from the Innertube captions URL
    const transcriptResponse = await fetch(captionsUrl, {
      headers: {
        "User-Agent":
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
      },
      signal: AbortSignal.timeout(10000),
    });

    if (!transcriptResponse.ok) {
      const errorText = await transcriptResponse.text();
      console.error(
        `[${videoId}] Innertube captions URL fetch failed: Status ${
          transcriptResponse.status
        }. Response: ${errorText.substring(0, 200)}`
      );
      throw new Error(
        `Innertube captions URL fetch failed: ${transcriptResponse.statusText}`
      );
    }

    const transcriptContent = await transcriptResponse.text();
    if (!transcriptContent || transcriptContent.length < 50) {
      throw new Error("Empty or invalid transcript content from Innertube URL");
    }

    // 4. Parse the transcript (reuse direct fetch parsing logic)
    const transcript = [];
    const xmlRegex = /<text start="([^"]*)" dur="([^"]*)">([^<]*)<\/text>/g;
    const ttmlRegex = /<p begin="([^"]*)" end="([^"]*)"[^>]*>([^<]*)<\/p>/g;
    let match;

    while ((match = xmlRegex.exec(transcriptContent)) !== null) {
      const offset = parseFloat(match[1]);
      const duration = parseFloat(match[2]);
      if (!isNaN(offset) && !isNaN(duration)) {
        transcript.push({
          text: match[3]
            .replace(/&amp;/g, "&")
            .replace(/&lt;/g, "<")
            .replace(/&gt;/g, ">")
            .replace(/&#39;/g, "'")
            .replace(/&quot;/g, '"')
            .replace(/\\n/g, " ")
            .trim(),
          duration: duration,
          offset: offset,
        });
      }
    }

    if (transcript.length === 0) {
      console.log(
        `[${videoId}] Innertube XML pattern found no segments, trying TTML pattern...`
      );
      while ((match = ttmlRegex.exec(transcriptContent)) !== null) {
        const offset = parseTimestamp(match[1]);
        const end = parseTimestamp(match[2]);
        const duration = end - offset;
        if (!isNaN(offset) && !isNaN(duration) && duration >= 0) {
          transcript.push({
            text: match[3]
              .replace(/&amp;/g, "&")
              .replace(/&lt;/g, "<")
              .replace(/&gt;/g, ">")
              .replace(/&#39;/g, "'")
              .replace(/&quot;/g, '"')
              .replace(/\\n/g, " ")
              .trim(),
            duration: duration,
            offset: offset,
          });
        }
      }
    }

    if (transcript.length === 0) {
      console.error(
        `[${videoId}] Failed to parse Innertube transcript content using both XML and TTML patterns. Content snippet: ${transcriptContent.substring(
          0,
          500
        )}...`
      );
      throw new Error("Failed to parse Innertube transcript content");
    }

    console.log(
      `[${videoId}] Successfully fetched and parsed transcript via Innertube API: ${transcript.length} segments`
    );
    return transcript;
  } catch (error: any) {
    console.error(
      `[${videoId}] Innertube API transcript fetch failed: ${
        error.message || error
      }`
    );
    // Add specific logging based on error messages
    if (error.message?.includes("fetch video page")) {
      console.error(
        `[${videoId}] Innertube Failure: Fetching initial page. Status: ${
          error.message.split(": ")[1]
        }`
      );
    } else if (error.message?.includes("extract Innertube API key")) {
      console.error(
        `[${videoId}] Innertube Failure: Extracting API key. Check page structure.`
      );
      // console.log(`[${videoId}] Innertube Page Snippet: ${pageContent?.substring(0, 500)}`); // Debugging
    } else if (error.message?.includes("extract client version")) {
      console.error(
        `[${videoId}] Innertube Failure: Extracting client version. Check page structure.`
      );
    } else if (error.message?.includes("extract player context")) {
      console.error(
        `[${videoId}] Innertube Failure: Extracting ytInitialPlayerResponse. Check page structure.`
      );
    } else if (error.message?.includes("parse player context")) {
      console.error(
        `[${videoId}] Innertube Failure: Parsing ytInitialPlayerResponse JSON.`
      );
    } else if (error.message?.includes("No caption tracks found")) {
      console.warn(
        `[${videoId}] Innertube Info: No caption tracks listed in player response.`
      );
    } else if (error.message?.includes("Could not find captions baseUrl")) {
      console.error(
        `[${videoId}] Innertube Failure: Found tracks but no baseUrl.`
      );
    } else if (error.message?.includes("Innertube captions URL fetch failed")) {
      console.error(
        `[${videoId}] Innertube Failure: Fetching final transcript content. Status: ${
          error.message.split(": ")[1]
        }`
      );
    } else if (
      error.message?.includes("Failed to parse Innertube transcript content")
    ) {
      console.error(
        `[${videoId}] Innertube Failure: Parsing final transcript content.`
      );
    } else if (error.name === "AbortError") {
      console.error(
        `[${videoId}] Innertube API transcript fetch sub-request timed out.`
      );
    } else if (error.message?.includes("Empty or too short response")) {
      console.error(
        `[${videoId}] Innertube Failure: Empty/short response fetching initial page.`
      );
    } else if (error.message?.includes("Empty or invalid transcript content")) {
      console.error(
        `[${videoId}] Innertube Failure: Empty/short response fetching final transcript.`
      );
    }
    return null; // Indicate failure to the caller to try next method
  }
}

// Main POST handler (modified to handle metadata failure and try multiple transcript methods)
export async function POST(request: Request) {
  let videoId = ""; // Initialize videoId for broader scope logging
  try {
    const { url } = await request.json();

    // Extract video ID
    const videoIdMatch = url.match(
      /(?:youtu\.be\/|youtube\.com\/(?:watch\?v=|embed\/|v\/))([^&?\s]+)/
    );

    if (!videoIdMatch) {
      console.error("Invalid YouTube URL received:", url);
      return NextResponse.json(
        { error: "Invalid YouTube URL format." },
        { status: 400 }
      );
    }
    videoId = videoIdMatch[1];

    // --- Fetch Metadata ---
    console.log(`[${videoId}] Starting process: Fetching metadata...`);
    // Use Partial<PodcastMetadata> as the functions return partial data
    const metadata: Partial<PodcastMetadata> | null =
      await fetchPodcastMetadataForSummarize(videoId);

    if (!metadata) {
      console.error(
        `[${videoId}] Critical failure: Could not fetch required metadata after all attempts.`
      );
      return NextResponse.json(
        {
          error:
            "Failed to fetch essential video metadata. The video might be private, deleted, or there could be an issue with the YouTube API/oEmbed services.",
        },
        { status: 500 } // Internal Server Error might be more appropriate than 404
      );
    }
    // Log only essential metadata fields fetched
    console.log(
      `[${videoId}] Metadata fetched (Title: ${metadata.title}, Channel: ${metadata.channelName}). Proceeding...`
    );

    // --- Transcript Fetching Logic with Fallbacks ---
    let transcript: any = null; // Use 'any' or a specific transcript segment type
    let lastTranscriptError: Error | null = null;
    const transcriptMethods = [
      {
        name: "Library",
        func: () =>
          YoutubeTranscript.fetchTranscript(videoId, {
            lang: "en",
          } as TranscriptConfig),
      }, // Specify lang if possible
      { name: "Direct", func: () => fetchYouTubeTranscriptDirectly(videoId) },
      // { name: 'API', func: () => fetchYouTubeTranscriptViaAPI(videoId) }, // Often requires OAuth, less reliable with key alone. Uncomment if OAuth is implemented or key works.
      {
        name: "Innertube",
        func: () => fetchYouTubeTranscriptViaInnertubeAPI(videoId),
      },
    ];

    console.log(`[${videoId}] Starting transcript fetching sequence...`);
    for (const method of transcriptMethods) {
      try {
        console.log(`[${videoId}] Trying transcript method: ${method.name}...`);
        // Ensure the function call is awaited
        transcript = await method.func();
        // Check if transcript is valid (not null and has content)
        if (transcript && Array.isArray(transcript) && transcript.length > 0) {
          console.log(
            `[${videoId}] Transcript successfully fetched using method: ${method.name} (${transcript.length} segments).`
          );
          break; // Exit loop on success
        } else if (
          transcript &&
          Array.isArray(transcript) &&
          transcript.length === 0
        ) {
          // Handle cases where the function returns empty array without throwing an error
          console.warn(
            `[${videoId}] Transcript method ${method.name} completed but returned an empty array.`
          );
          lastTranscriptError = new Error(
            `Method ${method.name} returned empty transcript array.`
          );
        } else if (!transcript) {
          console.warn(
            `[${videoId}] Transcript method ${method.name} completed but returned null/undefined.`
          );
          lastTranscriptError = new Error(
            `Method ${method.name} returned null/undefined.`
          );
        }
      } catch (error: any) {
        console.warn(`[${videoId}] Transcript method ${method.name} failed.`); // Detailed log happens inside function
        lastTranscriptError = error; // Store the last error encountered
        // Specific handling for common library errors
        if (method.name === "Library") {
          if (error.message?.includes("[YoutubeTranscript]")) {
            // Check for specific known library errors like "disabled" or "no captions"
            if (
              error.message.toLowerCase().includes("disabled") ||
              error.message.toLowerCase().includes("no captions")
            ) {
              console.log(
                `[${videoId}] Library indicates transcript unavailable/disabled.`
              );
            }
          }
        }
        // If direct or innertube fail with "disabled" or "no captions found", log it
        if (
          (method.name === "Direct" || method.name === "Innertube") &&
          (error.message?.toLowerCase().includes("disabled") ||
            error.message?.toLowerCase().includes("no captions data found") ||
            error.message?.toLowerCase().includes("no caption tracks found"))
        ) {
          console.log(
            `[${videoId}] ${method.name} method indicates transcript unavailable/disabled.`
          );
        }
      }
    }

    // --- Process Transcript (if fetched) ---
    let summary = "Could not generate summary."; // Default summary
    let processedTranscriptText = "Transcript not available.";

    if (transcript && Array.isArray(transcript) && transcript.length > 0) {
      console.log(
        `[${videoId}] Transcript obtained (${transcript.length} segments), proceeding to format and summarize...`
      );
      // Join transcript segments into a single string
      processedTranscriptText = transcript
        .map((item: { text: string }) => item.text)
        .join(" ");

      // Basic check for meaningful transcript content
      if (processedTranscriptText.trim().length < 50) {
        // Arbitrary short length check
        console.warn(
          `[${videoId}] Transcript text seems very short (${
            processedTranscriptText.trim().length
          } chars), summary might be poor.`
        );
        summary =
          "Summary could not be generated (transcript content was too short or invalid).";
        processedTranscriptText =
          "Transcript content appears invalid or too short.";
        // Decide if we should still return 200 or an error state
        return NextResponse.json(
          {
            metadata: metadata as PodcastMetadata, // Return metadata
            summary: summary,
            transcript: processedTranscriptText,
            error: "Transcript content invalid/short.",
          },
          { status: 200 }
        ); // Return 200 OK but indicate transcript issue in payload
      } else {
        // --- Summarization Logic ---
        try {
          const MAX_TRANSCRIPT_LENGTH = 100000; // gpt-4o-mini has large context, but keep reasonable limit
          let transcriptForPrompt = processedTranscriptText;
          if (processedTranscriptText.length > MAX_TRANSCRIPT_LENGTH) {
            console.warn(
              `[${videoId}] Transcript length (${processedTranscriptText.length}) exceeds limit (${MAX_TRANSCRIPT_LENGTH}), truncating.`
            );
            transcriptForPrompt = processedTranscriptText.substring(
              0,
              MAX_TRANSCRIPT_LENGTH
            );
          }

          // Construct a more detailed prompt
          const prompt = `
                  **Analyze the following podcast transcript:**

                  **Metadata:**
                  - Title: ${metadata.title || "N/A"}
                  - Channel: ${metadata.channelName || "N/A"}
                  ${metadata.duration ? `- Duration: ${metadata.duration}` : ""}

                  **Transcript:**
                  """
                  ${transcriptForPrompt}
                  """

                  **Instructions:**
                  Generate a comprehensive yet concise summary in Markdown format. Structure the summary with the following sections, ensuring each section has meaningful content derived *only* from the provided transcript and metadata. If a section cannot be populated from the text, omit it or state "Not applicable based on transcript."

                  1.  **Executive Summary:** (1-2 paragraphs capturing the core essence and main topics discussed).
                  2.  **Key Insights:** (3-5 distinct bullet points highlighting the most important takeaways, arguments, or findings).
                  3.  **Notable Quotes:** (2-3 impactful or representative quotes directly from the transcript, correctly attributed if possible, otherwise just list the quote).
                  4.  **Potential Action Items / Further Questions:** (1-3 actionable suggestions or thought-provoking questions raised by the content).

                  **Formatting Requirements:**
                  - Use standard Markdown (headers, bolding, lists).
                  - Ensure clarity, accuracy, and conciseness.
                  - Do NOT add any introductory or concluding phrases outside of the requested sections.
                  - Output only the Markdown content.
               `;

          console.log(
            `[${videoId}] Sending request to OpenAI (gpt-4o-mini) for summarization...`
          );
          const response = await openai.chat.completions.create({
            model: "gpt-4o-mini",
            messages: [{ role: "user", content: prompt }],
            max_tokens: 1200, // Increased slightly for potentially longer summaries
            temperature: 0.5,
            // stop: ["\n\n"], // Optional: control stop sequences if needed
          });
          console.log(`[${videoId}] OpenAI summarization response received.`);

          if (
            !response.choices ||
            response.choices.length === 0 ||
            !response.choices[0].message?.content
          ) {
            // Log the API response if available and failed
            console.error(
              `[${videoId}] Invalid or empty response received from OpenAI. Response:`,
              JSON.stringify(response, null, 2)
            );
            throw new Error("Invalid or empty response received from OpenAI.");
          }

          summary = response.choices[0].message.content.trim();
        } catch (openaiError: any) {
          console.error(
            `[${videoId}] Error during OpenAI summarization: ${
              openaiError.message || openaiError
            }`
          );
          // Check for specific OpenAI errors (e.g., rate limits, content policy)
          if (openaiError.response) {
            // Check if it's an API error response
            console.error(
              `[${videoId}] OpenAI API Error Details: Status ${
                openaiError.response.status
              }, Data: ${JSON.stringify(openaiError.response.data)}`
            );
            summary = `Error generating summary due to OpenAI API issue (Status: ${openaiError.response.status}). Please try again later.`;
          } else {
            summary = "Error generating summary. Please try again later.";
          }
          // Keep the processed transcript text available even if summary fails
        }
      } // End else block for valid transcript length
    } else {
      // Handle case where transcript could not be fetched by any method
      console.error(
        `[${videoId}] Failed to fetch transcript using all available methods. Last error: ${lastTranscriptError?.message}`
      );

      // Determine the most likely reason for failure based on last error
      let userErrorMessage =
        "Could not retrieve transcript after multiple attempts.";
      if (
        lastTranscriptError?.message?.toLowerCase().includes("disabled") ||
        lastTranscriptError?.message
          ?.toLowerCase()
          .includes("no captions data found") ||
        lastTranscriptError?.message
          ?.toLowerCase()
          .includes("no caption tracks found") ||
        lastTranscriptError?.message?.toLowerCase().includes("api 403") || // Specific API disabled code
        lastTranscriptError?.message
          ?.toLowerCase()
          .includes("no transcript tracks available")
      ) {
        userErrorMessage =
          "Transcripts are disabled or unavailable for this video.";
      } else if (lastTranscriptError?.name === "AbortError") {
        userErrorMessage =
          "Could not retrieve transcript due to network timeouts.";
      } else if (lastTranscriptError) {
        // Generic technical failure
        userErrorMessage = `Could not retrieve transcript due to a technical issue (${
          lastTranscriptError.name || "Error"
        }).`;
      }

      // Return metadata along with the error message
      return NextResponse.json(
        {
          metadata: metadata as PodcastMetadata, // Return fetched metadata (cast to full type)
          summary: "Transcript unavailable.", // Clear summary
          transcript: processedTranscriptText, // Contains "Transcript not available."
          error: userErrorMessage, // Provide specific error reason to UI
        },
        { status: 200 }
      ); // Return 200 OK but indicate transcript failure in payload
    }

    // Return successful response with metadata and summary
    console.log(
      `[${videoId}] Process completed successfully. Returning summary and transcript.`
    );
    return NextResponse.json({
      metadata: metadata as PodcastMetadata, // Cast to full type
      summary,
      transcript: processedTranscriptText, // Return the full processed text
      error: null, // Explicitly null on success
    });
  } catch (error: any) {
    // General catch block for unexpected errors (e.g., JSON parsing before videoId is set)
    const videoIdMsg = videoId
      ? `for video ID ${videoId}`
      : "(videoId unknown)";
    console.error(
      `[${
        videoId || "N/A"
      }] Unhandled error in POST /api/summarize ${videoIdMsg}: ${
        error.message || error
      }`,
      error.stack
    );
    return NextResponse.json(
      {
        error: `An unexpected server error occurred ${videoIdMsg}. Please check server logs.`,
      },
      { status: 500 }
    );
  }
}

// Removed the old fetchPodcastMetadata internal fetch logic
// Added enhanced logging to catch blocks of transcript fetchers
// Updated main POST handler to try multiple transcript methods sequentially
// Updated OpenAI model and prompt structure
// Improved error handling and user messaging when transcript fetching fails completely
// Refactored metadata fetching to use shared functions
// Added TTML parsing fallback to direct fetch
// Added helper for timestamp parsing
</file>

</files>
